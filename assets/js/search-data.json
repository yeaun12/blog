{
  
    
        "post0": {
            "title": "Data mining Final Project",
            "content": "&#45936;&#51060;&#53552; &#47560;&#51060;&#45789; &#44592;&#47568; &#54532;&#47196;&#51229;&#53944; . 1. 분석 배경 1.1 대회 소개 | 1.2 선정 이유 | 1.3 방향 및 목적 | . | 2. 데이터 설명 및 처리 2.1 패키지 로드 | 2.2 데이터 설명 | 2.3 데이터 전처리 | . | 3. 시각화 및 분석 3.1 데이터 시각화 | 3.2 분석 결과 | . | 4. 결론 4.1 최적 노선 제시 | 4.2. 활용 방안 시사점 | . | . 1. &#48516;&#49437; &#48176;&#44221; . 1.1 &#45824;&#54924; &#49548;&#44060; . 화성시는 전국 기초단체 중 가장 공장이 많은 지역이며 동탄, 병점 등 신도시의 입주로 인해 2018년 한 해 동안 인구가 가장 많이 증가한 지자체이다. 이에 따른 심각한 교통 문제를 겪고 있습니다. | 대회의 해결 과제는 화성시 내 인구와 이동형태를 고려하여 관내 시내버스에 대한 노선 신설이나 기존 노선을 개선하는 방안을 제시합니다. | . . 1.2 &#49440;&#51221; &#51060;&#50976; . 국토교통부에서 발표한 2020년 대중교통 이용 현황에 따르면 주 이용 교통수단은 버스인 것을 알 수 있습니다. | . 실제로 외출을 할 때 대부분 버스로 이동하며 주변에 학교에서 이용하는 학생이나 여러 아파트 단지에서 이용하는 주민들이 많지만 배차 간격, 환승 등의 불편함을 겪고 대중교통에서 오는 불편함이 큰 것을 알기 때문에 실제 경험을 바탕으로 하여 분석을 진행하기 위해 선정하였습니다. | . | . 1.3 &#48169;&#54693; &#48143; &#47785;&#51201; . 기본적으로 원래 대회의 목적에 맞추어 진행하였습니다. 시각화 결과에 따른 노선 제시로 이어지지만 그래프나 지도 시각화를 통한 분석을 우선으로 진행합니다. | 구체화된 완벽한 노선을 제시하기 보다는 유동 인구가 많은 중요한 정류장 포함한 노선 구축을 목적으로 합니다. | . | . . 2. &#45936;&#51060;&#53552; &#49444;&#47749; &#48143; &#52376;&#47532; . 2.1 &#54056;&#53412;&#51648; &#47196;&#46300; . from google.colab import drive drive.mount(&#39;/content/drive&#39;) #구글드라이브와 코랩 연결 . Mounted at /content/drive . pip install geopandas # 코랩 geopandas 설치 . pip install pydeck # 코랩 pydeck 설치 . import pandas as pd import numpy as np import geopandas as gpd import pydeck import folium import matplotlib.pyplot as plt import seaborn as sns import matplotlib #한글깨짐방지 폰트 설정 plt.rc(&#39;font&#39;, family=&#39;NanumGothic&#39;) from matplotlib import rc import matplotlib.font_manager as fm fm._rebuild() fm.get_fontconfig_fonts() font_location = &#39;/content/drive/MyDrive/NanumGothic.ttf&#39; # 폰트 파일 이름, 디렉토리 font_name = fm.FontProperties(fname=font_location).get_name() matplotlib.rc(&#39;font&#39;, family=font_name) . from matplotlib import font_manager import matplotlib matplotlib.font_manager._rebuild() for i in font_manager.fontManager.ttflist: if &#39;Nanum&#39; in i.name: print(i.name, i.fname) . 2.2 &#45936;&#51060;&#53552; &#49444;&#47749; . TripChain 버스 탑승 정보, 승하차역 정보, 환승 정보를 담은 버스 카드 태깅 데이터셋입니다. | . | . TripChain = pd.read_csv(&#39;/content/drive/MyDrive/TripChain_re.csv&#39;) TripChain.head() . 암호화카드번호||&#39; &#39;||환승횟수||&#39; &#39;||사용자구분||&#39; &#39;||버스노선ID1||&#39; &#39;||버스노선ID2||&#39; &#39;||버스노선ID3||&#39; &#39;||버스노선ID4||&#39; &#39;||버스노선ID5||&#39; &#39;||차량ID1||&#39; &#39;||차량ID2||&#39; ... &#39;||총탑승시간||&#39; &#39;||총소요시간||&#39; &#39;||최초승차일시||&#39; &#39;||최종하차일시||&#39; &#39;||승차역ID1||&#39; &#39;||승차역ID2||&#39; &#39;||승차역ID3||&#39; &#39;||하차역ID4||&#39; &#39;||하차역ID5||&#39; &#39;||최종하차역ID||&#39; . 0 900079696430 | 2 | 1 | 41002045.0 | 41002044.0 | NaN | NaN | NaN | 141771735.0 | 141771587.0 | ... | 25 | 25 | 20180701052543 | 20180701064826 | 4116828.0 | 4150144.0 | NaN | NaN | NaN | 4116708.0 | . 1 900079697651 | 1 | 1 | 41031040.0 | NaN | NaN | NaN | NaN | 141701792.0 | NaN | ... | 3 | 3 | 20180701072156 | 20180701072520 | 4117280.0 | NaN | NaN | NaN | NaN | 4117269.0 | . 2 900079698254 | 1 | 1 | 41031121.0 | NaN | NaN | NaN | NaN | 141701843.0 | NaN | ... | 66 | 66 | 20180701123653 | 20180701134223 | 4199619.0 | NaN | NaN | NaN | NaN | 4107936.0 | . 3 900079699257 | 1 | 1 | 41031013.0 | NaN | NaN | NaN | NaN | 141701450.0 | NaN | ... | 1 | 1 | 20180701224424 | 20180701224543 | 4108130.0 | NaN | NaN | NaN | NaN | 4116717.0 | . 4 900079701419 | 1 | 1 | 41020001.0 | NaN | NaN | NaN | NaN | 141703985.0 | NaN | ... | 8 | 8 | 20180701085058 | 20180701085910 | 4100122.0 | NaN | NaN | NaN | NaN | 4116848.0 | . 5 rows × 24 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; station_table 정류소 ID, 위치, 위도, 경도 정보를 담은 경기도 버스 정류장에 대한 데이터셋입니다. | . | . station_table = pd.read_csv(&#39;/content/drive/MyDrive/stations_table.csv&#39;) station_table . 표준정류장ID 시군명 정류소명 정류소영문명 정류소번호 중앙차로여부 관할관청 위치 WGS84위도 WGS84경도 모바일정류장ID 이비카드정류장ID . 0 228003422 | 용인시 | 손골마을회관.국제학교 | Songol Community Center, | 56443.0 | 노변정류장 | 경기도 용인시 | NaN | 37.342517 | 127.066817 | 56443.0 | NaN | . 1 228003423 | 용인시 | 풀잎사랑 | Pulipsarang | 56444.0 | 노변정류장 | 경기도 용인시 | NaN | 37.341800 | 127.068983 | 56444.0 | NaN | . 2 228003424 | 용인시 | 풀잎사랑 | Pulipsarang | 56445.0 | 노변정류장 | 경기도 용인시 | NaN | 37.341817 | 127.069083 | 56445.0 | NaN | . 3 228003425 | 용인시 | 대성공정 | Daesung Process | 56446.0 | 노변정류장 | 경기도 용인시 | NaN | 37.339350 | 127.073067 | 56446.0 | NaN | . 4 228003426 | 용인시 | 대성공정 | Daesung Process | 56447.0 | 노변정류장 | 경기도 용인시 | NaN | 37.339183 | 127.073400 | 56447.0 | NaN | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 38504 228003381 | 용인시 | 현대빌리지 | Hyundai Village | 56402.0 | 노변정류장 | 경기도 용인시 | NaN | 37.326433 | 127.071317 | 56402.0 | NaN | . 38505 228003382 | 용인시 | 현대빌리지 | Hyundai Village | 56403.0 | 노변정류장 | 경기도 용인시 | NaN | 37.326450 | 127.071100 | 56403.0 | NaN | . 38506 236001229 | 포천시 | 평강식물원 | Pyunggang Botanical Garden | 40691.0 | 노변정류장 | 경기도 포천시 | 경기도 포천시 영북면 | 38.050650 | 127.306617 | 40691.0 | NaN | . 38507 236001230 | 포천시 | 양문1리.영중면사무소 | Yangmun 1-ri, Yeongjung-myeon Office | 40699.0 | 노변정류장 | 경기도 포천시 | NaN | 38.005517 | 127.245667 | 40699.0 | NaN | . 38508 236001234 | 포천시 | 포천농협.하나로마트 | Pocheon Nonghyup, Hanaro Mart | 40692.0 | 노변정류장 | 경기도 포천시 | NaN | 37.901217 | 127.206933 | 40692.0 | NaN | . 38509 rows × 12 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; emd_scco_geo 화성시 읍면동 geometry, 이름, 코드를 담은 화성시 읍면동 데이터셋입니다. | . | . emd_scco_geo =gpd.read_file(open(&#39;/content/drive/MyDrive/tl_scco_emd.geojson&#39;, encoding=&#39;utf-8&#39;)) emd_scco_geo . id emd_cd emd_eng_nm emd_kor_nm geometry . 0 2122 | 41590400 | Yanggam-myeon | 양감면 | MULTIPOLYGON (((126.99921 37.12304, 126.99906 ... | . 1 2123 | 41590410 | Jeongnam-myeon | 정남면 | MULTIPOLYGON (((127.00591 37.18681, 127.00595 ... | . 2 2087 | 41590116 | Jinan-dong | 진안동 | MULTIPOLYGON (((127.03486 37.22713, 127.03491 ... | . 3 2088 | 41590117 | Byeongjeom-dong | 병점동 | MULTIPOLYGON (((127.02686 37.20033, 127.02680 ... | . 4 2089 | 41590118 | Neung-dong | 능동 | MULTIPOLYGON (((127.05947 37.19984, 127.05948 ... | . 5 2090 | 41590119 | Gisan-dong | 기산동 | MULTIPOLYGON (((127.05295 37.21400, 127.05278 ... | . 6 2091 | 41590120 | Banwol-dong | 반월동 | MULTIPOLYGON (((127.06647 37.24009, 127.06650 ... | . 7 2092 | 41590121 | Banjeong-dong | 반정동 | MULTIPOLYGON (((127.04253 37.24702, 127.04263 ... | . 8 2093 | 41590122 | Hwanggye-dong | 황계동 | MULTIPOLYGON (((127.02352 37.22536, 127.02611 ... | . 9 2094 | 41590123 | Baeyang-dong | 배양동 | MULTIPOLYGON (((126.99364 37.23062, 126.99370 ... | . 10 2095 | 41590124 | Gian-dong | 기안동 | MULTIPOLYGON (((126.98181 37.21318, 126.98172 ... | . 11 2096 | 41590125 | Songsan-dong | 송산동 | MULTIPOLYGON (((127.02429 37.20579, 127.02426 ... | . 12 2097 | 41590126 | Annyeong-dong | 안녕동 | MULTIPOLYGON (((127.00591 37.18681, 127.00592 ... | . 13 2098 | 41590127 | Bansong-dong | 반송동 | MULTIPOLYGON (((127.08299 37.18425, 127.08290 ... | . 14 2099 | 41590128 | Seogu-dong | 석우동 | MULTIPOLYGON (((127.08799 37.21660, 127.08810 ... | . 15 2100 | 41590129 | Osan-dong | 오산동 | MULTIPOLYGON (((127.08299 37.18425, 127.08300 ... | . 16 2101 | 41590130 | Cheonggye-dong | 청계동 | MULTIPOLYGON (((127.13001 37.20120, 127.13034 ... | . 17 2102 | 41590131 | Yeongcheon-dong | 영천동 | MULTIPOLYGON (((127.08583 37.20471, 127.08586 ... | . 18 2103 | 41590132 | Jung-dong | 중동 | MULTIPOLYGON (((127.15087 37.21984, 127.15099 ... | . 19 2104 | 41590133 | Sin-dong | 신동 | MULTIPOLYGON (((127.15228 37.19841, 127.15231 ... | . 20 2105 | 41590134 | Mok-dong | 목동 | MULTIPOLYGON (((127.13150 37.19569, 127.13150 ... | . 21 2106 | 41590135 | Sancheok-dong | 산척동 | MULTIPOLYGON (((127.11354 37.17947, 127.11375 ... | . 22 2107 | 41590136 | Jangji-dong | 장지동 | MULTIPOLYGON (((127.12891 37.16213, 127.12901 ... | . 23 2108 | 41590137 | Song-dong | 송동 | MULTIPOLYGON (((127.10469 37.17752, 127.10473 ... | . 24 2109 | 41590138 | Banggyo-dong | 방교동 | MULTIPOLYGON (((127.09437 37.18396, 127.09460 ... | . 25 2110 | 41590139 | Geumgok-dong | 금곡동 | MULTIPOLYGON (((127.07119 37.18403, 127.07182 ... | . 26 2111 | 41590253 | Bongdam-eup | 봉담읍 | MULTIPOLYGON (((126.95798 37.24281, 126.95799 ... | . 27 2112 | 41590256 | Ujeong-eup | 우정읍 | MULTIPOLYGON (((126.73088 37.04639, 126.73141 ... | . 28 2113 | 41590259 | Hyangnam-eup | 향남읍 | MULTIPOLYGON (((126.99724 37.12423, 126.99732 ... | . 29 2114 | 41590262 | Namyang-eup | 남양읍 | MULTIPOLYGON (((126.79805 37.17035, 126.79780 ... | . 30 2115 | 41590310 | Maesong-myeon | 매송면 | MULTIPOLYGON (((126.95620 37.24524, 126.95574 ... | . 31 2116 | 41590320 | Bibong-myeon | 비봉면 | MULTIPOLYGON (((126.83330 37.23058, 126.83273 ... | . 32 2117 | 41590330 | Mado-myeon | 마도면 | MULTIPOLYGON (((126.79590 37.19074, 126.79613 ... | . 33 2118 | 41590340 | Songsan-myeon | 송산면 | MULTIPOLYGON (((126.66984 37.27413, 126.66946 ... | . 34 2119 | 41590350 | Seosin-myeon | 서신면 | MULTIPOLYGON (((126.62268 37.17914, 126.62301 ... | . 35 2120 | 41590360 | Paltan-myeon | 팔탄면 | MULTIPOLYGON (((126.83055 37.15032, 126.83110 ... | . 36 2121 | 41590370 | Jangan-myeon | 장안면 | MULTIPOLYGON (((126.81262 37.01341, 126.81262 ... | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; cell_pop 인구밀도, geometry 를 담은 거주 인구 데이터셋입니다. | . | . cell_pop = gpd.read_file(open(&#39;/content/drive/MyDrive/h_100m_cell_pop.geojson&#39;, encoding=&#39;utf-8&#39;)) cell_pop . id pop_mn_0_9 pop_mn_10_ pop_mn_20_ pop_mn_30_ pop_mn_40_ pop_mn_50_ pop_mn_60_ pop_mn_70_ pop_mn_80_ ... pop_wmn_40 pop_wmn_50 pop_wmn_60 pop_wmn_70 pop_wmn_80 pop_wmn_90 pop_wmn_tt pop_ttl emd_cd geometry . 0 1 | 0.0 | 0.0 | 0.0 | 0.0 | 2.0 | 1.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 3.0 | 1.0 | 0.0 | 0.0 | 0.0 | 4.0 | 7.0 | None | POLYGON ((126.74685 37.03640, 126.74797 37.036... | . 1 2 | 0.0 | 0.0 | 1.0 | 0.0 | 1.0 | 3.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 4.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.0 | 10.0 | None | POLYGON ((126.74572 37.03730, 126.74684 37.037... | . 2 4 | 2.0 | 0.0 | 1.0 | 0.0 | 1.0 | 1.0 | 1.0 | 0.0 | 0.0 | ... | 0.0 | 2.0 | 0.0 | 2.0 | 0.0 | 0.0 | 6.0 | 12.0 | None | POLYGON ((126.74797 37.03730, 126.74909 37.037... | . 3 16 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | ... | 0.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 3.0 | 5.0 | None | POLYGON ((126.74570 37.04271, 126.74682 37.042... | . 4 18 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | ... | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 5.0 | 8.0 | None | POLYGON ((126.68357 37.11554, 126.68470 37.115... | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 10642 10643 | 1.0 | 1.0 | 2.0 | 2.0 | 1.0 | 3.0 | 3.0 | 1.0 | 0.0 | ... | 1.0 | 4.0 | 2.0 | 1.0 | 0.0 | 0.0 | 16.0 | 30.0 | 41590132 | POLYGON ((127.14281 37.20598, 127.14393 37.205... | . 10643 10644 | 1.0 | 0.0 | 1.0 | 0.0 | 2.0 | 3.0 | 1.0 | 0.0 | 0.0 | ... | 1.0 | 3.0 | 0.0 | 0.0 | 0.0 | 0.0 | 8.0 | 16.0 | 41590132 | POLYGON ((127.14393 37.20598, 127.14506 37.205... | . 10644 10645 | 0.0 | 1.0 | 1.0 | 0.0 | 2.0 | 1.0 | 0.0 | 1.0 | 0.0 | ... | 3.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 6.0 | 12.0 | 41590132 | POLYGON ((127.14393 37.20688, 127.14506 37.206... | . 10645 10646 | 0.0 | 0.0 | 2.0 | 0.0 | 0.0 | 1.0 | 2.0 | 0.0 | 0.0 | ... | 1.0 | 2.0 | 1.0 | 0.0 | 0.0 | 0.0 | 5.0 | 10.0 | 41590132 | POLYGON ((127.14506 37.20688, 127.14619 37.206... | . 10646 10647 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 3.0 | 2.0 | 0.0 | 0.0 | ... | 0.0 | 1.0 | 0.0 | 1.0 | 0.0 | 0.0 | 3.0 | 9.0 | 41590132 | POLYGON ((127.14957 37.21048, 127.15070 37.210... | . 10647 rows × 26 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; moc_link link ID, node, road name, geometry 정보를 담은 화성시 도로 네트워크 링크 데이터셋입니다. | . | . moc_link = gpd.read_file(open(&#39;/content/drive/MyDrive/moc_link_2018.geojson&#39;, encoding=&#39;utf-8&#39;)) moc_link . id link_id f_node t_node road_use lanes road_rank road_name multi_link connect ... dept_code stnl_reg road_type road_no tmpid upload_id sosfnodeid sostnodeid shape_stle geometry . 0 63255 | 2333070000 | 2330045500 | 2330045200 | 0 | 1.0 | 107 | - | 0 | 000 | ... | 41590 | 233 | 000 | - | NTIC_0616 U | None | TEMP1074 | TEMP1073 | 154.657772 | MULTILINESTRING ((126.81451 37.14096, 126.8155... | . 1 63119 | 2330018207 | 2330007302 | 2330007304 | 0 | 2.0 | 103 | 일반국도43호선 | 0 | 000 | ... | 41590 | 233 | 000 | 43 | None | None | None | None | 235.832927 | MULTILINESTRING ((126.91828 37.14319, 126.9186... | . 2 63120 | 2330018108 | 2330007305 | 2330007304 | 0 | 2.0 | 103 | 일반국도43호선 | 0 | 000 | ... | 41590 | 233 | 000 | 43 | None | None | None | None | 77.012754 | MULTILINESTRING ((126.91982 37.14574, 126.9197... | . 3 63150 | 2333144900 | 2330079100 | 2330076000 | 0 | 2.0 | 107 | - | 0 | 000 | ... | 41590 | 233 | 000 | - | None | None | TEMP846 | TEMP636 | 368.373468 | MULTILINESTRING ((127.04169 37.20703, 127.0417... | . 4 63176 | 2230004902 | 2330009100 | 2330009101 | 0 | 1.0 | 106 | 지방도314호선 | 0 | 000 | ... | 41590 | 233 | 000 | 314 | None | None | None | None | 2351.958941 | MULTILINESTRING ((127.00177 37.16565, 127.0017... | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 10558 298721 | 2280726100 | 2280260400 | 2280260100 | 0 | 1.0 | 107 | 고매로43번길 | 0 | 000 | ... | 60170 | 228 | 000 | - | TMPID20180618112952 | geostoryco20180709161522 | TMPID20180618111732 | TMPID20180618111733 | 246.410905 | MULTILINESTRING ((127.11885 37.23641, 127.1187... | . 10559 298722 | 2280726200 | 2280260400 | 2280260300 | 0 | 1.0 | 107 | 고매로43번길 | 0 | 000 | ... | 60170 | 228 | 004 | - | TMPID20180618112957 | geostoryco20180709161522 | TMPID20180618111732 | TMPID20180618111731 | 118.352306 | MULTILINESTRING ((127.11872 37.23639, 127.1187... | . 10560 298723 | 2280726300 | 2280260300 | 2280260400 | 0 | 1.0 | 107 | 고매로43번길 | 0 | 000 | ... | 60170 | 228 | 004 | - | TMPID20180618112958 | geostoryco20180709161522 | TMPID20180618111731 | TMPID20180618111732 | 128.318085 | MULTILINESTRING ((127.11847 37.23533, 127.1186... | . 10561 298724 | 2280726400 | 2280260600 | 2280260500 | 0 | 2.0 | 107 | - | 0 | 000 | ... | 60170 | 228 | 000 | - | TMPID20180618113003 | geostoryco20180709161522 | TMPID20180618111741 | TMPID20180618111740 | 486.744967 | MULTILINESTRING ((127.12064 37.22955, 127.1206... | . 10562 298725 | 2280726500 | 2280260500 | 2280260600 | 0 | 2.0 | 107 | - | 0 | 000 | ... | 60170 | 228 | 000 | - | TMPID20180618113004 | geostoryco20180709161522 | TMPID20180618111740 | TMPID20180618111741 | 495.410883 | MULTILINESTRING ((127.11913 37.22545, 127.1196... | . 10563 rows × 27 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; route_mapping 운수사 정보, 이비노선 ID, 표준노선 ID, 노선명(버스번호) 을 담은 버스노선-정류장 매핑 테이블 데이터셋입니다. | . | . route_mapping = pd.read_csv(&#39;/content/drive/MyDrive/routestationmapping.csv&#39;) route_mapping . 구분 운수사명 운수사ID 이비노선ID 표준노선ID 노선명 . 0 경기시내 | 경원여객M | 2805000 | 216000044 | 28050900 | M6410 | . 1 경기시내 | 강화운수 | 4100100 | 232000028 | 41001001 | 2 | . 2 경기시내 | 강화운수 | 4100100 | 232000029 | 41001013 | 88 | . 3 경기시내 | 강화운수 | 4100100 | 232000061 | 41001020 | 3000 | . 4 경기시내 | 강화운수 | 4100100 | 232000067 | 41001024 | 388 | . ... ... | ... | ... | ... | ... | ... | . 2122 경기시내 | 서현운수 | 4108800 | 229000060 | 41088004 | 330 | . 2123 경기시내 | 서현운수 | 4108800 | 229000063 | 41088005 | 850 | . 2124 경기시내 | 코레일네트웍스 | 4108900 | 213000024 | 41089001 | 8507 | . 2125 경기시내 | 신성교통 | 4109100 | 229000102 | 41091900 | M7111 | . 2126 경기시내 | 군포여객 | 4109400 | 225000004 | 41094001 | 100 | . 2127 rows × 6 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; route_info 버스정류장 이름, 노선 ID, 정류장 ID, 모바일 정류장 ID 를 담은 버스노선 매핑 데이터셋입니다. | . | . route_info = pd.read_csv(&#39;/content/drive/MyDrive/routestationinfo.csv&#39;) route_info.head() . seq pr_station_id bus_line_no bus_line_no_seq station_nm station_id mobile_no . 0 65286 | 228000018 | 10-4 | 1 | 용인터미널 | 228001552 | 47634.0 | . 1 65287 | 228000018 | 10-4 | 2 | 용인터미널(경유) | 277102443 | NaN | . 2 65288 | 228000018 | 10-4 | 3 | 포브스병원 | 228000443 | 29439.0 | . 3 65289 | 228000018 | 10-4 | 4 | 제일교회 | 228000665 | 29881.0 | . 4 65290 | 228000018 | 10-4 | 5 | 라이프아파트 | 228000664 | 29457.0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 많게는 백만이 넘는 행과 51열로 구성되었고, 적게는 37행 5열의 크기로 제공되었습니다. | . 2.3 &#45936;&#51060;&#53552; &#51204;&#52376;&#47532; . 버스 카드 태깅 정보를 담고 있는 데이터셋에서 사람마다 환승하는 횟수가 다르기 때문에 환승에서 파생된 정보에 NA값이 발생하였고, 데이터 타입 변환을 위해 모두 0으로 치환하였습니다. | 같은 데이터셋에서 정류장 ID가 비정상적으로 크게 나오는 것을 확인하였고 크게 의미가 없다고 생각하여 별다른 처리를 하지 않았습니다. | 경기도 버스 정류장에 대한 정보를 담은 데이터셋에서 이 대회는 화성시에 대한 정보만 제공하기 때문에 화성시가 아닌 행에서는 NA값이 발생하였고 문제가 발생한 행을 분석 목적이 화성시에 한해 진행하기 때문에 제거해주었습니다. | . Trip_new = pd.merge(TripChain,route_mapping, how=&#39;left&#39;, left_on = &quot;&#39;||버스노선ID1||&#39;&quot;, right_on = &#39;표준노선ID&#39;).drop([&quot;&#39;||사용자구분||&#39;&quot;, &quot;&#39;||총통행거리||&#39;&quot;, &quot;&#39;||총탑승시간||&#39;&quot;,&quot;&#39;||총소요시간||&#39;&quot;], axis = 1) # TripChain,route_mapping 데이터셋을 합친 후 사용하지 않는 열 제거 . station_table 전처리 | . station_table = station_table.dropna() # na값 제거 station_table = station_table[station_table[&#39;시군명&#39;] == &#39;화성시&#39;] # 시군명이 화성시인 행을 제외하고 모두 삭제 station_table[&#39;emd&#39;] = station_table[&#39;위치&#39;].str[7:11] # 주소정보에서 읍면동만 추출 station_table = station_table[~station_table[&#39;emd&#39;].str.contains(&#39;구&#39;)] # 화성시는 읍면동 데이터만 있는데 위치는 수원시이지만 관할구청이 화성시이기 때문에 포함되어있던 행 제거 station_table[&#39;emd&#39;] = station_table[&#39;emd&#39;].str.strip() # emd 열의 공백 제거 . 표준정류장ID 시군명 정류소명 정류소영문명 정류소번호 중앙차로여부 관할관청 위치 WGS84위도 WGS84경도 모바일정류장ID 이비카드정류장ID emd . 15072 233100553 | 화성시 | 우미제일.전하리교회 | Woomi Jeil, Jeonhari Church | 37760.0 | 노변정류장 | 경기도 화성시 | 경기도 화성시 석우동 | 37.212133 | 127.079383 | [None None None None None None None None None ... | 4130121.0 | 석우동 | . 22829 233000001 | 화성시 | 수영오거리.방송통신대입구 | Suyeong Ogeori, Korea National Open University | 2051.0 | 노변정류장 | 경기도 화성시 | 경기도 수원시 권선구 오목천동 | 37.237717 | 126.962400 | [None None None None None None None None None ... | 4116687.0 | 권선구 | . 22830 233000002 | 화성시 | 한국농수산대학 | Korea National College of Agriculture and Fish... | 36161.0 | 노변정류장 | 경기도 화성시 | 경기도 화성시 봉담읍 | 37.229317 | 126.970433 | [None None None None None None None None None ... | 4116684.0 | 봉담읍 | . 22831 233000003 | 화성시 | 상두2리 | Sangdu 2-ri | 37320.0 | 노변정류장 | 경기도 화성시 | 경기도 화성시 향남읍 | 37.101367 | 126.948400 | [None None None None None None None None None ... | 4130294.0 | 향남읍 | . 22832 233000004 | 화성시 | 대양2리 | Daeyang 2-ri | 36108.0 | 노변정류장 | 경기도 화성시 | 경기도 화성시 양감면 | 37.086417 | 126.942483 | [None None None None None None None None None ... | 4130270.0 | 양감면 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 36481 233001145 | 화성시 | 덕다리입구 | Deokdari | 37489.0 | 노변정류장 | 경기도 화성시 | 경기도 화성시 장안면 | 37.067700 | 126.838083 | [None None None None None None None None None ... | 4170855.0 | 장안면 | . 36482 233001146 | 화성시 | 노진초등학교 | Nojin Elementary School | 36772.0 | 노변정류장 | 경기도 화성시 | 경기도 화성시 장안면 | 37.052617 | 126.809567 | [None None None None None None None None None ... | 4170316.0 | 장안면 | . 36491 233001148 | 화성시 | 노진2리(노인정) | Nojin 2-ri Seniors Community Center | 36770.0 | 노변정류장 | 경기도 화성시 | 경기도 화성시 장안면 | 37.049117 | 126.812383 | [None None None None None None None None None ... | 4170318.0 | 장안면 | . 36494 233001149 | 화성시 | 노진2리 | Nojin 2-ri | 37488.0 | 노변정류장 | 경기도 화성시 | 경기도 화성시 장안면 | 37.047600 | 126.812850 | [None None None None None None None None None ... | 4170856.0 | 장안면 | . 36498 233001150 | 화성시 | 노진5 | Nojin 5-ri | 36769.0 | 노변정류장 | 경기도 화성시 | 경기도 화성시 장안면 | 37.042000 | 126.815617 | [None None None None None None None None None ... | 4170319.0 | 장안면 | . 1420 rows × 13 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; cell_pop 전처리 | . unique 함수를 써보면 전체 데이터에서 화성시에 해당하는 읍면동 코드만 있는 것을 볼 수 있다. . cell_pop[&#39;emd_cd&#39;].unique() . array([None, &#39;41590256&#39;, &#39;41590350&#39;, &#39;41590370&#39;, &#39;41590360&#39;, &#39;41590259&#39;, &#39;41590400&#39;, &#39;41590330&#39;, &#39;41590340&#39;, &#39;41590262&#39;, &#39;41590253&#39;, &#39;41590410&#39;, &#39;41590320&#39;, &#39;41590126&#39;, &#39;41590124&#39;, &#39;41590123&#39;, &#39;41590125&#39;, &#39;41590310&#39;, &#39;41590136&#39;, &#39;41590139&#39;, &#39;41590138&#39;, &#39;41590137&#39;, &#39;41590127&#39;, &#39;41590129&#39;, &#39;41590135&#39;, &#39;41590134&#39;, &#39;41590133&#39;, &#39;41590117&#39;, &#39;41590118&#39;, &#39;41590116&#39;, &#39;41590122&#39;, &#39;41590119&#39;, &#39;41590130&#39;, &#39;41590131&#39;, &#39;41590128&#39;, &#39;41590132&#39;, &#39;41590121&#39;, &#39;41590120&#39;], dtype=object) . 화성시가 아닌 데이터가 na로 되어있기 때문에 모두 삭제한다. . cell_pop = cell_pop.dropna() # 화성시가 아닌 행 즉, None값 제거 . 읍면동 코드별로 그룹화 . grouped = cell_pop[&#39;pop_ttl&#39;].groupby(cell_pop[&#39;emd_cd&#39;]) #emd_cd 별로 pop_ttl 그룹화 grouped = pd.DataFrame(grouped.sum()) # 그룹화 된 값을 더해서 읍면동별 인구수 데이터프레임 생성 grouped[&#39;index&#39;] = grouped.index # 인덱스값으로 인덱스 지정 new_emd = pd.merge(emd_scco_geo, grouped, how=&#39;inner&#39;, on=&#39;emd_cd&#39;) # emd_scco_geo, grouped 합치기 new_emd = new_emd.drop([&#39;index&#39;], axis = 1) # index 값 제거 new_emd_sort = new_emd.sort_values(by = &#39;pop_ttl&#39;, ascending = False) # pop_ttl 을 기준으로 내림차순 정렬 . 시각화할 때 y축의 스케일 차이를 줄이기 위해 100을 기준으로 인구 상위권과 하위권 구분 . top_emd = new_emd_sort.head(14) tail_emd = new_emd_sort.tail(23) . Tripchain 전처리 | . 사용하지 않는 컬럼 제거 . TripChain = TripChain.fillna(0) #데이터 처리를 위해 모든 na값을 0으로 대체 TripChain[[&quot;&#39;||버스노선ID1||&#39;&quot;,&quot;&#39;||버스노선ID2||&#39;&quot;,&quot;&#39;||버스노선ID3||&#39;&quot;,&quot;&#39;||버스노선ID4||&#39;&quot;,&quot;&#39;||버스노선ID5||&#39;&quot;, &quot;&#39;||차량ID1||&#39;&quot;, &quot;&#39;||차량ID2||&#39;&quot;, &quot;&#39;||차량ID3||&#39;&quot;, &quot;&#39;||차량ID4||&#39;&quot;, &quot;&#39;||차량ID5||&#39;&quot;,&quot;&#39;||승차역ID1||&#39;&quot;,&quot;&#39;||승차역ID2||&#39;&quot;,&quot;&#39;||승차역ID3||&#39;&quot;,&quot;&#39;||하차역ID4||&#39;&quot;,&quot;&#39;||하차역ID5||&#39;&quot;, &quot;&#39;||최종하차역ID||&#39;&quot;]] = TripChain[[&quot;&#39;||버스노선ID1||&#39;&quot;,&quot;&#39;||버스노선ID2||&#39;&quot;,&quot;&#39;||버스노선ID3||&#39;&quot;,&quot;&#39;||버스노선ID4||&#39;&quot;,&quot;&#39;||버스노선ID5||&#39;&quot;, &quot;&#39;||차량ID1||&#39;&quot;, &quot;&#39;||차량ID2||&#39;&quot;, &quot;&#39;||차량ID3||&#39;&quot;, &quot;&#39;||차량ID4||&#39;&quot;, &quot;&#39;||차량ID5||&#39;&quot;,&quot;&#39;||승차역ID1||&#39;&quot;,&quot;&#39;||승차역ID2||&#39;&quot;,&quot;&#39;||승차역ID3||&#39;&quot;,&quot;&#39;||하차역ID4||&#39;&quot;,&quot;&#39;||하차역ID5||&#39;&quot;,&quot;&#39;||최종하차역ID||&#39;&quot;]].astype(int) # 메모리가 소모가 크기 때문에 float 인 데이터를 int로 변환 . TripChain.drop([&quot;&#39;||사용자구분||&#39;&quot;, &quot;&#39;||총탑승시간||&#39;&quot;, &quot;&#39;||총소요시간||&#39;&quot;], axis = 1) . 암호화카드번호||&#39; &#39;||환승횟수||&#39; &#39;||버스노선ID1||&#39; &#39;||버스노선ID2||&#39; &#39;||버스노선ID3||&#39; &#39;||버스노선ID4||&#39; &#39;||버스노선ID5||&#39; &#39;||차량ID1||&#39; &#39;||차량ID2||&#39; &#39;||차량ID3||&#39; ... &#39;||차량ID5||&#39; &#39;||총통행거리||&#39; &#39;||최초승차일시||&#39; &#39;||최종하차일시||&#39; &#39;||승차역ID1||&#39; &#39;||승차역ID2||&#39; &#39;||승차역ID3||&#39; &#39;||하차역ID4||&#39; &#39;||하차역ID5||&#39; &#39;||최종하차역ID||&#39; . 0 900079696430 | 2 | 41002045 | 41002044 | 0 | 0 | 0 | 141771735 | 141771587 | 0 | ... | 0 | 11170 | 20180701052543 | 20180701064826 | 4116828 | 4150144 | 0 | 0 | 0 | 4116708 | . 1 900079697651 | 1 | 41031040 | 0 | 0 | 0 | 0 | 141701792 | 0 | 0 | ... | 0 | 1700 | 20180701072156 | 20180701072520 | 4117280 | 0 | 0 | 0 | 0 | 4117269 | . 2 900079698254 | 1 | 41031121 | 0 | 0 | 0 | 0 | 141701843 | 0 | 0 | ... | 0 | 23180 | 20180701123653 | 20180701134223 | 4199619 | 0 | 0 | 0 | 0 | 4107936 | . 3 900079699257 | 1 | 41031013 | 0 | 0 | 0 | 0 | 141701450 | 0 | 0 | ... | 0 | 500 | 20180701224424 | 20180701224543 | 4108130 | 0 | 0 | 0 | 0 | 4116717 | . 4 900079701419 | 1 | 41020001 | 0 | 0 | 0 | 0 | 141703985 | 0 | 0 | ... | 0 | 3240 | 20180701085058 | 20180701085910 | 4100122 | 0 | 0 | 0 | 0 | 4116848 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1048570 900016808011 | 1 | 41067012 | 0 | 0 | 0 | 0 | 141748107 | 0 | 0 | ... | 0 | 4720 | 20180704152354 | 20180704154330 | 4130419 | 0 | 0 | 0 | 0 | 4130524 | . 1048571 900016808389 | 1 | 41061025 | 0 | 0 | 0 | 0 | 141761176 | 0 | 0 | ... | 0 | 4120 | 20180704163548 | 20180704165100 | 4108112 | 0 | 0 | 0 | 0 | 4105383 | . 1048572 900016808481 | 2 | 41036131 | 41205007 | 0 | 0 | 0 | 141705287 | 141761323 | 0 | ... | 0 | 10609 | 20180704093348 | 20180704102535 | 4170340 | 4116674 | 0 | 0 | 0 | 4119436 | . 1048573 900016684666 | 1 | 41036113 | 0 | 0 | 0 | 0 | 141705620 | 0 | 0 | ... | 0 | 3930 | 20180704084717 | 20180704090128 | 4116734 | 0 | 0 | 0 | 0 | 4108146 | . 1048574 900016684666 | 1 | 41036113 | 0 | 0 | 0 | 0 | 141705637 | 0 | 0 | ... | 0 | 3940 | 20180704212154 | 20180704213835 | 4108147 | 0 | 0 | 0 | 0 | 4116733 | . 1048575 rows × 21 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; route_mapping 전처리 | . route_mapping = route_mapping.drop([&quot;운수사명&quot;,&quot;운수사ID&quot;, &quot;구분&quot;], axis = 1) # 사용하지 않는 열 제거 . Trip_new[&quot;&#39;||최초승차일시||&#39;&quot;] = Trip_new[&quot;&#39;||최초승차일시||&#39;&quot;].astype(str) # 시간으로 사용하기 위해 Trip_new[&quot;start_time&quot;] = pd.to_datetime(Trip_new[&quot;&#39;||최초승차일시||&#39;&quot;], infer_datetime_format = True) # 시간 형식으로 변환 Trip_new[&quot;time_hour&quot;] = Trip_new[&quot;start_time&quot;].dt.hour # 시간 추출해서 time_hour 에 저장 time_table = pd.DataFrame(Trip_new.groupby(&quot;time_hour&quot;)[&quot;암호화카드번호||&#39;&quot;].count()) # 시간대별 사용자 수 데이터프레임 생성 time_table[&#39;time_hour&#39;] = time_table.index # 인덱스 번호(시간)로 열 생성 time_table # 시간대별 이용자 수 . plt.figure(figsize = (18,4)) sns.barplot(x=&quot;time_hour&quot;, y=&quot;암호화카드번호||&#39;&quot;, data=time_table) plt.xlabel(&quot;시간(0~23시)&quot;, fontsize = 20) plt.ylabel(&quot;이용자수(명)&quot;, fontsize = 20) . condition = (Trip_new[&quot;time_hour&quot;]== 7) | (Trip_new[&quot;time_hour&quot;]== 8) # 출퇴근 시간인 7,8시로 조건식 작성 df_work = Trip_new[condition] # 데이터셋에서 이용 시간이 7,8시인 행 추출 . df_work.groupby(&quot;노선명&quot;).count() # 버스 노선 별 이용 횟수 . 암호화카드번호||&#39; &#39;||환승횟수||&#39; &#39;||버스노선ID1||&#39; &#39;||버스노선ID2||&#39; &#39;||버스노선ID3||&#39; &#39;||버스노선ID4||&#39; &#39;||버스노선ID5||&#39; &#39;||차량ID1||&#39; &#39;||차량ID2||&#39; &#39;||차량ID3||&#39; ... &#39;||승차역ID1||&#39; &#39;||승차역ID2||&#39; &#39;||승차역ID3||&#39; &#39;||하차역ID4||&#39; &#39;||하차역ID5||&#39; &#39;||최종하차역ID||&#39; 이비노선ID 표준노선ID start_time time_hour . 노선명 . 1 107 | 107 | 107 | 107 | 107 | 107 | 107 | 107 | 107 | 107 | ... | 107 | 107 | 107 | 107 | 107 | 107 | 107 | 107 | 107 | 107 | . 1-1 54 | 54 | 54 | 54 | 54 | 54 | 54 | 54 | 54 | 54 | ... | 54 | 54 | 54 | 54 | 54 | 54 | 54 | 54 | 54 | 54 | . 1-2 59 | 59 | 59 | 59 | 59 | 59 | 59 | 59 | 59 | 59 | ... | 59 | 59 | 59 | 59 | 59 | 59 | 59 | 59 | 59 | 59 | . 1-4 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | . 1-5 11 | 11 | 11 | 11 | 11 | 11 | 11 | 11 | 11 | 11 | ... | 11 | 11 | 11 | 11 | 11 | 11 | 11 | 11 | 11 | 11 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . M5107 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | ... | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | . M5121 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | . M5422 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | ... | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | 4 | . M6427 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | ... | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | . M7731 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | ... | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | 3 | . 360 rows × 24 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; t = pd.DataFrame(Trip_new.groupby(&quot;노선명&quot;)[&quot;암호화카드번호||&#39;&quot;].count()) # 노선명 별 이용 횟수 t.sort_values(by = &quot;암호화카드번호||&#39;&quot;, ascending= False).head(10) # 카드번호의 카운트 횟수를 기준으로 내림차순 정렬 . 암호화카드번호||&#39; . 노선명 . 92-1 72343 | . 720-2 58793 | . 3 51687 | . 98 47714 | . 62-1 46534 | . 2-1 42843 | . 13-5 42474 | . 720-1 41148 | . 301 36198 | . 13-1 33098 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; 이용자 수가 가장 많은 92-1번 버스는 동탄에서 수원을 잇는 버스 노선이다. | . .  ⦁ 2018년 7월 승차일 기준 1~4일의 버스 카드태깅 정보을 담은 TripChain 데이터셋 -  사람마다 환승을 하는 횟수가 다르기 때문에 환승에서 파생된 정보에 NA값 발생한다. .  ⦁ 17~18년 기준, 경기도 버스 정류장에 대한 정보를 담은 stations_table 데이터셋 -  경기도 내에 있는 버스정류장 정보를 담았지만 대회에서는  화성시에 대한 정보만 제공했기 때문에 화성시가 아닌 경우 NA값 발생한다. . -&gt; 가장 큰 두개의 데이터셋에서 NA가 대량 발생하였고 이 외에는 큰 결측치나 이상치는 없는 것을 확인 . 3. &#49884;&#44033;&#54868; &#48143; &#48516;&#49437; . 3.1 &#45936;&#51060;&#53552; &#49884;&#44033;&#54868; . plt.rcParams[&#39;font.size&#39;] = 25 # 글씨 크기 설정 . plt.figure(figsize = (45,10)) # 그림 크기 설정 sns.barplot(x=&quot;emd_kor_nm&quot;, y=&quot;pop_ttl&quot;, data=new_emd_sort) # x축, y축 설정 후 그래프 그리기 plt.xlabel(&quot;읍면동&quot;) # x축 이름 plt.ylabel(&quot;인구수(명)&quot;) # y축 이름 plt.xticks(rotation=45) # x축에 들어가는 데이터가 많기 때문에 가독성을 위해 x축 글씨 설정 . 전체적으로 봤을 때는 y축의 스케일 차이가 많이 나서 구분하기 쉽게 보기 위해 상위권과 하위권을 구분지어 시각화 . plt.figure(figsize = (18,4)) # 그림 크기 설정 sns.barplot(x=&quot;emd_kor_nm&quot;, y=&quot;pop_ttl&quot;, data=top_emd) # x축, y축 설정 후 그래프 그리기 plt.xlabel(&quot;읍면동&quot;) # x축 이름 plt.ylabel(&quot;인구수&quot;) # y축 이름 . Text(0, 0.5, &#39;인구수&#39;) . plt.figure(figsize = (30,6)) # 그림 크기 설정 sns.barplot(x=&quot;emd_kor_nm&quot;, y=&quot;pop_ttl&quot;, data=tail_emd) # x축, y축 설정 후 그래프 그리기 plt.xlabel(&quot;읍면동&quot;) # x축 이름 plt.ylabel(&quot;인구수&quot;) # y축 이름 . (array([ 0., 2500., 5000., 7500., 10000., 12500.]), &lt;a list of 6 Text major ticklabel objects&gt;) . 인구수를 살펴보면 향남읍, 봉담읍, 반송동이 인구가 가장 많고 y축을 살펴보면 신동, 반정동, 중동과 큰 차이를 보이고 있습니다. | . cell_pop top_emd = 향남읍(41590259), 봉담읍(41590253), 반송동(41590127), 청계동(41590130), 병정동(41590117) | tail_emd = 신동(41590133), 양감면(41590400), 서신면(41590350), 중동(41590132), 마도면(41590330) | . | . pop_test = cell_pop.groupby([&#39;emd_cd&#39;]).sum().drop([&#39;id&#39;], axis = 1) # 사용하지 않는 id를 제거한 읍면동별 인구수 pop_test . pop_mn_0_9 pop_mn_10_ pop_mn_20_ pop_mn_30_ pop_mn_40_ pop_mn_50_ pop_mn_60_ pop_mn_70_ pop_mn_80_ pop_mn_90_ ... pop_wmn_20 pop_wmn_30 pop_wmn_40 pop_wmn_50 pop_wmn_60 pop_wmn_70 pop_wmn_80 pop_wmn_90 pop_wmn_tt pop_ttl . emd_cd . 41590116 916.0 | 846.0 | 2049.0 | 3034.0 | 2042.0 | 1282.0 | 600.0 | 281.0 | 86.0 | 1.0 | ... | 1697.0 | 1869.0 | 1380.0 | 1091.0 | 600.0 | 401.0 | 174.0 | 0.0 | 8806.0 | 19956.0 | . 41590117 2820.0 | 3446.0 | 2855.0 | 3646.0 | 4718.0 | 2780.0 | 1207.0 | 610.0 | 207.0 | 0.0 | ... | 2726.0 | 3786.0 | 4725.0 | 2523.0 | 1389.0 | 877.0 | 322.0 | 2.0 | 22507.0 | 44812.0 | . 41590118 3295.0 | 2498.0 | 2074.0 | 4207.0 | 3805.0 | 1815.0 | 820.0 | 443.0 | 122.0 | 0.0 | ... | 1992.0 | 4339.0 | 3604.0 | 1739.0 | 1083.0 | 579.0 | 234.0 | 1.0 | 19112.0 | 38207.0 | . 41590119 321.0 | 229.0 | 289.0 | 636.0 | 407.0 | 250.0 | 128.0 | 48.0 | 8.0 | 0.0 | ... | 282.0 | 557.0 | 381.0 | 257.0 | 122.0 | 50.0 | 25.0 | 0.0 | 2300.0 | 4617.0 | . 41590120 2284.0 | 1835.0 | 1853.0 | 2794.0 | 2752.0 | 1796.0 | 847.0 | 278.0 | 92.0 | 1.0 | ... | 1664.0 | 2947.0 | 2542.0 | 1839.0 | 800.0 | 278.0 | 173.0 | 0.0 | 14197.0 | 28740.0 | . 41590121 2.0 | 3.0 | 11.0 | 9.0 | 17.0 | 25.0 | 15.0 | 10.0 | 4.0 | 0.0 | ... | 5.0 | 10.0 | 10.0 | 16.0 | 12.0 | 15.0 | 4.0 | 0.0 | 77.0 | 175.0 | . 41590122 8.0 | 18.0 | 35.0 | 22.0 | 35.0 | 67.0 | 50.0 | 23.0 | 18.0 | 0.0 | ... | 21.0 | 16.0 | 23.0 | 37.0 | 45.0 | 38.0 | 27.0 | 0.0 | 225.0 | 503.0 | . 41590123 10.0 | 27.0 | 41.0 | 36.0 | 45.0 | 93.0 | 59.0 | 31.0 | 4.0 | 0.0 | ... | 29.0 | 15.0 | 37.0 | 46.0 | 42.0 | 24.0 | 27.0 | 0.0 | 261.0 | 608.0 | . 41590124 824.0 | 1012.0 | 711.0 | 848.0 | 1374.0 | 910.0 | 450.0 | 217.0 | 69.0 | 1.0 | ... | 650.0 | 942.0 | 1293.0 | 830.0 | 472.0 | 262.0 | 117.0 | 1.0 | 6201.0 | 12620.0 | . 41590125 242.0 | 333.0 | 489.0 | 518.0 | 630.0 | 566.0 | 352.0 | 175.0 | 51.0 | 0.0 | ... | 381.0 | 396.0 | 529.0 | 563.0 | 328.0 | 208.0 | 96.0 | 0.0 | 3066.0 | 6425.0 | . 41590126 534.0 | 515.0 | 805.0 | 861.0 | 947.0 | 881.0 | 510.0 | 190.0 | 59.0 | 0.0 | ... | 557.0 | 769.0 | 755.0 | 876.0 | 500.0 | 220.0 | 100.0 | 0.0 | 4706.0 | 10010.0 | . 41590127 4959.0 | 5710.0 | 3846.0 | 5652.0 | 7838.0 | 4295.0 | 1415.0 | 680.0 | 197.0 | 1.0 | ... | 3916.0 | 6492.0 | 7978.0 | 3585.0 | 1619.0 | 868.0 | 350.0 | 3.0 | 35132.0 | 69743.0 | . 41590128 1070.0 | 1258.0 | 996.0 | 1295.0 | 1771.0 | 945.0 | 227.0 | 111.0 | 24.0 | 0.0 | ... | 913.0 | 1300.0 | 1697.0 | 714.0 | 258.0 | 130.0 | 57.0 | 0.0 | 7279.0 | 14977.0 | . 41590129 1003.0 | 556.0 | 809.0 | 1798.0 | 1050.0 | 626.0 | 309.0 | 106.0 | 26.0 | 0.0 | ... | 819.0 | 1618.0 | 971.0 | 607.0 | 338.0 | 108.0 | 61.0 | 0.0 | 6067.0 | 12353.0 | . 41590130 5468.0 | 4225.0 | 2154.0 | 4795.0 | 6751.0 | 2695.0 | 868.0 | 299.0 | 78.0 | 0.0 | ... | 2240.0 | 6118.0 | 6103.0 | 2227.0 | 940.0 | 413.0 | 173.0 | 0.0 | 27644.0 | 54986.0 | . 41590131 2718.0 | 1582.0 | 1470.0 | 4136.0 | 3099.0 | 1464.0 | 765.0 | 348.0 | 83.0 | 0.0 | ... | 1720.0 | 4109.0 | 2798.0 | 1500.0 | 941.0 | 415.0 | 184.0 | 0.0 | 15882.0 | 31558.0 | . 41590132 2.0 | 4.0 | 19.0 | 14.0 | 8.0 | 25.0 | 25.0 | 6.0 | 2.0 | 0.0 | ... | 14.0 | 6.0 | 10.0 | 26.0 | 15.0 | 6.0 | 5.0 | 0.0 | 95.0 | 200.0 | . 41590133 0.0 | 1.0 | 3.0 | 1.0 | 4.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 2.0 | 5.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 9.0 | 20.0 | . 41590134 1159.0 | 515.0 | 574.0 | 1679.0 | 1040.0 | 504.0 | 281.0 | 95.0 | 16.0 | 0.0 | ... | 568.0 | 1632.0 | 941.0 | 579.0 | 282.0 | 90.0 | 32.0 | 1.0 | 5661.0 | 11525.0 | . 41590135 403.0 | 245.0 | 308.0 | 657.0 | 435.0 | 252.0 | 130.0 | 42.0 | 6.0 | 0.0 | ... | 285.0 | 586.0 | 429.0 | 287.0 | 144.0 | 55.0 | 27.0 | 0.0 | 2488.0 | 4966.0 | . 41590136 561.0 | 266.0 | 328.0 | 892.0 | 573.0 | 313.0 | 194.0 | 65.0 | 15.0 | 0.0 | ... | 360.0 | 822.0 | 504.0 | 342.0 | 209.0 | 63.0 | 40.0 | 0.0 | 3114.0 | 6323.0 | . 41590137 633.0 | 220.0 | 195.0 | 819.0 | 503.0 | 190.0 | 92.0 | 48.0 | 12.0 | 0.0 | ... | 195.0 | 808.0 | 397.0 | 201.0 | 114.0 | 37.0 | 16.0 | 0.0 | 2540.0 | 5254.0 | . 41590138 91.0 | 14.0 | 134.0 | 341.0 | 105.0 | 64.0 | 34.0 | 15.0 | 5.0 | 0.0 | ... | 117.0 | 202.0 | 65.0 | 67.0 | 35.0 | 13.0 | 6.0 | 0.0 | 606.0 | 1410.0 | . 41590139 24.0 | 35.0 | 61.0 | 48.0 | 56.0 | 94.0 | 53.0 | 31.0 | 12.0 | 0.0 | ... | 36.0 | 30.0 | 47.0 | 68.0 | 36.0 | 33.0 | 20.0 | 2.0 | 322.0 | 737.0 | . 41590253 4235.0 | 4794.0 | 4404.0 | 5372.0 | 7580.0 | 5464.0 | 2813.0 | 1376.0 | 356.0 | 5.0 | ... | 3908.0 | 5705.0 | 6999.0 | 4872.0 | 2962.0 | 1601.0 | 753.0 | 9.0 | 35989.0 | 72421.0 | . 41590256 595.0 | 743.0 | 1084.0 | 1001.0 | 1570.0 | 1607.0 | 1022.0 | 572.0 | 242.0 | 3.0 | ... | 735.0 | 786.0 | 1097.0 | 1292.0 | 981.0 | 665.0 | 434.0 | 2.0 | 7347.0 | 15810.0 | . 41590259 5743.0 | 4882.0 | 4778.0 | 9042.0 | 8587.0 | 4990.0 | 2381.0 | 1162.0 | 374.0 | 5.0 | ... | 4236.0 | 7489.0 | 6884.0 | 4186.0 | 2485.0 | 1420.0 | 769.0 | 3.0 | 37603.0 | 79581.0 | . 41590262 1997.0 | 1735.0 | 2633.0 | 3635.0 | 3211.0 | 2885.0 | 1637.0 | 684.0 | 281.0 | 0.0 | ... | 1949.0 | 2755.0 | 2529.0 | 2500.0 | 1523.0 | 820.0 | 465.0 | 3.0 | 16101.0 | 34815.0 | . 41590310 241.0 | 286.0 | 528.0 | 426.0 | 620.0 | 775.0 | 524.0 | 271.0 | 104.0 | 1.0 | ... | 395.0 | 362.0 | 465.0 | 660.0 | 506.0 | 355.0 | 195.0 | 1.0 | 3448.0 | 7238.0 | . 41590320 122.0 | 198.0 | 364.0 | 320.0 | 410.0 | 644.0 | 481.0 | 247.0 | 112.0 | 2.0 | ... | 240.0 | 193.0 | 277.0 | 469.0 | 365.0 | 291.0 | 199.0 | 0.0 | 2368.0 | 5277.0 | . 41590330 151.0 | 213.0 | 385.0 | 405.0 | 492.0 | 631.0 | 434.0 | 261.0 | 105.0 | 1.0 | ... | 255.0 | 221.0 | 286.0 | 476.0 | 381.0 | 246.0 | 153.0 | 1.0 | 2365.0 | 5448.0 | . 41590340 285.0 | 426.0 | 598.0 | 511.0 | 804.0 | 1063.0 | 751.0 | 449.0 | 201.0 | 0.0 | ... | 430.0 | 386.0 | 608.0 | 851.0 | 717.0 | 524.0 | 323.0 | 8.0 | 4581.0 | 9695.0 | . 41590350 92.0 | 174.0 | 301.0 | 278.0 | 421.0 | 641.0 | 516.0 | 262.0 | 143.0 | 3.0 | ... | 206.0 | 187.0 | 262.0 | 504.0 | 418.0 | 285.0 | 215.0 | 4.0 | 2355.0 | 5205.0 | . 41590360 196.0 | 259.0 | 647.0 | 605.0 | 860.0 | 1120.0 | 679.0 | 323.0 | 143.0 | 0.0 | ... | 342.0 | 346.0 | 456.0 | 770.0 | 556.0 | 343.0 | 249.0 | 7.0 | 3546.0 | 8392.0 | . 41590370 254.0 | 333.0 | 559.0 | 538.0 | 773.0 | 1056.0 | 700.0 | 423.0 | 182.0 | 4.0 | ... | 364.0 | 360.0 | 448.0 | 726.0 | 618.0 | 475.0 | 279.0 | 4.0 | 3852.0 | 8690.0 | . 41590400 67.0 | 99.0 | 222.0 | 231.0 | 270.0 | 444.0 | 280.0 | 151.0 | 70.0 | 0.0 | ... | 140.0 | 121.0 | 125.0 | 274.0 | 247.0 | 161.0 | 135.0 | 1.0 | 1354.0 | 3193.0 | . 41590410 269.0 | 408.0 | 732.0 | 631.0 | 931.0 | 1137.0 | 701.0 | 360.0 | 151.0 | 2.0 | ... | 461.0 | 384.0 | 614.0 | 841.0 | 586.0 | 415.0 | 292.0 | 5.0 | 4252.0 | 9586.0 | . 37 rows × 23 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; pop_total = pop_test.iloc[:, 0:21].drop([&#39;pop_mn_ttl&#39;], axis = 1) # 위의 데이터셋에서 전체 인구수를 제외 pop_total[&#39;index&#39;] = pop_total.index # 인덱스를 열로 생성 pop_total.T.drop([&#39;index&#39;], axis = 0) # 데이터셋 열-&gt;행, 행-&gt;열로 변환 . emd_cd 41590116 41590117 41590118 41590119 41590120 41590121 41590122 41590123 41590124 41590125 ... 41590262 41590310 41590320 41590330 41590340 41590350 41590360 41590370 41590400 41590410 . pop_mn_0_9 916.0 | 2820.0 | 3295.0 | 321.0 | 2284.0 | 2.0 | 8.0 | 10.0 | 824.0 | 242.0 | ... | 1997.0 | 241.0 | 122.0 | 151.0 | 285.0 | 92.0 | 196.0 | 254.0 | 67.0 | 269.0 | . pop_mn_10_ 846.0 | 3446.0 | 2498.0 | 229.0 | 1835.0 | 3.0 | 18.0 | 27.0 | 1012.0 | 333.0 | ... | 1735.0 | 286.0 | 198.0 | 213.0 | 426.0 | 174.0 | 259.0 | 333.0 | 99.0 | 408.0 | . pop_mn_20_ 2049.0 | 2855.0 | 2074.0 | 289.0 | 1853.0 | 11.0 | 35.0 | 41.0 | 711.0 | 489.0 | ... | 2633.0 | 528.0 | 364.0 | 385.0 | 598.0 | 301.0 | 647.0 | 559.0 | 222.0 | 732.0 | . pop_mn_30_ 3034.0 | 3646.0 | 4207.0 | 636.0 | 2794.0 | 9.0 | 22.0 | 36.0 | 848.0 | 518.0 | ... | 3635.0 | 426.0 | 320.0 | 405.0 | 511.0 | 278.0 | 605.0 | 538.0 | 231.0 | 631.0 | . pop_mn_40_ 2042.0 | 4718.0 | 3805.0 | 407.0 | 2752.0 | 17.0 | 35.0 | 45.0 | 1374.0 | 630.0 | ... | 3211.0 | 620.0 | 410.0 | 492.0 | 804.0 | 421.0 | 860.0 | 773.0 | 270.0 | 931.0 | . pop_mn_50_ 1282.0 | 2780.0 | 1815.0 | 250.0 | 1796.0 | 25.0 | 67.0 | 93.0 | 910.0 | 566.0 | ... | 2885.0 | 775.0 | 644.0 | 631.0 | 1063.0 | 641.0 | 1120.0 | 1056.0 | 444.0 | 1137.0 | . pop_mn_60_ 600.0 | 1207.0 | 820.0 | 128.0 | 847.0 | 15.0 | 50.0 | 59.0 | 450.0 | 352.0 | ... | 1637.0 | 524.0 | 481.0 | 434.0 | 751.0 | 516.0 | 679.0 | 700.0 | 280.0 | 701.0 | . pop_mn_70_ 281.0 | 610.0 | 443.0 | 48.0 | 278.0 | 10.0 | 23.0 | 31.0 | 217.0 | 175.0 | ... | 684.0 | 271.0 | 247.0 | 261.0 | 449.0 | 262.0 | 323.0 | 423.0 | 151.0 | 360.0 | . pop_mn_80_ 86.0 | 207.0 | 122.0 | 8.0 | 92.0 | 4.0 | 18.0 | 4.0 | 69.0 | 51.0 | ... | 281.0 | 104.0 | 112.0 | 105.0 | 201.0 | 143.0 | 143.0 | 182.0 | 70.0 | 151.0 | . pop_mn_90_ 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 1.0 | 2.0 | 1.0 | 0.0 | 3.0 | 0.0 | 4.0 | 0.0 | 2.0 | . pop_wmn_0_ 817.0 | 2670.0 | 3202.0 | 364.0 | 2200.0 | 3.0 | 4.0 | 10.0 | 731.0 | 236.0 | ... | 1878.0 | 233.0 | 126.0 | 144.0 | 264.0 | 87.0 | 179.0 | 211.0 | 60.0 | 215.0 | . pop_wmn_10 747.0 | 3425.0 | 2298.0 | 255.0 | 1719.0 | 2.0 | 10.0 | 26.0 | 888.0 | 319.0 | ... | 1576.0 | 250.0 | 169.0 | 179.0 | 396.0 | 146.0 | 254.0 | 320.0 | 78.0 | 389.0 | . pop_wmn_20 1697.0 | 2726.0 | 1992.0 | 282.0 | 1664.0 | 5.0 | 21.0 | 29.0 | 650.0 | 381.0 | ... | 1949.0 | 395.0 | 240.0 | 255.0 | 430.0 | 206.0 | 342.0 | 364.0 | 140.0 | 461.0 | . pop_wmn_30 1869.0 | 3786.0 | 4339.0 | 557.0 | 2947.0 | 10.0 | 16.0 | 15.0 | 942.0 | 396.0 | ... | 2755.0 | 362.0 | 193.0 | 221.0 | 386.0 | 187.0 | 346.0 | 360.0 | 121.0 | 384.0 | . pop_wmn_40 1380.0 | 4725.0 | 3604.0 | 381.0 | 2542.0 | 10.0 | 23.0 | 37.0 | 1293.0 | 529.0 | ... | 2529.0 | 465.0 | 277.0 | 286.0 | 608.0 | 262.0 | 456.0 | 448.0 | 125.0 | 614.0 | . pop_wmn_50 1091.0 | 2523.0 | 1739.0 | 257.0 | 1839.0 | 16.0 | 37.0 | 46.0 | 830.0 | 563.0 | ... | 2500.0 | 660.0 | 469.0 | 476.0 | 851.0 | 504.0 | 770.0 | 726.0 | 274.0 | 841.0 | . pop_wmn_60 600.0 | 1389.0 | 1083.0 | 122.0 | 800.0 | 12.0 | 45.0 | 42.0 | 472.0 | 328.0 | ... | 1523.0 | 506.0 | 365.0 | 381.0 | 717.0 | 418.0 | 556.0 | 618.0 | 247.0 | 586.0 | . pop_wmn_70 401.0 | 877.0 | 579.0 | 50.0 | 278.0 | 15.0 | 38.0 | 24.0 | 262.0 | 208.0 | ... | 820.0 | 355.0 | 291.0 | 246.0 | 524.0 | 285.0 | 343.0 | 475.0 | 161.0 | 415.0 | . pop_wmn_80 174.0 | 322.0 | 234.0 | 25.0 | 173.0 | 4.0 | 27.0 | 27.0 | 117.0 | 96.0 | ... | 465.0 | 195.0 | 199.0 | 153.0 | 323.0 | 215.0 | 249.0 | 279.0 | 135.0 | 292.0 | . pop_wmn_90 0.0 | 2.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 3.0 | 1.0 | 0.0 | 1.0 | 8.0 | 4.0 | 7.0 | 4.0 | 1.0 | 5.0 | . 20 rows × 37 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Te = pop_total.T.drop([&#39;index&#39;], axis = 0) #인덱스 제거 Te . emd_cd 41590116 41590117 41590118 41590119 41590120 41590121 41590122 41590123 41590124 41590125 ... 41590262 41590310 41590320 41590330 41590340 41590350 41590360 41590370 41590400 41590410 . pop_mn_0_9 916.0 | 2820.0 | 3295.0 | 321.0 | 2284.0 | 2.0 | 8.0 | 10.0 | 824.0 | 242.0 | ... | 1997.0 | 241.0 | 122.0 | 151.0 | 285.0 | 92.0 | 196.0 | 254.0 | 67.0 | 269.0 | . pop_mn_10_ 846.0 | 3446.0 | 2498.0 | 229.0 | 1835.0 | 3.0 | 18.0 | 27.0 | 1012.0 | 333.0 | ... | 1735.0 | 286.0 | 198.0 | 213.0 | 426.0 | 174.0 | 259.0 | 333.0 | 99.0 | 408.0 | . pop_mn_20_ 2049.0 | 2855.0 | 2074.0 | 289.0 | 1853.0 | 11.0 | 35.0 | 41.0 | 711.0 | 489.0 | ... | 2633.0 | 528.0 | 364.0 | 385.0 | 598.0 | 301.0 | 647.0 | 559.0 | 222.0 | 732.0 | . pop_mn_30_ 3034.0 | 3646.0 | 4207.0 | 636.0 | 2794.0 | 9.0 | 22.0 | 36.0 | 848.0 | 518.0 | ... | 3635.0 | 426.0 | 320.0 | 405.0 | 511.0 | 278.0 | 605.0 | 538.0 | 231.0 | 631.0 | . pop_mn_40_ 2042.0 | 4718.0 | 3805.0 | 407.0 | 2752.0 | 17.0 | 35.0 | 45.0 | 1374.0 | 630.0 | ... | 3211.0 | 620.0 | 410.0 | 492.0 | 804.0 | 421.0 | 860.0 | 773.0 | 270.0 | 931.0 | . pop_mn_50_ 1282.0 | 2780.0 | 1815.0 | 250.0 | 1796.0 | 25.0 | 67.0 | 93.0 | 910.0 | 566.0 | ... | 2885.0 | 775.0 | 644.0 | 631.0 | 1063.0 | 641.0 | 1120.0 | 1056.0 | 444.0 | 1137.0 | . pop_mn_60_ 600.0 | 1207.0 | 820.0 | 128.0 | 847.0 | 15.0 | 50.0 | 59.0 | 450.0 | 352.0 | ... | 1637.0 | 524.0 | 481.0 | 434.0 | 751.0 | 516.0 | 679.0 | 700.0 | 280.0 | 701.0 | . pop_mn_70_ 281.0 | 610.0 | 443.0 | 48.0 | 278.0 | 10.0 | 23.0 | 31.0 | 217.0 | 175.0 | ... | 684.0 | 271.0 | 247.0 | 261.0 | 449.0 | 262.0 | 323.0 | 423.0 | 151.0 | 360.0 | . pop_mn_80_ 86.0 | 207.0 | 122.0 | 8.0 | 92.0 | 4.0 | 18.0 | 4.0 | 69.0 | 51.0 | ... | 281.0 | 104.0 | 112.0 | 105.0 | 201.0 | 143.0 | 143.0 | 182.0 | 70.0 | 151.0 | . pop_mn_90_ 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 0.0 | 1.0 | 2.0 | 1.0 | 0.0 | 3.0 | 0.0 | 4.0 | 0.0 | 2.0 | . pop_wmn_0_ 817.0 | 2670.0 | 3202.0 | 364.0 | 2200.0 | 3.0 | 4.0 | 10.0 | 731.0 | 236.0 | ... | 1878.0 | 233.0 | 126.0 | 144.0 | 264.0 | 87.0 | 179.0 | 211.0 | 60.0 | 215.0 | . pop_wmn_10 747.0 | 3425.0 | 2298.0 | 255.0 | 1719.0 | 2.0 | 10.0 | 26.0 | 888.0 | 319.0 | ... | 1576.0 | 250.0 | 169.0 | 179.0 | 396.0 | 146.0 | 254.0 | 320.0 | 78.0 | 389.0 | . pop_wmn_20 1697.0 | 2726.0 | 1992.0 | 282.0 | 1664.0 | 5.0 | 21.0 | 29.0 | 650.0 | 381.0 | ... | 1949.0 | 395.0 | 240.0 | 255.0 | 430.0 | 206.0 | 342.0 | 364.0 | 140.0 | 461.0 | . pop_wmn_30 1869.0 | 3786.0 | 4339.0 | 557.0 | 2947.0 | 10.0 | 16.0 | 15.0 | 942.0 | 396.0 | ... | 2755.0 | 362.0 | 193.0 | 221.0 | 386.0 | 187.0 | 346.0 | 360.0 | 121.0 | 384.0 | . pop_wmn_40 1380.0 | 4725.0 | 3604.0 | 381.0 | 2542.0 | 10.0 | 23.0 | 37.0 | 1293.0 | 529.0 | ... | 2529.0 | 465.0 | 277.0 | 286.0 | 608.0 | 262.0 | 456.0 | 448.0 | 125.0 | 614.0 | . pop_wmn_50 1091.0 | 2523.0 | 1739.0 | 257.0 | 1839.0 | 16.0 | 37.0 | 46.0 | 830.0 | 563.0 | ... | 2500.0 | 660.0 | 469.0 | 476.0 | 851.0 | 504.0 | 770.0 | 726.0 | 274.0 | 841.0 | . pop_wmn_60 600.0 | 1389.0 | 1083.0 | 122.0 | 800.0 | 12.0 | 45.0 | 42.0 | 472.0 | 328.0 | ... | 1523.0 | 506.0 | 365.0 | 381.0 | 717.0 | 418.0 | 556.0 | 618.0 | 247.0 | 586.0 | . pop_wmn_70 401.0 | 877.0 | 579.0 | 50.0 | 278.0 | 15.0 | 38.0 | 24.0 | 262.0 | 208.0 | ... | 820.0 | 355.0 | 291.0 | 246.0 | 524.0 | 285.0 | 343.0 | 475.0 | 161.0 | 415.0 | . pop_wmn_80 174.0 | 322.0 | 234.0 | 25.0 | 173.0 | 4.0 | 27.0 | 27.0 | 117.0 | 96.0 | ... | 465.0 | 195.0 | 199.0 | 153.0 | 323.0 | 215.0 | 249.0 | 279.0 | 135.0 | 292.0 | . pop_wmn_90 0.0 | 2.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | ... | 3.0 | 1.0 | 0.0 | 1.0 | 8.0 | 4.0 | 7.0 | 4.0 | 1.0 | 5.0 | . 20 rows × 37 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Te.index = [&quot;10대 미만 남자&quot;, &quot;10대 남자&quot;, &quot;20대 남자&quot;, &quot;30대 남자&quot;, &quot;40대 남자&quot;, &quot;50대 남자&quot;, &quot;60대 남자&quot;, &quot;70대 남자&quot;,&quot;80대 남자&quot;,&quot;90대 남자&quot;, &quot;10대 미만 여자&quot;, &quot;10대 여자&quot;, &quot;20대 여자&quot;, &quot;30대 여자&quot;, &quot;40대 여자&quot;, &quot;50대 여자&quot;, &quot;60대 여자&quot;, &quot;70대 여자&quot;,&quot;80대 여자&quot;,&quot;90대 여자&quot;] # 인덱스의 이름을 각 나이대별 성별로 지정 . 시각화에서 사용한 범례 | . . 인구수 1번 그래프 - 향남읍 | . Te_1 =pd.DataFrame(Te.sort_values(by = &#39;41590259&#39;, ascending = False)[&#39;41590259&#39;]) #인구수가 가장 많은 향남읍의 인구 분포 plt.rcParams[&#39;font.size&#39;] = 25 # 글씨 크기 설정 fig = plt.figure(figsize=(25,12)) # 캔버스 생성 circle_1 = plt.pie(Te_1[&#39;41590259&#39;],labels = Te.index,autopct= &#39;%1.1f%%&#39;, explode = [0.1,0.1,0.1,0.1,0.1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]) # 중앙에서 떨어진 정도를 설정해서 인구 분포가 많은 나이대 강조 #plt.legend(loc = (1, 1)) # 원래 범례를 사용하였지만 하나의 범례를 사용하기 때문에 일단 그래프에서 범례 제거 plt.title(&#39;향남읍&#39;, size = 40) # 제목 크기 . Text(0.5, 1.0, &#39;향남읍&#39;) . 하위 1번 그래프 -신동 | . Te_4 =pd.DataFrame(Te.sort_values(by = &#39;41590133&#39;, ascending = False)[&#39;41590133&#39;]) # 인구수가 가장 적은 신동의 인구 분포 plt.rcParams[&#39;font.size&#39;] = 25 # 글씨 크기 설정 fig = plt.figure(figsize=(25,12)) # 캔버스 생성 circle_4 = plt.pie(Te_4[&#39;41590133&#39;],labels = Te.index,autopct= &#39;%1.1f%%&#39;, explode = [0.1,0.1,0.1,0.1,0.1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]) # 중앙에서 떨어진 정도를 설정해서 인구 분포가 많은 나이대 강조 #plt.legend(loc = (1, 1)) # 원래 범례를 사용하였지만 하나의 범례를 사용하기 때문에 일단 그래프에서 범례 제거 plt.title(&#39;신동&#39;, size = 40) # 제목 크기 . Text(0.5, 1.0, &#39;신동&#39;) . 기본적으로 연령대 별 인구 분포에 따라 이용자 수가 나뉘는 것이 아님을 확인하고 신도시이기 때문에 영향을 받는다고 판단할 수 있다. . 10-50대가 주로 이용을 한다면 학생들이 학교에 통학을 하거나 직장인이 직장으로 출퇴근을 하는 목적으로 이용할 것이라고 생각하였고 확인해보고자 아래의 코드를 실행하였습니다. | . | . 출근 시간 시각화 자료 위의 코드 실행 시 한글 글꼴이 적용되지 않아 첨부한 원본 그래프 사진 | . | . . 실제로 그래프를 보면 출근시간인 7-8시, 퇴근시간인 17-18시에 이용자 수가 가장 많은 것을 볼 수 있습니다. | . 환승횟수별 시각화 | . sns.countplot(data = TripChain, x = &quot;&#39;||환승횟수||&#39;&quot;) . &lt;AxesSubplot:xlabel=&#34;&#39;||환승횟수||&#39;&#34;, ylabel=&#39;count&#39;&gt; . 기본 한번의 환승과 두 세번을 하는 경우도 적지 않게 일어났고 소수의 인원은 네번에서 다섯번까지 환승을 하여 버스를 이용하였습니다. | . emd 시각화 | . 어떤 경우에서 환승을 하였는지 알아보기 위해 화성시의 인구 분포와 전체 버스 정류장의 위치를 지도에 표시해보았습니다. | . districts = emd_scco_geo[[&quot;id&quot;, &quot;geometry&quot;]].set_index(&quot;id&quot;) #id를 인덱스로 지정 districts.head() #5개의 데이터로 미리보기 . geometry . id . 2122 MULTIPOLYGON (((126.99921 37.12304, 126.99906 ... | . 2123 MULTIPOLYGON (((127.00591 37.18681, 127.00595 ... | . 2087 MULTIPOLYGON (((127.03486 37.22713, 127.03491 ... | . 2088 MULTIPOLYGON (((127.02686 37.20033, 127.02680 ... | . 2089 MULTIPOLYGON (((127.05947 37.19984, 127.05948 ... | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; plot_dict = emd_scco_geo.id.value_counts() # id값 추출 plot_dict.head() #5개의 데이터로 미리보기 . 2122 1 2104 1 2106 1 2107 1 2108 1 Name: id, dtype: int64 . emd_geo = new_emd_sort[&#39;geometry&#39;] #geometry 정보 저장 new_emd_sort[&#39;pop_ttl&#39;] = new_emd_sort[&#39;pop_ttl&#39;].astype(int) # int형으로 변환 . emd_select = new_emd_sort[[&#39;id&#39;, &#39;pop_ttl&#39;]] #new_emd_sort 에서 id와 인구수 뽑기 . m = folium.Map(location=[36, 127], tiles=&quot;OpenStreetMap&quot;, zoom_start=7) # 기본 지도 설정 m.choropleth( geo_data=emd_scco_geo, #geometry 정보를 담은 데이터셋을 설정 name=&#39;인구수&#39;, data=emd_select, # 지도에 표시되는 데이터셋 지정 columns=[&#39;id&#39;, &#39;pop_ttl&#39;], #id와 인구수 사용 key_on=&#39;feature.properties.id&#39;, #id별로 사용 color=&#39;grey&#39;, fill_opacity=0.7, line_opacity=0.3, fill_color = &#39;PuRd&#39; #크기별 구분 색깔 ) folium.LayerControl(collapsed=False).add_to(m) # 지도에 위에서 크기별로 색깔 변화준 것을 표시 m . /usr/local/lib/python3.7/dist-packages/folium/folium.py:426: FutureWarning: The choropleth method has been deprecated. Instead use the new Choropleth class, which has the same arguments. See the example notebook &#39;GeoJSON_and_choropleth&#39; for how to do this. . Make this Notebook Trusted to load map: File -&gt; Trust Notebook 인구 수가 중심구역과 동쪽 지역에 밀집되어 있고 서쪽에는 특별히 인구가 많은 구역이 없습니다. | . 전체 버스정류장 시각화 | . for idx, row in station_table.iterrows(): folium.Marker([row[&#39;WGS84위도&#39;], row[&#39;WGS84경도&#39;]]).add_to(m) # 위도 경도 표시 m.add_child(folium.LatLngPopup()) # 위도 경도 마커를 지도에 표시 . Make this Notebook Trusted to load map: File -&gt; Trust Notebook 두개의 지도를 살펴보면 인구수에 거의 비례하게 중심과 동쪽에는 인구수도 많고 신도시의 영향으로 버스 정류장의 수도 많은데에 비해 서쪽에는 버스 정류장의 수가 적은 것을 볼 수 있습니다. | . emd_name_cd = pd.DataFrame(new_emd.loc[:,[&#39;emd_cd&#39;,&#39;emd_kor_nm&#39;]]) #읍면동 코드와 이름으로 새로운 데이터셋 생성 emd_name_cd = emd_name_cd.set_index(&#39;emd_cd&#39;) # 코드를 인덱스로 지정 . emd_name_cd[&#39;index&#39;] = emd_name_cd.index # 코드를 열로 생성 . vis_new_data = pd.merge(pop_test, emd_name_cd, left_index = True,right_index = True) # pop_test, emd_name_cd 를 합치기 vis_new_data . pop_mn_0_9 pop_mn_10_ pop_mn_20_ pop_mn_30_ pop_mn_40_ pop_mn_50_ pop_mn_60_ pop_mn_70_ pop_mn_80_ pop_mn_90_ ... pop_wmn_30 pop_wmn_40 pop_wmn_50 pop_wmn_60 pop_wmn_70 pop_wmn_80 pop_wmn_90 pop_wmn_tt pop_ttl emd_kor_nm . emd_cd . 41590116 916.0 | 846.0 | 2049.0 | 3034.0 | 2042.0 | 1282.0 | 600.0 | 281.0 | 86.0 | 1.0 | ... | 1869.0 | 1380.0 | 1091.0 | 600.0 | 401.0 | 174.0 | 0.0 | 8806.0 | 19956.0 | 진안동 | . 41590117 2820.0 | 3446.0 | 2855.0 | 3646.0 | 4718.0 | 2780.0 | 1207.0 | 610.0 | 207.0 | 0.0 | ... | 3786.0 | 4725.0 | 2523.0 | 1389.0 | 877.0 | 322.0 | 2.0 | 22507.0 | 44812.0 | 병점동 | . 41590118 3295.0 | 2498.0 | 2074.0 | 4207.0 | 3805.0 | 1815.0 | 820.0 | 443.0 | 122.0 | 0.0 | ... | 4339.0 | 3604.0 | 1739.0 | 1083.0 | 579.0 | 234.0 | 1.0 | 19112.0 | 38207.0 | 능동 | . 41590119 321.0 | 229.0 | 289.0 | 636.0 | 407.0 | 250.0 | 128.0 | 48.0 | 8.0 | 0.0 | ... | 557.0 | 381.0 | 257.0 | 122.0 | 50.0 | 25.0 | 0.0 | 2300.0 | 4617.0 | 기산동 | . 41590120 2284.0 | 1835.0 | 1853.0 | 2794.0 | 2752.0 | 1796.0 | 847.0 | 278.0 | 92.0 | 1.0 | ... | 2947.0 | 2542.0 | 1839.0 | 800.0 | 278.0 | 173.0 | 0.0 | 14197.0 | 28740.0 | 반월동 | . 41590121 2.0 | 3.0 | 11.0 | 9.0 | 17.0 | 25.0 | 15.0 | 10.0 | 4.0 | 0.0 | ... | 10.0 | 10.0 | 16.0 | 12.0 | 15.0 | 4.0 | 0.0 | 77.0 | 175.0 | 반정동 | . 41590122 8.0 | 18.0 | 35.0 | 22.0 | 35.0 | 67.0 | 50.0 | 23.0 | 18.0 | 0.0 | ... | 16.0 | 23.0 | 37.0 | 45.0 | 38.0 | 27.0 | 0.0 | 225.0 | 503.0 | 황계동 | . 41590123 10.0 | 27.0 | 41.0 | 36.0 | 45.0 | 93.0 | 59.0 | 31.0 | 4.0 | 0.0 | ... | 15.0 | 37.0 | 46.0 | 42.0 | 24.0 | 27.0 | 0.0 | 261.0 | 608.0 | 배양동 | . 41590124 824.0 | 1012.0 | 711.0 | 848.0 | 1374.0 | 910.0 | 450.0 | 217.0 | 69.0 | 1.0 | ... | 942.0 | 1293.0 | 830.0 | 472.0 | 262.0 | 117.0 | 1.0 | 6201.0 | 12620.0 | 기안동 | . 41590125 242.0 | 333.0 | 489.0 | 518.0 | 630.0 | 566.0 | 352.0 | 175.0 | 51.0 | 0.0 | ... | 396.0 | 529.0 | 563.0 | 328.0 | 208.0 | 96.0 | 0.0 | 3066.0 | 6425.0 | 송산동 | . 41590126 534.0 | 515.0 | 805.0 | 861.0 | 947.0 | 881.0 | 510.0 | 190.0 | 59.0 | 0.0 | ... | 769.0 | 755.0 | 876.0 | 500.0 | 220.0 | 100.0 | 0.0 | 4706.0 | 10010.0 | 안녕동 | . 41590127 4959.0 | 5710.0 | 3846.0 | 5652.0 | 7838.0 | 4295.0 | 1415.0 | 680.0 | 197.0 | 1.0 | ... | 6492.0 | 7978.0 | 3585.0 | 1619.0 | 868.0 | 350.0 | 3.0 | 35132.0 | 69743.0 | 반송동 | . 41590128 1070.0 | 1258.0 | 996.0 | 1295.0 | 1771.0 | 945.0 | 227.0 | 111.0 | 24.0 | 0.0 | ... | 1300.0 | 1697.0 | 714.0 | 258.0 | 130.0 | 57.0 | 0.0 | 7279.0 | 14977.0 | 석우동 | . 41590129 1003.0 | 556.0 | 809.0 | 1798.0 | 1050.0 | 626.0 | 309.0 | 106.0 | 26.0 | 0.0 | ... | 1618.0 | 971.0 | 607.0 | 338.0 | 108.0 | 61.0 | 0.0 | 6067.0 | 12353.0 | 오산동 | . 41590130 5468.0 | 4225.0 | 2154.0 | 4795.0 | 6751.0 | 2695.0 | 868.0 | 299.0 | 78.0 | 0.0 | ... | 6118.0 | 6103.0 | 2227.0 | 940.0 | 413.0 | 173.0 | 0.0 | 27644.0 | 54986.0 | 청계동 | . 41590131 2718.0 | 1582.0 | 1470.0 | 4136.0 | 3099.0 | 1464.0 | 765.0 | 348.0 | 83.0 | 0.0 | ... | 4109.0 | 2798.0 | 1500.0 | 941.0 | 415.0 | 184.0 | 0.0 | 15882.0 | 31558.0 | 영천동 | . 41590132 2.0 | 4.0 | 19.0 | 14.0 | 8.0 | 25.0 | 25.0 | 6.0 | 2.0 | 0.0 | ... | 6.0 | 10.0 | 26.0 | 15.0 | 6.0 | 5.0 | 0.0 | 95.0 | 200.0 | 중동 | . 41590133 0.0 | 1.0 | 3.0 | 1.0 | 4.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | ... | 5.0 | 0.0 | 2.0 | 0.0 | 0.0 | 0.0 | 0.0 | 9.0 | 20.0 | 신동 | . 41590134 1159.0 | 515.0 | 574.0 | 1679.0 | 1040.0 | 504.0 | 281.0 | 95.0 | 16.0 | 0.0 | ... | 1632.0 | 941.0 | 579.0 | 282.0 | 90.0 | 32.0 | 1.0 | 5661.0 | 11525.0 | 목동 | . 41590135 403.0 | 245.0 | 308.0 | 657.0 | 435.0 | 252.0 | 130.0 | 42.0 | 6.0 | 0.0 | ... | 586.0 | 429.0 | 287.0 | 144.0 | 55.0 | 27.0 | 0.0 | 2488.0 | 4966.0 | 산척동 | . 41590136 561.0 | 266.0 | 328.0 | 892.0 | 573.0 | 313.0 | 194.0 | 65.0 | 15.0 | 0.0 | ... | 822.0 | 504.0 | 342.0 | 209.0 | 63.0 | 40.0 | 0.0 | 3114.0 | 6323.0 | 장지동 | . 41590137 633.0 | 220.0 | 195.0 | 819.0 | 503.0 | 190.0 | 92.0 | 48.0 | 12.0 | 0.0 | ... | 808.0 | 397.0 | 201.0 | 114.0 | 37.0 | 16.0 | 0.0 | 2540.0 | 5254.0 | 송동 | . 41590138 91.0 | 14.0 | 134.0 | 341.0 | 105.0 | 64.0 | 34.0 | 15.0 | 5.0 | 0.0 | ... | 202.0 | 65.0 | 67.0 | 35.0 | 13.0 | 6.0 | 0.0 | 606.0 | 1410.0 | 방교동 | . 41590139 24.0 | 35.0 | 61.0 | 48.0 | 56.0 | 94.0 | 53.0 | 31.0 | 12.0 | 0.0 | ... | 30.0 | 47.0 | 68.0 | 36.0 | 33.0 | 20.0 | 2.0 | 322.0 | 737.0 | 금곡동 | . 41590253 4235.0 | 4794.0 | 4404.0 | 5372.0 | 7580.0 | 5464.0 | 2813.0 | 1376.0 | 356.0 | 5.0 | ... | 5705.0 | 6999.0 | 4872.0 | 2962.0 | 1601.0 | 753.0 | 9.0 | 35989.0 | 72421.0 | 봉담읍 | . 41590256 595.0 | 743.0 | 1084.0 | 1001.0 | 1570.0 | 1607.0 | 1022.0 | 572.0 | 242.0 | 3.0 | ... | 786.0 | 1097.0 | 1292.0 | 981.0 | 665.0 | 434.0 | 2.0 | 7347.0 | 15810.0 | 우정읍 | . 41590259 5743.0 | 4882.0 | 4778.0 | 9042.0 | 8587.0 | 4990.0 | 2381.0 | 1162.0 | 374.0 | 5.0 | ... | 7489.0 | 6884.0 | 4186.0 | 2485.0 | 1420.0 | 769.0 | 3.0 | 37603.0 | 79581.0 | 향남읍 | . 41590262 1997.0 | 1735.0 | 2633.0 | 3635.0 | 3211.0 | 2885.0 | 1637.0 | 684.0 | 281.0 | 0.0 | ... | 2755.0 | 2529.0 | 2500.0 | 1523.0 | 820.0 | 465.0 | 3.0 | 16101.0 | 34815.0 | 남양읍 | . 41590310 241.0 | 286.0 | 528.0 | 426.0 | 620.0 | 775.0 | 524.0 | 271.0 | 104.0 | 1.0 | ... | 362.0 | 465.0 | 660.0 | 506.0 | 355.0 | 195.0 | 1.0 | 3448.0 | 7238.0 | 매송면 | . 41590320 122.0 | 198.0 | 364.0 | 320.0 | 410.0 | 644.0 | 481.0 | 247.0 | 112.0 | 2.0 | ... | 193.0 | 277.0 | 469.0 | 365.0 | 291.0 | 199.0 | 0.0 | 2368.0 | 5277.0 | 비봉면 | . 41590330 151.0 | 213.0 | 385.0 | 405.0 | 492.0 | 631.0 | 434.0 | 261.0 | 105.0 | 1.0 | ... | 221.0 | 286.0 | 476.0 | 381.0 | 246.0 | 153.0 | 1.0 | 2365.0 | 5448.0 | 마도면 | . 41590340 285.0 | 426.0 | 598.0 | 511.0 | 804.0 | 1063.0 | 751.0 | 449.0 | 201.0 | 0.0 | ... | 386.0 | 608.0 | 851.0 | 717.0 | 524.0 | 323.0 | 8.0 | 4581.0 | 9695.0 | 송산면 | . 41590350 92.0 | 174.0 | 301.0 | 278.0 | 421.0 | 641.0 | 516.0 | 262.0 | 143.0 | 3.0 | ... | 187.0 | 262.0 | 504.0 | 418.0 | 285.0 | 215.0 | 4.0 | 2355.0 | 5205.0 | 서신면 | . 41590360 196.0 | 259.0 | 647.0 | 605.0 | 860.0 | 1120.0 | 679.0 | 323.0 | 143.0 | 0.0 | ... | 346.0 | 456.0 | 770.0 | 556.0 | 343.0 | 249.0 | 7.0 | 3546.0 | 8392.0 | 팔탄면 | . 41590370 254.0 | 333.0 | 559.0 | 538.0 | 773.0 | 1056.0 | 700.0 | 423.0 | 182.0 | 4.0 | ... | 360.0 | 448.0 | 726.0 | 618.0 | 475.0 | 279.0 | 4.0 | 3852.0 | 8690.0 | 장안면 | . 41590400 67.0 | 99.0 | 222.0 | 231.0 | 270.0 | 444.0 | 280.0 | 151.0 | 70.0 | 0.0 | ... | 121.0 | 125.0 | 274.0 | 247.0 | 161.0 | 135.0 | 1.0 | 1354.0 | 3193.0 | 양감면 | . 41590410 269.0 | 408.0 | 732.0 | 631.0 | 931.0 | 1137.0 | 701.0 | 360.0 | 151.0 | 2.0 | ... | 384.0 | 614.0 | 841.0 | 586.0 | 415.0 | 292.0 | 5.0 | 4252.0 | 9586.0 | 정남면 | . 37 rows × 24 columns . 분석을 진행하던 중 station_table 과 cell_pop 의 읍면동 이름이 다른 것을 확인 | . station_table[&#39;emd&#39;].unique() # station_table 의 읍면동 이름 . array([&#39;석우동&#39;, &#39;봉담읍&#39;, &#39;향남읍&#39;, &#39;양감면&#39;, &#39;정남면&#39;, &#39;병점동&#39;, &#39;진안동&#39;, &#39;능동&#39;, &#39;기산동&#39;, &#39;반월동&#39;, &#39;동탄면&#39;, &#39;송산면&#39;, &#39;팔탄면&#39;, &#39;비봉면&#39;, &#39;서신면&#39;, &#39;우정읍&#39;, &#39;장안면&#39;, &#39;매송면&#39;, &#39;안녕동&#39;, &#39;송산동&#39;, &#39;기안동&#39;, &#39;황계동&#39;, &#39;반정동&#39;, &#39;마도면&#39;, &#39;반송동&#39;, &#39;서탄면&#39;, &#39;시 탑동&#39;, &#39;오산동&#39;, &#39;영천동&#39;, &#39;신장동&#39;], dtype=object) . vis_new_data[&#39;emd_kor_nm&#39;].unique() # vis_new_data 의 읍면동 이름 . array([&#39;진안동&#39;, &#39;병점동&#39;, &#39;능동&#39;, &#39;기산동&#39;, &#39;반월동&#39;, &#39;반정동&#39;, &#39;황계동&#39;, &#39;배양동&#39;, &#39;기안동&#39;, &#39;송산동&#39;, &#39;안녕동&#39;, &#39;반송동&#39;, &#39;석우동&#39;, &#39;오산동&#39;, &#39;청계동&#39;, &#39;영천동&#39;, &#39;중동&#39;, &#39;신동&#39;, &#39;목동&#39;, &#39;산척동&#39;, &#39;장지동&#39;, &#39;송동&#39;, &#39;방교동&#39;, &#39;금곡동&#39;, &#39;봉담읍&#39;, &#39;우정읍&#39;, &#39;향남읍&#39;, &#39;남양읍&#39;, &#39;매송면&#39;, &#39;비봉면&#39;, &#39;마도면&#39;, &#39;송산면&#39;, &#39;서신면&#39;, &#39;팔탄면&#39;, &#39;장안면&#39;, &#39;양감면&#39;, &#39;정남면&#39;], dtype=object) . 배양동(기안동), 청계동(동탄4동), 중동(동탄5동), 신동(동탄7동), 목동(동탄7동), 산척동(동탄7동), 장지동(동탄7동), 송동(동탄7동), 방교동(동탄6동), 금곡동(동탄6동), 남양읍(미포함) | station_table 과 cell_pop에서 읍면동 이름이 다른 이유는 동탄으로 읍면동이 계속해서 변경되었기 때문에 발생한 문제였습니다. -&gt; 크게 문제가 되지는 않다고 생각하여 그대로 진행하였습니다. | . 전체 버스 정류장에 대한 매핑 정보를 마커보다 보기 쉽게 읍면동별로 점으로 표시해보았습니다. | . fig = px.scatter_mapbox(station_table, lat = &#39;WGS84위도&#39;, lon = &#39;WGS84경도&#39;,# 위도 경도 설정 color = &#39;emd&#39;, # 읍면동별로 색상 설정 opacity = 0.4, #투명도 hover_name = &#39;정류소명&#39;, #정류소 명 표시 hover_data = { &#39;WGS84위도&#39; : False, &#39;WGS84경도&#39; : False, &#39;emd&#39; : True # 읍면동 표시 }, mapbox_style = &quot;carto-positron&quot;, # 기본 지도 설정 center = dict(lat = 37.15, lon = 126.9), # 지도 중심 설정 zoom = 9) # 확대 정도 fig.show() # 지도 보기 . . . with open(&#39;C: Users 82108 Desktop data mining tl_scco_emd.geojson&#39;, encoding=&#39;UTF-8&#39;) as f: geo_data = json.load(f) # 지도 시각화에 사용하기 위해 geojson 으로 파일 오픈 geo_data . for x in geo_data[&#39;features&#39;]: x[&#39;id&#39;] = x[&#39;properties&#39;][&#39;emd_kor_nm&#39;] for idx, _ in enumerate(geo_data[&#39;features&#39;]): print(geo_data[&#39;features&#39;][idx][&#39;id&#39;]) # 지도 시각화를 위한 설정 . 양감면 정남면 진안동 병점동 능동 기산동 반월동 반정동 황계동 배양동 기안동 송산동 안녕동 반송동 석우동 오산동 청계동 영천동 중동 신동 목동 산척동 장지동 송동 방교동 금곡동 봉담읍 우정읍 향남읍 남양읍 매송면 비봉면 마도면 송산면 서신면 팔탄면 장안면 . emd_scco_geo[&#39;id_key&#39;] = (emd_scco_geo.index)*10 #구분을 짓기 위해 index에 10을 곱한 id_key 열 생성 emd_scco_geo . id emd_cd emd_eng_nm emd_kor_nm geometry id_key . 0 2122 | 41590400 | Yanggam-myeon | 양감면 | MULTIPOLYGON (((126.99921 37.12304, 126.99906 ... | 0 | . 1 2123 | 41590410 | Jeongnam-myeon | 정남면 | MULTIPOLYGON (((127.00591 37.18681, 127.00595 ... | 10 | . 2 2087 | 41590116 | Jinan-dong | 진안동 | MULTIPOLYGON (((127.03486 37.22713, 127.03491 ... | 20 | . 3 2088 | 41590117 | Byeongjeom-dong | 병점동 | MULTIPOLYGON (((127.02686 37.20033, 127.02680 ... | 30 | . 4 2089 | 41590118 | Neung-dong | 능동 | MULTIPOLYGON (((127.05947 37.19984, 127.05948 ... | 40 | . 5 2090 | 41590119 | Gisan-dong | 기산동 | MULTIPOLYGON (((127.05295 37.21400, 127.05278 ... | 50 | . 6 2091 | 41590120 | Banwol-dong | 반월동 | MULTIPOLYGON (((127.06647 37.24009, 127.06650 ... | 60 | . 7 2092 | 41590121 | Banjeong-dong | 반정동 | MULTIPOLYGON (((127.04253 37.24702, 127.04263 ... | 70 | . 8 2093 | 41590122 | Hwanggye-dong | 황계동 | MULTIPOLYGON (((127.02352 37.22536, 127.02611 ... | 80 | . 9 2094 | 41590123 | Baeyang-dong | 배양동 | MULTIPOLYGON (((126.99364 37.23062, 126.99370 ... | 90 | . 10 2095 | 41590124 | Gian-dong | 기안동 | MULTIPOLYGON (((126.98181 37.21318, 126.98172 ... | 100 | . 11 2096 | 41590125 | Songsan-dong | 송산동 | MULTIPOLYGON (((127.02429 37.20579, 127.02426 ... | 110 | . 12 2097 | 41590126 | Annyeong-dong | 안녕동 | MULTIPOLYGON (((127.00591 37.18681, 127.00592 ... | 120 | . 13 2098 | 41590127 | Bansong-dong | 반송동 | MULTIPOLYGON (((127.08299 37.18425, 127.08290 ... | 130 | . 14 2099 | 41590128 | Seogu-dong | 석우동 | MULTIPOLYGON (((127.08799 37.21660, 127.08810 ... | 140 | . 15 2100 | 41590129 | Osan-dong | 오산동 | MULTIPOLYGON (((127.08299 37.18425, 127.08300 ... | 150 | . 16 2101 | 41590130 | Cheonggye-dong | 청계동 | MULTIPOLYGON (((127.13001 37.20120, 127.13034 ... | 160 | . 17 2102 | 41590131 | Yeongcheon-dong | 영천동 | MULTIPOLYGON (((127.08583 37.20471, 127.08586 ... | 170 | . 18 2103 | 41590132 | Jung-dong | 중동 | MULTIPOLYGON (((127.15087 37.21984, 127.15099 ... | 180 | . 19 2104 | 41590133 | Sin-dong | 신동 | MULTIPOLYGON (((127.15228 37.19841, 127.15231 ... | 190 | . 20 2105 | 41590134 | Mok-dong | 목동 | MULTIPOLYGON (((127.13150 37.19569, 127.13150 ... | 200 | . 21 2106 | 41590135 | Sancheok-dong | 산척동 | MULTIPOLYGON (((127.11354 37.17947, 127.11375 ... | 210 | . 22 2107 | 41590136 | Jangji-dong | 장지동 | MULTIPOLYGON (((127.12891 37.16213, 127.12901 ... | 220 | . 23 2108 | 41590137 | Song-dong | 송동 | MULTIPOLYGON (((127.10469 37.17752, 127.10473 ... | 230 | . 24 2109 | 41590138 | Banggyo-dong | 방교동 | MULTIPOLYGON (((127.09437 37.18396, 127.09460 ... | 240 | . 25 2110 | 41590139 | Geumgok-dong | 금곡동 | MULTIPOLYGON (((127.07119 37.18403, 127.07182 ... | 250 | . 26 2111 | 41590253 | Bongdam-eup | 봉담읍 | MULTIPOLYGON (((126.95798 37.24281, 126.95799 ... | 260 | . 27 2112 | 41590256 | Ujeong-eup | 우정읍 | MULTIPOLYGON (((126.73088 37.04639, 126.73141 ... | 270 | . 28 2113 | 41590259 | Hyangnam-eup | 향남읍 | MULTIPOLYGON (((126.99724 37.12423, 126.99732 ... | 280 | . 29 2114 | 41590262 | Namyang-eup | 남양읍 | MULTIPOLYGON (((126.79805 37.17035, 126.79780 ... | 290 | . 30 2115 | 41590310 | Maesong-myeon | 매송면 | MULTIPOLYGON (((126.95620 37.24524, 126.95574 ... | 300 | . 31 2116 | 41590320 | Bibong-myeon | 비봉면 | MULTIPOLYGON (((126.83330 37.23058, 126.83273 ... | 310 | . 32 2117 | 41590330 | Mado-myeon | 마도면 | MULTIPOLYGON (((126.79590 37.19074, 126.79613 ... | 320 | . 33 2118 | 41590340 | Songsan-myeon | 송산면 | MULTIPOLYGON (((126.66984 37.27413, 126.66946 ... | 330 | . 34 2119 | 41590350 | Seosin-myeon | 서신면 | MULTIPOLYGON (((126.62268 37.17914, 126.62301 ... | 340 | . 35 2120 | 41590360 | Paltan-myeon | 팔탄면 | MULTIPOLYGON (((126.83055 37.15032, 126.83110 ... | 350 | . 36 2121 | 41590370 | Jangan-myeon | 장안면 | MULTIPOLYGON (((126.81262 37.01341, 126.81262 ... | 360 | . fig2 = px.choropleth_mapbox( emd_scco_geo, geojson=geo_data, locations=&#39;emd_kor_nm&#39;, color=&#39;id_key&#39;, color_continuous_scale=px.colors.sequential.Rainbow, # featureidkey=&quot;properties.CTP_KOR_NM&quot;, # featureidkey를 사용하여 id 값을 갖는 키값 지정 mapbox_style=&quot;carto-positron&quot;, zoom=9, center = {&quot;lat&quot;: 37.15, &quot;lon&quot;: 126.9} ) fig2.show() fig.show() . fig3 = px.choropleth_mapbox( new_emd, geojson=geo_data, locations=&#39;emd_kor_nm&#39;, color=&#39;pop_ttl&#39;, color_continuous_scale=px.colors.sequential.Blues, # featureidkey=&quot;properties.CTP_KOR_NM&quot;, # featureidkey를 사용하여 id 값을 갖는 키값 지정 mapbox_style=&quot;carto-positron&quot;, zoom=9, center = {&quot;lat&quot;: 37.15, &quot;lon&quot;: 126.9} ) fig3.show() fig.show() . trans_2 = TripChain[TripChain[&quot;&#39;||버스노선ID2||&#39;&quot;].notnull()] # 2번 환승 trans_3 = TripChain[TripChain[&quot;&#39;||버스노선ID3||&#39;&quot;].notnull()] # 3번 환승 trans_4 = TripChain[TripChain[&quot;&#39;||버스노선ID4||&#39;&quot;].notnull()] # 4번 환승 trans_5 = TripChain[TripChain[&quot;&#39;||버스노선ID5||&#39;&quot;].notnull()] # 5번 환승 . 위의 상황을 바탕으로 1번 환승 외에 2-3번 환승을 주 타겟으로 하여 분석하였습니다. | . TripChain[(TripChain[&quot;&#39;||환승횟수||&#39;&quot;]==2) |(TripChain[&quot;&#39;||환승횟수||&#39;&quot;]==3)] # 환승 횟수가 2,3 번인 행 추출 . 암호화카드번호||&#39; &#39;||환승횟수||&#39; &#39;||사용자구분||&#39; &#39;||버스노선ID1||&#39; &#39;||버스노선ID2||&#39; &#39;||버스노선ID3||&#39; &#39;||버스노선ID4||&#39; &#39;||버스노선ID5||&#39; &#39;||차량ID1||&#39; &#39;||차량ID2||&#39; ... &#39;||총탑승시간||&#39; &#39;||총소요시간||&#39; &#39;||최초승차일시||&#39; &#39;||최종하차일시||&#39; &#39;||승차역ID1||&#39; &#39;||승차역ID2||&#39; &#39;||승차역ID3||&#39; &#39;||하차역ID4||&#39; &#39;||하차역ID5||&#39; &#39;||최종하차역ID||&#39; . 0 900079696430 | 2 | 1 | 41002045.0 | 41002044.0 | NaN | NaN | NaN | 141771735.0 | 141771587.0 | ... | 25 | 25 | 20180701052543 | 20180701064826 | 4116828.0 | 4150144.0 | NaN | NaN | NaN | 4116708.0 | . 6 900079705567 | 3 | 1 | 41067109.0 | NaN | 41021002.0 | NaN | NaN | 141763724.0 | 999999999.0 | ... | 39 | 39 | 20180701075731 | 20180701085359 | 4197606.0 | 1716.0 | 4116611.0 | NaN | NaN | 4116627.0 | . 7 900079705567 | 3 | 1 | 41110050.0 | NaN | 41067109.0 | NaN | NaN | 999999999.0 | 999999999.0 | ... | 41 | 41 | 20180701154641 | 20180701163919 | 9814.0 | 1708.0 | 4151651.0 | NaN | NaN | 4197607.0 | . 10 900079971595 | 2 | 1 | 41031065.0 | 41027004.0 | NaN | NaN | NaN | 141701159.0 | 141701340.0 | ... | 16 | 16 | 20180701091018 | 20180701093231 | 4116704.0 | 4108130.0 | NaN | NaN | NaN | 4151749.0 | . 12 900079974199 | 2 | 1 | 41020001.0 | 41324001.0 | NaN | NaN | NaN | 141703938.0 | 141769942.0 | ... | 21 | 21 | 20180701144943 | 20180701152035 | 4100084.0 | 9151651.0 | NaN | NaN | NaN | 4170526.0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 1048560 900016612555 | 2 | 1 | 41031030.0 | NaN | NaN | NaN | NaN | 141701126.0 | 999999999.0 | ... | 62 | 62 | 20180704134616 | 20180704145403 | 4179371.0 | 1872.0 | NaN | NaN | NaN | 4509.0 | . 1048562 900016615035 | 2 | 4 | 41036108.0 | 41036032.0 | NaN | NaN | NaN | 141703650.0 | 141705507.0 | ... | 12 | 12 | 20180704133539 | 20180704135156 | 4116860.0 | 4116948.0 | NaN | NaN | NaN | 4108215.0 | . 1048563 900016615619 | 2 | 1 | 41036113.0 | NaN | NaN | NaN | NaN | 141705619.0 | 999999999.0 | ... | 26 | 26 | 20180704080159 | 20180704083009 | 4116750.0 | 1711.0 | NaN | NaN | NaN | 1708.0 | . 1048567 900016616508 | 2 | 1 | 41086011.0 | 41044001.0 | NaN | NaN | NaN | 141771523.0 | 141721061.0 | ... | 54 | 54 | 20180704074240 | 20180704083724 | 4170942.0 | 4100021.0 | NaN | NaN | NaN | 4107053.0 | . 1048572 900016808481 | 2 | 1 | 41036131.0 | 41205007.0 | NaN | NaN | NaN | 141705287.0 | 141761323.0 | ... | 29 | 29 | 20180704093348 | 20180704102535 | 4170340.0 | 4116674.0 | NaN | NaN | NaN | 4119436.0 | . 419645 rows × 24 columns . df_trip = pd.DataFrame(TripChain.groupby(&quot;&#39;||승차역ID1||&#39;&quot;)[&quot;암호화카드번호||&#39;&quot;].count()) # 승차역별 이용자 수 . df_trip[&#39;이비카드정류장ID&#39;] = df_trip.index #인덱스값을 이비카드정류장 ID로 설정 . df_sta = station_table[[&quot;정류소명&quot;, &quot;이비카드정류장ID&quot;, &quot;emd&quot;,&quot;WGS84위도&quot;, &quot;WGS84경도&quot;]] # 사용할 열을 새로운 데이터셋에 저장 . trip_station = pd.merge(df_sta, df_trip, how = &#39;inner&#39;, on = &quot;이비카드정류장ID&quot;) # df_sta, df_trip 합치기 . df_trip_3 = pd.DataFrame(TripChain.groupby(&quot;&#39;||승차역ID2||&#39;&quot;)[&quot;암호화카드번호||&#39;&quot;].count()) # 승차역2 별 이용자 수 df_trip_3[&#39;이비카드정류장ID&#39;] = df_trip_3.index #인덱스값을 이비카드정류장 ID로 설정 trip_station_3 = pd.merge(df_sta, df_trip_3, how = &#39;inner&#39;, on = &quot;이비카드정류장ID&quot;) # df_sta, df_trip3 합치기 trans_2= trip_station_3.sort_values(&quot;암호화카드번호||&#39;&quot;,ascending = False).head(10) # 이용자 수가 많은 정류장 10개 추출 trans_2 . 정류소명 이비카드정류장ID emd WGS84위도 WGS84경도 암호화카드번호||&#39; . 26 신창미션힐.송화초교 | 4100048.0 | 병점동 | 37.203300 | 127.038550 | 5062 | . 125 병점역사거리 | 4151651.0 | 병점동 | 37.206917 | 127.035633 | 4913 | . 131 동부출장소.병점초등학교 | 4100051.0 | 진안동 | 37.208800 | 127.034433 | 3512 | . 25 홈플러스.벌말초교 | 4100049.0 | 병점동 | 37.203300 | 127.038133 | 3405 | . 126 병점사거리 | 4170271.0 | 진안동 | 37.208383 | 127.034433 | 2638 | . 124 병점역후문 | 4130131.0 | 진안동 | 37.206867 | 127.031833 | 2089 | . 251 한빛마을(중) | 4197467.0 | 반송동 | 37.207567 | 127.069200 | 1442 | . 75 수원대학교 | 4116671.0 | 봉담읍 | 37.214167 | 126.979150 | 1216 | . 197 반월리큰고개 | 4108035.0 | 반월동 | 37.232550 | 127.064783 | 1146 | . 203 메타폴리스(중) | 4170973.0 | 반송동 | 37.203700 | 127.067500 | 977 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df_trip_4 = pd.DataFrame(TripChain.groupby(&quot;&#39;||승차역ID3||&#39;&quot;)[&quot;암호화카드번호||&#39;&quot;].count()) # 승차역3 별 이용자 수 df_trip_4[&#39;이비카드정류장ID&#39;] = df_trip_4.index #인덱스값을 이비카드정류장 ID로 설정 trip_station_4 = pd.merge(df_sta, df_trip_4, how = &#39;inner&#39;, on = &quot;이비카드정류장ID&quot;) # df_sta, df_trip4 합치기 trans_1= trip_station_4.sort_values(&quot;암호화카드번호||&#39;&quot;,ascending = False).head(10) # 이용자 수가 많은 정류장 10개 추출 trans_1 . 정류소명 이비카드정류장ID emd WGS84위도 WGS84경도 암호화카드번호||&#39; . 110 병점역사거리 | 4151651.0 | 병점동 | 37.206917 | 127.035633 | 1340 | . 109 병점역후문 | 4130131.0 | 진안동 | 37.206867 | 127.031833 | 654 | . 115 동부출장소.병점초등학교 | 4100051.0 | 진안동 | 37.208800 | 127.034433 | 564 | . 22 홈플러스.벌말초교 | 4100049.0 | 병점동 | 37.203300 | 127.038133 | 399 | . 111 병점사거리 | 4170271.0 | 진안동 | 37.208383 | 127.034433 | 392 | . 207 한빛마을(중) | 4197467.0 | 반송동 | 37.207567 | 127.069200 | 389 | . 23 신창미션힐.송화초교 | 4100048.0 | 병점동 | 37.203300 | 127.038550 | 318 | . 0 우미제일.전하리교회 | 4130121.0 | 석우동 | 37.212133 | 127.079383 | 248 | . 64 수원대학교 | 4116671.0 | 봉담읍 | 37.214167 | 126.979150 | 227 | . 167 다은마을(중) | 4195716.0 | 반송동 | 37.200417 | 127.067300 | 218 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; bus_top_15 = trip_station.sort_values(&quot;암호화카드번호||&#39;&quot;,ascending = False).head(20) # 버스정류장 이용 탑 20 . bus_top_40 = trip_station.sort_values(&quot;암호화카드번호||&#39;&quot;,ascending = False).head(40) # 버스정류장 이용 탑 40 . df_trip2 = pd.DataFrame(TripChain.groupby(&quot;&#39;||최종하차역ID||&#39;&quot;)[&quot;암호화카드번호||&#39;&quot;].count()) df_trip2 # 하차역 별 이용자 수 . 암호화카드번호||&#39; . &#39;||최종하차역ID||&#39; . 150.0 237 | . 151.0 196 | . 152.0 238 | . 153.0 128 | . 154.0 91 | . ... ... | . 9636750.0 6 | . 9636753.0 1 | . 9636754.0 2 | . 9636962.0 1 | . 9636968.0 1 | . 12600 rows × 1 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; df_trip2[&#39;이비카드정류장ID&#39;] = df_trip2.index # 인덱스 값을 열로 설정 . trip_station_2 = pd.merge(df_sta, df_trip2, how = &#39;inner&#39;, on = &quot;이비카드정류장ID&quot;) #df_sta, df_trip2 합치기 =&gt; 정류장 별 이용자 수 trip_station_2 . 정류소명 이비카드정류장ID emd WGS84위도 WGS84경도 암호화카드번호||&#39; . 0 우미제일.전하리교회 | 4130121.0 | 석우동 | 37.212133 | 127.079383 | 496 | . 1 한국농수산대학 | 4116684.0 | 봉담읍 | 37.229317 | 126.970433 | 37 | . 2 대양2리 | 4130270.0 | 양감면 | 37.086417 | 126.942483 | 1 | . 3 대양4리 | 4130272.0 | 양감면 | 37.090283 | 126.941967 | 3 | . 4 대양1리 | 4130274.0 | 양감면 | 37.095567 | 126.939317 | 2 | . ... ... | ... | ... | ... | ... | ... | . 943 솔안고개 | 4130258.0 | 양감면 | 37.075350 | 126.963717 | 1 | . 944 신왕리 | 4170309.0 | 양감면 | 37.083333 | 126.944217 | 2 | . 945 빼골 | 4117380.0 | 정남면 | 37.180033 | 126.988250 | 130 | . 946 괘랑2리.차운혁충신정려문 | 4117378.0 | 정남면 | 37.182833 | 126.993150 | 271 | . 947 앞말 | 4117374.0 | 정남면 | 37.183567 | 127.001417 | 25 | . 948 rows × 6 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; bus_tail_15 = trip_station_2.sort_values(&quot;암호화카드번호||&#39;&quot;,ascending = False).head(20) # 버스정류장 이용 하위 20 . bus_tail_40 = trip_station_2.sort_values(&quot;암호화카드번호||&#39;&quot;,ascending = False).head(40) # 버스정류장 이용 하위 40 . bus_top_15 . 정류소명 이비카드정류장ID emd WGS84위도 WGS84경도 암호화카드번호||&#39; . 77 신영통현대타운.두산위브 | 4108036.0 | 반월동 | 37.235467 | 127.062467 | 3721 | . 63 신창미션힐.송화초교 | 4100048.0 | 병점동 | 37.203300 | 127.038550 | 3202 | . 611 한림대병원(중) | 4199455.0 | 석우동 | 37.216633 | 127.078117 | 2539 | . 418 메타폴리스(중) | 4170973.0 | 반송동 | 37.203700 | 127.067500 | 2489 | . 62 홈플러스.벌말초교 | 4100049.0 | 병점동 | 37.203300 | 127.038133 | 2431 | . 599 복합문화센터 | 4199435.0 | 반송동 | 37.201117 | 127.071100 | 2392 | . 190 수원대학교 | 4116671.0 | 봉담읍 | 37.214167 | 126.979150 | 2343 | . 569 동탄1동주민센터 | 4170243.0 | 반송동 | 37.206650 | 127.072567 | 2318 | . 75 반월리큰고개 | 4108034.0 | 반월동 | 37.233000 | 127.064800 | 2178 | . 414 삼성반도체후문 | 4130123.0 | 반월동 | 37.227667 | 127.070683 | 2075 | . 271 동부출장소.병점초등학교 | 4100051.0 | 진안동 | 37.208800 | 127.034433 | 1962 | . 822 이주택지.상록.경남아파트 | 4121008.0 | 동탄면 | 37.207217 | 127.110683 | 1939 | . 661 IT단지(중) | 4197592.0 | 석우동 | 37.224717 | 127.074183 | 1865 | . 509 한빛마을(중) | 4197467.0 | 반송동 | 37.207567 | 127.069200 | 1814 | . 65 삼성반도체 | 4108031.0 | 반월동 | 37.225083 | 127.064000 | 1768 | . 883 와우2리 | 4116674.0 | 봉담읍 | 37.215983 | 126.976317 | 1701 | . 265 병점역사거리 | 4151651.0 | 병점동 | 37.206917 | 127.035633 | 1671 | . 657 반석초등학교 | 4197463.0 | 반송동 | 37.193400 | 127.077467 | 1649 | . 775 삼성반도체후문 | 4170507.0 | 반월동 | 37.228417 | 127.071283 | 1635 | . 201 신일해피트리1차 | 4116678.0 | 봉담읍 | 37.222467 | 126.973933 | 1613 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; bus_tail_15.columns = [&#39;정류소명&#39;, &#39;이비카드정류장ID&#39;, &#39;emd&#39; ,&#39;WGS84위도&#39; ,&#39;WGS84경도&#39;, &#39;이용자수&#39;] bus_top_15.columns = [&#39;정류소명&#39;, &#39;이비카드정류장ID&#39;, &#39;emd&#39; ,&#39;WGS84위도&#39; ,&#39;WGS84경도&#39;, &#39;이용자수&#39;] bus_top_40.columns = [&#39;정류소명&#39;, &#39;이비카드정류장ID&#39;, &#39;emd&#39; ,&#39;WGS84위도&#39; ,&#39;WGS84경도&#39;, &#39;이용자수&#39;] bus_tail_40.columns = [&#39;정류소명&#39;, &#39;이비카드정류장ID&#39;, &#39;emd&#39; ,&#39;WGS84위도&#39; ,&#39;WGS84경도&#39;, &#39;이용자수&#39;] # 모든 테이블의 컬럼명을 변경 . total_bus = pd.concat([bus_top_15, bus_tail_15], axis = 0) # 상위20개와 하위20개 데이터셋을 합치기 . fig4 = px.scatter_mapbox(total_bus, lat = &#39;WGS84위도&#39;, lon = &#39;WGS84경도&#39;, color = &#39;emd&#39;, opacity = 0.4, hover_name = &#39;정류소명&#39;, hover_data = { &#39;WGS84위도&#39; : False, &#39;WGS84경도&#39; : False, &#39;emd&#39; : True }, mapbox_style = &quot;carto-positron&quot;, center = dict(lat = 37.15, lon = 126.9), zoom = 9) fig4.show() # 상위, 하위 이용자 수의 버스정류장 시각화 . . . bus_top_15.head(10) # 상위 10개 . 정류소명 이비카드정류장ID emd WGS84위도 WGS84경도 이용자수 . 77 신영통현대타운.두산위브 | 4108036.0 | 반월동 | 37.235467 | 127.062467 | 3721 | . 63 신창미션힐.송화초교 | 4100048.0 | 병점동 | 37.203300 | 127.038550 | 3202 | . 611 한림대병원(중) | 4199455.0 | 석우동 | 37.216633 | 127.078117 | 2539 | . 418 메타폴리스(중) | 4170973.0 | 반송동 | 37.203700 | 127.067500 | 2489 | . 62 홈플러스.벌말초교 | 4100049.0 | 병점동 | 37.203300 | 127.038133 | 2431 | . 599 복합문화센터 | 4199435.0 | 반송동 | 37.201117 | 127.071100 | 2392 | . 190 수원대학교 | 4116671.0 | 봉담읍 | 37.214167 | 126.979150 | 2343 | . 569 동탄1동주민센터 | 4170243.0 | 반송동 | 37.206650 | 127.072567 | 2318 | . 75 반월리큰고개 | 4108034.0 | 반월동 | 37.233000 | 127.064800 | 2178 | . 414 삼성반도체후문 | 4130123.0 | 반월동 | 37.227667 | 127.070683 | 2075 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; total_vis = bus_top_15.iloc[[0,1,2,3,4,5,7,9]] # 상위 하위에서 겹치는 정류장 . for idx, row in total_vis.iterrows(): folium.Marker([row[&#39;WGS84위도&#39;], row[&#39;WGS84경도&#39;]]).add_to(m) m.add_child(folium.LatLngPopup()) #겹치는 정류장의 지도 시각화 . Make this Notebook Trusted to load map: File -&gt; Trust Notebook 위의 지도를 보면 주로 이용하는 정류장의 위치가 대부분 화성시의 동쪽에 위치한 것을 볼 수 있습니다. | . 3.2 &#48516;&#49437; &#44208;&#44284; . 위에서 진행한 자료들을 통해 이 문제가 주거지역이 신도시쪽으로 몰리고, 많은 인구가 이동을 하면서 문제가 발생했음을 알 수 있었습니다. | 인구 수에 따라 고령화가 진행된 구역에서나 이용자 수가 많은 중심 구역과 동쪽 구역에서 모두 10-50대 사이에서 주로 출퇴근을 목적으로 이용하고 있었습니다. | 이에따라 같은 입장이지만 대부분의 주된 노선은 중심 구역과 동쪽 구역을 잇는 방식으로 운행되고 있었고 소외된 서쪽 구역에서는 환승을 많이 진행하는 경우가 발생됬을 것이라고 예상할 수 있었습니다. | . 4. &#44208;&#47200; . 4.1 &#52572;&#51201; &#45432;&#49440; &#51228;&#49884; . 신영통현대타운.두산위브 + 신창미션힐.송화초교 + 한림대병원 + 메타폴리스 + 홈플러스.벌말초교 +복합문화센터 + 동탄1동주민센터 + 삼성반도체후문으로 이용자 수가 많은 정류장을 거치는 노선을 신설한다. | 기존 노선의 경우 위의 정류장으로 환승을 할 수 있도록 위의 중 한 정류장을 노선에 추가 하는 방향으로 노선을 개선할 수 있다. | . . 4.2 &#54876;&#50857; &#48169;&#50504; &#49884;&#49324;&#51216; . 이용자 수, 시간대에 맞춰 배차 간격 줄일 수 있습니다. | | 주요 정류장에 맞춘 새로운 노선을 신설하는 데에 이용할 수 있습니다. . | | 실제 생활에서 겪었던 불편함이 위에서 발견한 문제들과 비슷한 이유로 발생하였을 것이라고 생각할 수 있었습니다. | | 현재는 화성시에 한하여 분석을 진행하였지만 전국 어디든지 비슷하게 적용을 할 수 있을 것이고 버스 정류장 뿐만 아니라 교통 체증을 완화하기 위한 수단으로도 활용할 수 있을 것 같습니다. | | 청소년을 대상으로 진행하고 있는 사업에서  이용량이 적어 배차간격이 크고 배차되는 버스 차량의 수가 적은 동네에서 사용하는 교통비를 지원해주는 사업에 적용할 수 있습니다. | | 문제점에서 나타난 점들을 고려하여 노선에 대한 문제를 해결한다면 화성시 내에서 시민들이 겪는 불편함을 최소화할 수 있을 것입니다. | | . .",
            "url": "https://yeaun12.github.io/blog/jupyter/python/2022/06/19/bus_road_final.html",
            "relUrl": "/jupyter/python/2022/06/19/bus_road_final.html",
            "date": " • Jun 19, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Exercise: Your First Map",
            "content": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link. . . Introduction . Kiva.org is an online crowdfunding platform extending financial services to poor people around the world. Kiva lenders have provided over $1 billion dollars in loans to over 2 million people. . Kiva reaches some of the most remote places in the world through their global network of &quot;Field Partners&quot;. These partners are local organizations working in communities to vet borrowers, provide services, and administer loans. . In this exercise, you&#39;ll investigate Kiva loans in the Philippines. Can you identify regions that might be outside of Kiva&#39;s current network, in order to identify opportunities for recruiting new Field Partners? . To get started, run the code cell below to set up our feedback system. . import geopandas as gpd . 1) Get the data. . Use the next cell to load the shapefile located at loans_filepath to create a GeoDataFrame world_loans. . &#45936;&#51060;&#53552; &#47196;&#46300; . loans_filepath = &quot;C:/Users/82108/Desktop/data mining/archive/kiva_loans/kiva_loans/kiva_loans.shp&quot; # Your code here: Load the data world_loans = gpd.read_file(loans_filepath) # Check your answer #q_1.check() # Uncomment to view the first five rows of the data #world_loans.head() world_loans.head() . Partner ID Field Part sector Loan Theme country amount geometry . 0 9 | KREDIT Microfinance Institution | General Financial Inclusion | Higher Education | Cambodia | 450 | POINT (102.89751 13.66726) | . 1 9 | KREDIT Microfinance Institution | General Financial Inclusion | Vulnerable Populations | Cambodia | 20275 | POINT (102.98962 13.02870) | . 2 9 | KREDIT Microfinance Institution | General Financial Inclusion | Higher Education | Cambodia | 9150 | POINT (102.98962 13.02870) | . 3 9 | KREDIT Microfinance Institution | General Financial Inclusion | Vulnerable Populations | Cambodia | 604950 | POINT (105.31312 12.09829) | . 4 9 | KREDIT Microfinance Institution | General Financial Inclusion | Sanitation | Cambodia | 275 | POINT (105.31312 12.09829) | . #q_1.hint() #q_1.solution() . 2) Plot the data. . Run the next code cell without changes to load a GeoDataFrame world containing country boundaries. . world_filepath = gpd.datasets.get_path(&#39;naturalearth_lowres&#39;) world = gpd.read_file(world_filepath) world.head() . pop_est continent name iso_a3 gdp_md_est geometry . 0 920938 | Oceania | Fiji | FJI | 8374.0 | MULTIPOLYGON (((180.00000 -16.06713, 180.00000... | . 1 53950935 | Africa | Tanzania | TZA | 150600.0 | POLYGON ((33.90371 -0.95000, 34.07262 -1.05982... | . 2 603253 | Africa | W. Sahara | ESH | 906.5 | POLYGON ((-8.66559 27.65643, -8.66512 27.58948... | . 3 35623680 | North America | Canada | CAN | 1674000.0 | MULTIPOLYGON (((-122.84000 49.00000, -122.9742... | . 4 326625791 | North America | United States of America | USA | 18560000.0 | MULTIPOLYGON (((-122.84000 49.00000, -120.0000... | . Use the world and world_loans GeoDataFrames to visualize Kiva loan locations across the world. . world, world_loans &#45936;&#51060;&#53552;&#47196; &#51648;&#46020; &#49884;&#44033;&#54868; . world.plot() world_loans.plot() ax = world.plot(figsize=(10,10), color=&#39;whitesmoke&#39;, linestyle=&#39;:&#39;, edgecolor=&#39;black&#39;) world_loans.plot(ax=ax, markersize=2) # Uncomment to see a hint #q_2.hint() . &lt;AxesSubplot:&gt; . #q_2.check() # Uncomment to see our solution (your code may look different!) #q_2.solution() . 3) Select loans based in the Philippines. . Next, you&#39;ll focus on loans that are based in the Philippines. Use the next code cell to create a GeoDataFrame PHL_loans which contains all rows from world_loans with loans that are based in the Philippines. . &#54596;&#47532;&#54592;&#50640; &#44592;&#48152;&#51012; &#46164; loan &#45936;&#51060;&#53552;&#49483; &#49373;&#49457; . PHL_loans = world_loans.loc[world_loans.country==&quot;Philippines&quot;].copy() PHL_loans.head() # Check your answer #q_3.check() . Partner ID Field Part sector Loan Theme country amount geometry . 2859 123 | Alalay sa Kaunlaran (ASKI) | General Financial Inclusion | General | Philippines | 400 | POINT (121.73961 17.64228) | . 2860 123 | Alalay sa Kaunlaran (ASKI) | General Financial Inclusion | General | Philippines | 400 | POINT (121.74169 17.63235) | . 2861 123 | Alalay sa Kaunlaran (ASKI) | General Financial Inclusion | General | Philippines | 400 | POINT (121.46667 16.60000) | . 2862 123 | Alalay sa Kaunlaran (ASKI) | General Financial Inclusion | General | Philippines | 6050 | POINT (121.73333 17.83333) | . 2863 123 | Alalay sa Kaunlaran (ASKI) | General Financial Inclusion | General | Philippines | 625 | POINT (121.51800 16.72368) | . #q_3.hint() #q_3.solution() . 4) Understand loans in the Philippines. . Run the next code cell without changes to load a GeoDataFrame PHL containing boundaries for all islands in the Philippines. . PHL, PHL_loans &#51012; &#49324;&#50857;&#54644; &#49884;&#44033;&#54868; . gpd.io.file.fiona.drvsupport.supported_drivers[&#39;KML&#39;] = &#39;rw&#39; PHL = gpd.read_file(&quot;archive/Philippines_AL258.kml&quot;, driver=&#39;KML&#39;) PHL.head() . Name Description geometry . 0 Autonomous Region in Muslim Mindanao | | MULTIPOLYGON (((119.46690 4.58718, 119.46653 4... | . 1 Bicol Region | | MULTIPOLYGON (((124.04577 11.57862, 124.04594 ... | . 2 Cagayan Valley | | MULTIPOLYGON (((122.51581 17.04436, 122.51568 ... | . 3 Calabarzon | | MULTIPOLYGON (((120.49202 14.05403, 120.49201 ... | . 4 Caraga | | MULTIPOLYGON (((126.45401 8.24400, 126.45407 8... | . Use the PHL and PHL_loans GeoDataFrames to visualize loans in the Philippines. . ax = PHL.plot(figsize=(10,20), color=&#39;whitesmoke&#39;, linestyle=&#39;:&#39;, edgecolor=&#39;black&#39;) PHL_loans.plot(ax=ax, markersize=2) # Uncomment to see a hint #q_4.a.hint() . &lt;AxesSubplot:&gt; . #q_4.a.check() # Uncomment to see our solution (your code may look different!) #q_4.a.solution() . Can you identify any islands where it might be useful to recruit new Field Partners? Do any islands currently look outside of Kiva&#39;s reach? . You might find this map useful to answer the question. . #q_4.b.solution() . Keep going . Continue to learn about coordinate reference systems. . . Have questions or comments? Visit the course discussion forum to chat with other learners. .",
            "url": "https://yeaun12.github.io/blog/jupyter/python/2022/05/28/exercise-your-first-map.html",
            "relUrl": "/jupyter/python/2022/05/28/exercise-your-first-map.html",
            "date": " • May 28, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Exercise: Proximity Analysis",
            "content": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link. . . Introduction . You are part of a crisis response team, and you want to identify how hospitals have been responding to crash collisions in New York City. . Before you get started, run the code cell below to set everything up. . import math import geopandas as gpd import pandas as pd from shapely.geometry import MultiPolygon import folium from folium import Choropleth, Marker from folium.plugins import HeatMap, MarkerCluster . You&#39;ll use the embed_map() function to visualize your maps. . def embed_map(m, file_name): from IPython.display import IFrame m.save(file_name) return IFrame(file_name, width=&#39;100%&#39;, height=&#39;500px&#39;) . Exercises . 1) Visualize the collision data. . Run the code cell below to load a GeoDataFrame collisions tracking major motor vehicle collisions in 2013-2018. . &#45936;&#51060;&#53552; &#47196;&#46300; . collisions = gpd.read_file(&quot;archive/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions/NYPD_Motor_Vehicle_Collisions.shp&quot;) collisions.head() . DATE TIME BOROUGH ZIP CODE LATITUDE LONGITUDE LOCATION ON STREET CROSS STRE OFF STREET ... CONTRIBU_2 CONTRIBU_3 CONTRIBU_4 UNIQUE KEY VEHICLE TY VEHICLE _1 VEHICLE _2 VEHICLE _3 VEHICLE _4 geometry . 0 07/30/2019 | 0:00 | BRONX | 10464 | 40.841100 | -73.784960 | (40.8411, -73.78496) | None | None | 121 PILOT STREET | ... | Unspecified | None | None | 4180045 | Sedan | Station Wagon/Sport Utility Vehicle | Station Wagon/Sport Utility Vehicle | None | None | POINT (1043750.211 245785.815) | . 1 07/30/2019 | 0:10 | QUEENS | 11423 | 40.710827 | -73.770660 | (40.710827, -73.77066) | JAMAICA AVENUE | 188 STREET | None | ... | None | None | None | 4180007 | Sedan | Sedan | None | None | None | POINT (1047831.185 198333.171) | . 2 07/30/2019 | 0:25 | None | None | 40.880318 | -73.841286 | (40.880318, -73.841286) | BOSTON ROAD | None | None | ... | None | None | None | 4179575 | Sedan | Station Wagon/Sport Utility Vehicle | None | None | None | POINT (1028139.293 260041.178) | . 3 07/30/2019 | 0:35 | MANHATTAN | 10036 | 40.756744 | -73.984590 | (40.756744, -73.98459) | None | None | 155 WEST 44 STREET | ... | None | None | None | 4179544 | Box Truck | Station Wagon/Sport Utility Vehicle | None | None | None | POINT (988519.261 214979.320) | . 4 07/30/2019 | 10:00 | BROOKLYN | 11223 | 40.600090 | -73.965910 | (40.60009, -73.96591) | AVENUE T | OCEAN PARKWAY | None | ... | None | None | None | 4180660 | Station Wagon/Sport Utility Vehicle | Bike | None | None | None | POINT (993716.669 157907.212) | . 5 rows × 30 columns . Use the &quot;LATITUDE&quot; and &quot;LONGITUDE&quot; columns to create an interactive map to visualize the collision data. What type of map do you think is most effective? . LATITUDE, LONGITUDE &#51012; &#49324;&#50857;&#54644; &#49884;&#44033;&#54868; . m_1 = folium.Map(location=[40.7, -74], zoom_start=11) # Your code here: Visualize the collision data HeatMap(data=collisions[[&#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]], radius=8).add_to(m_1) #히트맵 사용 # Uncomment to see a hint #q_1.hint() # Show the map embed_map(m_1, &quot;q_1.html&quot;) . #q_1.check() # Uncomment to see our solution (your code may look different!) #q_1.solution() . 2) Understand hospital coverage. . Run the next code cell to load the hospital data. . &#45936;&#51060;&#53552; &#47196;&#46300; . hospitals = gpd.read_file(&quot;archive/nyu_2451_34494/nyu_2451_34494/nyu_2451_34494.shp&quot;) hospitals.head() . id name address zip factype facname capacity capname bcode xcoord ycoord latitude longitude geometry . 0 317000001H1178 | BRONX-LEBANON HOSPITAL CENTER - CONCOURSE DIVI... | 1650 Grand Concourse | 10457 | 3102 | Hospital | 415 | Beds | 36005 | 1008872.0 | 246596.0 | 40.843490 | -73.911010 | POINT (1008872.000 246596.000) | . 1 317000001H1164 | BRONX-LEBANON HOSPITAL CENTER - FULTON DIVISION | 1276 Fulton Ave | 10456 | 3102 | Hospital | 164 | Beds | 36005 | 1011044.0 | 242204.0 | 40.831429 | -73.903178 | POINT (1011044.000 242204.000) | . 2 317000011H1175 | CALVARY HOSPITAL INC | 1740-70 Eastchester Rd | 10461 | 3102 | Hospital | 225 | Beds | 36005 | 1027505.0 | 248287.0 | 40.848060 | -73.843656 | POINT (1027505.000 248287.000) | . 3 317000002H1165 | JACOBI MEDICAL CENTER | 1400 Pelham Pkwy | 10461 | 3102 | Hospital | 457 | Beds | 36005 | 1027042.0 | 251065.0 | 40.855687 | -73.845311 | POINT (1027042.000 251065.000) | . 4 317000008H1172 | LINCOLN MEDICAL &amp; MENTAL HEALTH CENTER | 234 E 149 St | 10451 | 3102 | Hospital | 362 | Beds | 36005 | 1005154.0 | 236853.0 | 40.816758 | -73.924478 | POINT (1005154.000 236853.000) | . Use the &quot;latitude&quot; and &quot;longitude&quot; columns to visualize the hospital locations. . 위도, 경도로 위치 시각화 | . m_2 = folium.Map(location=[40.7, -74], zoom_start=12) # Your code here: Visualize the hospital locations for idx, row in hospitals.iterrows(): Marker([row[&#39;latitude&#39;], row[&#39;longitude&#39;]], popup=row[&#39;name&#39;]).add_to(m_2) # Uncomment to see a hint #q_2.hint() # Show the map embed_map(m_2, &quot;q_2.html&quot;) . #q_2.check() # Uncomment to see our solution (your code may look different!) #q_2.solution() . 3) When was the closest hospital more than 10 kilometers away? . Create a DataFrame outside_range containing all rows from collisions with crashes that occurred more than 10 kilometers from the closest hospital. . Note that both hospitals and collisions have EPSG 2263 as the coordinate reference system, and EPSG 2263 has units of meters. . &#44032;&#44620;&#50868; &#48337;&#50896;&#51060; 10km &#51060;&#49345; &#46504;&#50612;&#51652; &#44221;&#50864; . coverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000) my_union = coverage.geometry.unary_union outside_range = collisions.loc[~collisions[&quot;geometry&quot;].apply(lambda x: my_union.contains(x))] # Check your answer #q_3.check() . #q_3.hint() #q_3.solution() . The next code cell calculates the percentage of collisions that occurred more than 10 kilometers away from the closest hospital. . 충돌 비율 계산 | . percentage = round(100*len(outside_range)/len(collisions), 2) print(&quot;Percentage of collisions more than 10 km away from the closest hospital: {}%&quot;.format(percentage)) . Percentage of collisions more than 10 km away from the closest hospital: 15.12% . 15.12% | . 4) Make a recommender. . When collisions occur in distant locations, it becomes even more vital that injured persons are transported to the nearest available hospital. . With this in mind, you decide to create a recommender that: . takes the location of the crash (in EPSG 2263) as input, | finds the closest hospital (where distance calculations are done in EPSG 2263), and | returns the name of the closest hospital. | . &#44032;&#51109; &#44032;&#44620;&#50868; &#48337;&#50896; &#48152;&#54872; . def best_hospital(collision_location): idx_min = hospitals.geometry.distance(collision_location).idxmin() my_hospital = hospitals.iloc[idx_min] name = my_hospital[&quot;name&quot;] return name # Test your function: this should suggest CALVARY HOSPITAL INC print(best_hospital(outside_range.geometry.iloc[0])) # Check your answer #q_4.check() . CALVARY HOSPITAL INC . #q_4.hint() #q_4.solution() . 5) Which hospital is under the highest demand? . Considering only collisions in the outside_range DataFrame, which hospital is most recommended? . Your answer should be a Python string that exactly matches the name of the hospital returned by the function you created in 4). . &#44032;&#51109; &#49688;&#50836;&#44032; &#47566;&#51008; &#48337;&#50896; . highest_demand = outside_range.geometry.apply(best_hospital).value_counts().idxmax() # Check your answer #q_5.check() . #q_5.hint() #q_5.solution() . 6) Where should the city construct new hospitals? . Run the next code cell (without changes) to visualize hospital locations, in addition to collisions that occurred more than 10 kilometers away from the closest hospital. . &#50612;&#46356;&#50640; &#48337;&#50896;&#51012; &#51648;&#50612;&#50556;&#54624;&#51648;&#50640; &#45824;&#54620; &#49884;&#44033;&#54868; . 충돌 위치, 병원 위치 | . m_6 = folium.Map(location=[40.7, -74], zoom_start=10) coverage = gpd.GeoDataFrame(geometry=hospitals.geometry).buffer(10000) folium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m_6) HeatMap(data=outside_range[[&#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]], radius=9).add_to(m_6) folium.LatLngPopup().add_to(m_6) embed_map(m_6, &#39;m_6.html&#39;) . Click anywhere on the map to see a pop-up with the corresponding location in latitude and longitude. . The city of New York reaches out to you for help with deciding locations for two brand new hospitals. They specifically want your help with identifying locations to bring the calculated percentage from step 3) to less than ten percent. Using the map (and without worrying about zoning laws or what potential buildings would have to be removed in order to build the hospitals), can you identify two locations that would help the city accomplish this goal? . Put the proposed latitude and longitude for hospital 1 in lat_1 and long_1, respectively. (Likewise for hospital 2.) . Then, run the rest of the cell as-is to see the effect of the new hospitals. Your answer will be marked correct, if the two new hospitals bring the percentage to less than ten percent. . lat_1 = 40.7397 long_1 = -73.7584 # Proposed location of hospital 2 lat_2 = 40.6781 long_2 = -73.7586 # Do not modify the code below this line try: new_df = pd.DataFrame( {&#39;Latitude&#39;: [lat_1, lat_2], &#39;Longitude&#39;: [long_1, long_2]}) new_gdf = gpd.GeoDataFrame(new_df, geometry=gpd.points_from_xy(new_df.Longitude, new_df.Latitude)) new_gdf.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} new_gdf = new_gdf.to_crs(epsg=2263) # get new percentage new_coverage = gpd.GeoDataFrame(geometry=new_gdf.geometry).buffer(10000) new_my_union = new_coverage.geometry.unary_union new_outside_range = outside_range.loc[~outside_range[&quot;geometry&quot;].apply(lambda x: new_my_union.contains(x))] new_percentage = round(100*len(new_outside_range)/len(collisions), 2) print(&quot;(NEW) Percentage of collisions more than 10 km away from the closest hospital: {}%&quot;.format(new_percentage)) # Did you help the city to meet its goal? q_6.check() # make the map m = folium.Map(location=[40.7, -74], zoom_start=11) folium.GeoJson(coverage.geometry.to_crs(epsg=4326)).add_to(m) folium.GeoJson(new_coverage.geometry.to_crs(epsg=4326)).add_to(m) for idx, row in new_gdf.iterrows(): Marker([row[&#39;Latitude&#39;], row[&#39;Longitude&#39;]]).add_to(m) HeatMap(data=new_outside_range[[&#39;LATITUDE&#39;, &#39;LONGITUDE&#39;]], radius=9).add_to(m) folium.LatLngPopup().add_to(m) display(embed_map(m, &#39;q_6.html&#39;)) except: print(&#39;&#39;) . c: Users 82108 AppData Local Programs Python Python38-32 lib site-packages pyproj crs crs.py:130: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 in_crs_string = _prepare_from_proj_string(in_crs_string) . (NEW) Percentage of collisions more than 10 km away from the closest hospital: 9.97% . 9.97% . #q_6.solution() . Congratulations! . You have just completed the Geospatial Analysis micro-course! Great job! . . Have questions or comments? Visit the course discussion forum to chat with other learners. .",
            "url": "https://yeaun12.github.io/blog/jupyter/python/2022/05/28/exercise(5)-proximity-analysis.html",
            "relUrl": "/jupyter/python/2022/05/28/exercise(5)-proximity-analysis.html",
            "date": " • May 28, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Exercise: Manipulating Geospatial Data",
            "content": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link. . . Introduction . You are a Starbucks big data analyst (that’s a real job!) looking to find the next store into a Starbucks Reserve Roastery. These roasteries are much larger than a typical Starbucks store and have several additional features, including various food and wine options, along with upscale lounge areas. You&#39;ll investigate the demographics of various counties in the state of California, to determine potentially suitable locations. . Before you get started, run the code cell below to set everything up. . import math import pandas as pd import geopandas as gpd from geopy.geocoders import Nominatim # What you&#39;d normally run import folium from folium import Marker from folium.plugins import MarkerCluster . You&#39;ll use the embed_map() function from the previous exercise to visualize your maps. . def embed_map(m, file_name): from IPython.display import IFrame m.save(file_name) return IFrame(file_name, width=&#39;100%&#39;, height=&#39;500px&#39;) . Exercises . 1) Geocode the missing locations. . Run the next code cell to create a DataFrame starbucks containing Starbucks locations in the state of California. . &#52888;&#47532;&#54252;&#45768;&#50500;&#50640; &#51080;&#45716; &#49828;&#53440;&#48261;&#49828; . starbucks = pd.read_csv(&quot;archive starbucks_locations.csv&quot;) starbucks.head() . Store Number Store Name Address City Longitude Latitude . 0 10429-100710 | Palmdale &amp; Hwy 395 | 14136 US Hwy 395 Adelanto CA | Adelanto | -117.40 | 34.51 | . 1 635-352 | Kanan &amp; Thousand Oaks | 5827 Kanan Road Agoura CA | Agoura | -118.76 | 34.16 | . 2 74510-27669 | Vons-Agoura Hills #2001 | 5671 Kanan Rd. Agoura Hills CA | Agoura Hills | -118.76 | 34.15 | . 3 29839-255026 | Target Anaheim T-0677 | 8148 E SANTA ANA CANYON ROAD AHAHEIM CA | AHAHEIM | -117.75 | 33.87 | . 4 23463-230284 | Safeway - Alameda 3281 | 2600 5th Street Alameda CA | Alameda | -122.28 | 37.79 | . Most of the stores have known (latitude, longitude) locations. But, all of the locations in the city of Berkeley are missing. . Berkeley&#50640; &#51080;&#45716; &#49828;&#53440;&#48261;&#49828;&#51032; &#50948;&#46020;, &#44221;&#46020; &#45936;&#51060;&#53552; &#45572;&#46973; . print(starbucks.isnull().sum()) # View rows with missing locations rows_with_missing = starbucks[starbucks[&quot;City&quot;]==&quot;Berkeley&quot;] rows_with_missing . Store Number 0 Store Name 0 Address 0 City 0 Longitude 5 Latitude 5 dtype: int64 . Store Number Store Name Address City Longitude Latitude . 153 5406-945 | 2224 Shattuck - Berkeley | 2224 Shattuck Avenue Berkeley CA | Berkeley | NaN | NaN | . 154 570-512 | Solano Ave | 1799 Solano Avenue Berkeley CA | Berkeley | NaN | NaN | . 155 17877-164526 | Safeway - Berkeley #691 | 1444 Shattuck Place Berkeley CA | Berkeley | NaN | NaN | . 156 19864-202264 | Telegraph &amp; Ashby | 3001 Telegraph Avenue Berkeley CA | Berkeley | NaN | NaN | . 157 9217-9253 | 2128 Oxford St. | 2128 Oxford Street Berkeley CA | Berkeley | NaN | NaN | . geocoder 사용 | . Use the code cell below to fill in these values with the Nominatim geocoder. . Note that in the tutorial, we used Nominatim() (from geopy.geocoders) to geocode values, and this is what you can use in your own projects outside of this course. . In this exercise, you will use a slightly different function Nominatim() (from learntools.geospatial.tools). This function was imported at the top of the notebook and works identically to the function from GeoPandas. . So, in other words, as long as: . you don&#39;t change the import statements at the top of the notebook, and | you call the geocoding function as geocode() in the code cell below, | . your code will work as intended! . geolocator = Nominatim(user_agent=&quot;kaggle_learn&quot;) # Your code here def my_geocoder(row): point = geolocator.geocode(row).point return pd.Series({&#39;Latitude&#39;: point.latitude, &#39;Longitude&#39;: point.longitude}) berkeley_locations = rows_with_missing.apply(lambda x: my_geocoder(x[&#39;Address&#39;]), axis=1) starbucks.update(berkeley_locations) # Check your answer #q_1.check() . #q_1.solution() . 2) View Berkeley locations. . Let&#39;s take a look at the locations you just found. Visualize the (latitude, longitude) locations in Berkeley in the OpenStreetMap style. . Berkeley &#50948;&#52824; &#49884;&#44033;&#54868; . m_2 = folium.Map(location=[37.88,-122.26], zoom_start=10) # 배율 설정 # Your code here: Add a marker for each Berkeley location for idx, row in starbucks[starbucks[&quot;City&quot;]==&#39;Berkeley&#39;].iterrows(): Marker([row[&#39;Latitude&#39;], row[&#39;Longitude&#39;]]).add_to(m_2) # 위치 표시 # Uncomment to see a hint #q_2.a.hint() # Show the map embed_map(m_2, &#39;q_2.html&#39;) . #q_2.a.check() # Uncomment to see our solution (your code may look different!) #q_2.a.solution() . Considering only the five locations in Berkeley, how many of the (latitude, longitude) locations seem potentially correct (are located in the correct city)? . Berkeley &#51032; &#50948;&#52824;&#47484; &#44256;&#47140;&#54664;&#51012; &#46412; &#51221;&#54869;&#54644;&#48372;&#51060;&#45716; &#50948;&#52824; . #q_2.b.solution() . 전체적으로 다섯개 전부 정확해보인다. | . 3) Consolidate your data. . Run the code below to load a GeoDataFrame CA_counties containing the name, area (in square kilometers), and a unique id (in the &quot;GEOID&quot; column) for each county in the state of California. The &quot;geometry&quot; column contains a polygon with county boundaries. . &#45936;&#51060;&#53552; &#53685;&#54633; . CA_counties = gpd.read_file(&quot;archive CA_county_boundaries CA_county_boundaries CA_county_boundaries.shp&quot;) CA_counties.head() . GEOID name area_sqkm geometry . 0 6091 | Sierra County | 2491.995494 | POLYGON ((-120.65560 39.69357, -120.65554 39.6... | . 1 6067 | Sacramento County | 2575.258262 | POLYGON ((-121.18858 38.71431, -121.18732 38.7... | . 2 6083 | Santa Barbara County | 9813.817958 | MULTIPOLYGON (((-120.58191 34.09856, -120.5822... | . 3 6009 | Calaveras County | 2685.626726 | POLYGON ((-120.63095 38.34111, -120.63058 38.3... | . 4 6111 | Ventura County | 5719.321379 | MULTIPOLYGON (((-119.63631 33.27304, -119.6360... | . Next, we create three DataFrames: . CA_pop contains an estimate of the population of each county. | CA_high_earners contains the number of households with an income of at least $150,000 per year. | CA_median_age contains the median age for each county. | . CA_pop = pd.read_csv(&quot;archive CA_county_population.csv&quot;, index_col=&quot;GEOID&quot;) CA_high_earners = pd.read_csv(&quot;archive CA_county_high_earners.csv&quot;, index_col=&quot;GEOID&quot;) CA_median_age = pd.read_csv(&quot;archive CA_county_median_age.csv&quot;, index_col=&quot;GEOID&quot;) . Use the next code cell to join the CA_counties GeoDataFrame with CA_pop, CA_high_earners, and CA_median_age. . Name the resultant GeoDataFrame CA_stats, and make sure it has 8 columns: &quot;GEOID&quot;, &quot;name&quot;, &quot;area_sqkm&quot;, &quot;geometry&quot;, &quot;population&quot;, &quot;high_earners&quot;, and &quot;median_age&quot;. Also, make sure the CRS is set to {&#39;init&#39;: &#39;epsg:4326&#39;}. . cols_to_add = CA_pop.join([CA_high_earners, CA_median_age]).reset_index() CA_stats = CA_counties.merge(cols_to_add, on=&quot;GEOID&quot;) CA_stats.crs = {&#39;init&#39;: &#39;epsg:4326&#39;} # Check your answer #q_3.check() . c: Users 82108 AppData Local Programs Python Python38-32 lib site-packages pyproj crs crs.py:130: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 in_crs_string = _prepare_from_proj_string(in_crs_string) . #q_3.hint() #q_3.solution() . Now that we have all of the data in one place, it&#39;s much easier to calculate statistics that use a combination of columns. Run the next code cell to create a &quot;density&quot; column with the population density. . &#51064;&#44396;&#48128;&#46020; &#50676; &#49373;&#49457; . CA_stats[&quot;density&quot;] = CA_stats[&quot;population&quot;] / CA_stats[&quot;area_sqkm&quot;] . 4) Which counties look promising? . Collapsing all of the information into a single GeoDataFrame also makes it much easier to select counties that meet specific criteria. . Use the next code cell to create a GeoDataFrame sel_counties that contains a subset of the rows (and all of the columns) from the CA_stats GeoDataFrame. In particular, you should select counties where: . there are at least 100,000 households making $150,000 per year, | the median age is less than 38.5, and | the density of inhabitants is at least 285 (per square kilometer). | . Additionally, selected counties should satisfy at least one of the following criteria: . there are at least 500,000 households making $150,000 per year, | the median age is less than 35.5, or | the density of inhabitants is at least 1400 (per square kilometer). | . &#50948;&#51032; &#51312;&#44148;&#50640; &#47564;&#51313;&#54616;&#45716; &#50976;&#47581;&#54620; &#52852;&#50868;&#54000;(&#54665;&#51221;&#44396;&#50669;) . sel_counties = CA_stats[((CA_stats.high_earners &gt; 100000) &amp; (CA_stats.median_age &lt; 38.5) &amp; (CA_stats.density &gt; 285) &amp; ((CA_stats.median_age &lt; 35.5) | (CA_stats.density &gt; 1400) | (CA_stats.high_earners &gt; 500000)))] sel_counties # Check your answer #q_4.check() . GEOID name area_sqkm geometry population high_earners median_age density . 5 6037 | Los Angeles County | 12305.376879 | MULTIPOLYGON (((-118.66761 33.47749, -118.6682... | 10105518 | 501413 | 36.0 | 821.227834 | . 8 6073 | San Diego County | 11721.342229 | POLYGON ((-117.43744 33.17953, -117.44955 33.1... | 3343364 | 194676 | 35.4 | 285.237299 | . 10 6075 | San Francisco County | 600.588247 | MULTIPOLYGON (((-122.60025 37.80249, -122.6123... | 883305 | 114989 | 38.3 | 1470.733077 | . #q_4.hint() #q_4.solution() . 5) How many stores did you identify? . When looking for the next Starbucks Reserve Roastery location, you&#39;d like to consider all of the stores within the counties that you selected. So, how many stores are within the selected counties? . To prepare to answer this question, run the next code cell to create a GeoDataFrame starbucks_gdf with all of the starbucks locations. . starbucks_gdf = gpd.GeoDataFrame(starbucks, geometry=gpd.points_from_xy(starbucks.Longitude, starbucks.Latitude)) starbucks_gdf.crs = {&#39;init&#39;: &#39;epsg:4326&#39;} . c: Users 82108 AppData Local Programs Python Python38-32 lib site-packages pyproj crs crs.py:130: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 in_crs_string = _prepare_from_proj_string(in_crs_string) . So, how many stores are in the counties you selected? . import matplotlib.image as img import matplotlib.pyplot as plt . fileName = &quot;kaggle_1.png&quot; ndarray = img.imread(fileName) plt.imshow(ndarray) plt.show() . fileName_2 = &quot;kaggle_2.png&quot; ndarray = img.imread(fileName_2) plt.imshow(ndarray) plt.show() . Keep going . Learn about how proximity analysis can help you to understand the relationships between points on a map. . . Have questions or comments? Visit the course discussion forum to chat with other learners. .",
            "url": "https://yeaun12.github.io/blog/jupyter/python/2022/05/28/exercise(4)-manipulating-geospatial-data.html",
            "relUrl": "/jupyter/python/2022/05/28/exercise(4)-manipulating-geospatial-data.html",
            "date": " • May 28, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Exercise: Interactive-Maps",
            "content": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link. . . Introduction . You are an urban safety planner in Japan, and you are analyzing which areas of Japan need extra earthquake reinforcement. Which areas are both high in population density and prone to earthquakes? . Before you get started, run the code cell below to set everything up. . import pandas as pd import geopandas as gpd import folium from folium import Choropleth from folium.plugins import HeatMap . We define a function embed_map() for displaying interactive maps. It accepts two arguments: the variable containing the map, and the name of the HTML file where the map will be saved. . This function ensures that the maps are visible in all web browsers. . def embed_map(m, file_name): from IPython.display import IFrame m.save(file_name) return IFrame(file_name, width=&#39;100%&#39;, height=&#39;500px&#39;) . Exercises . 1) Do earthquakes coincide with plate boundaries? . Run the code cell below to create a DataFrame plate_boundaries that shows global plate boundaries. The &quot;coordinates&quot; column is a list of (latitude, longitude) locations along the boundaries. . &#45936;&#51060;&#53552; &#47196;&#46300; . plate_boundaries = gpd.read_file(&quot;archive/Plate_Boundaries/Plate_Boundaries/Plate_Boundaries.shp&quot;) plate_boundaries[&#39;coordinates&#39;] = plate_boundaries.apply(lambda x: [(b,a) for (a,b) in list(x.geometry.coords)], axis=&#39;columns&#39;) plate_boundaries.drop(&#39;geometry&#39;, axis=1, inplace=True) plate_boundaries.head() . HAZ_PLATES HAZ_PLAT_1 HAZ_PLAT_2 Shape_Leng coordinates . 0 TRENCH | SERAM TROUGH (ACTIVE) | 6722 | 5.843467 | [(-5.444200361999947, 133.6808931800001), (-5.... | . 1 TRENCH | WETAR THRUST | 6722 | 1.829013 | [(-7.760600482999962, 125.47879802900002), (-7... | . 2 TRENCH | TRENCH WEST OF LUZON (MANILA TRENCH) NORTHERN ... | 6621 | 6.743604 | [(19.817899819000047, 120.09999798800004), (19... | . 3 TRENCH | BONIN TRENCH | 9821 | 8.329381 | [(26.175899215000072, 143.20620700100005), (26... | . 4 TRENCH | NEW GUINEA TRENCH | 8001 | 11.998145 | [(0.41880004000006466, 132.8273013480001), (0.... | . Next, run the code cell below without changes to load the historical earthquake data into a DataFrame earthquakes. . earthquakes = pd.read_csv(&quot;archive earthquakes1970-2014.csv&quot;, parse_dates=[&quot;DateTime&quot;]) earthquakes.head() . DateTime Latitude Longitude Depth Magnitude MagType NbStations Gap Distance RMS Source EventID . 0 1970-01-04 17:00:40.200 | 24.139 | 102.503 | 31.0 | 7.5 | Ms | 90.0 | NaN | NaN | 0.0 | NEI | 1.970010e+09 | . 1 1970-01-06 05:35:51.800 | -9.628 | 151.458 | 8.0 | 6.2 | Ms | 85.0 | NaN | NaN | 0.0 | NEI | 1.970011e+09 | . 2 1970-01-08 17:12:39.100 | -34.741 | 178.568 | 179.0 | 6.1 | Mb | 59.0 | NaN | NaN | 0.0 | NEI | 1.970011e+09 | . 3 1970-01-10 12:07:08.600 | 6.825 | 126.737 | 73.0 | 6.1 | Mb | 91.0 | NaN | NaN | 0.0 | NEI | 1.970011e+09 | . 4 1970-01-16 08:05:39.000 | 60.280 | -152.660 | 85.0 | 6.0 | ML | 0.0 | NaN | NaN | NaN | AK | NaN | . &#51648;&#46020; &#44221;&#44228;, &#51648;&#51652; &#54032; &#49884;&#44033;&#54868; . The code cell below visualizes the plate boundaries on a map. Use all of the earthquake data to add a heatmap to the same map, to determine whether earthquakes coincide with plate boundaries. . m_1 = folium.Map(location=[35,136], tiles=&#39;cartodbpositron&#39;, zoom_start=5) for i in range(len(plate_boundaries)): folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color=&#39;black&#39;).add_to(m_1) # Your code here: Add a heatmap to the map HeatMap(data=earthquakes[[&#39;Latitude&#39;, &#39;Longitude&#39;]], radius=15).add_to(m_1) # Uncomment to see a hint #q_1.a.hint() # Show the map embed_map(m_1, &#39;q_1.html&#39;) . #q_1.a.check() # Uncomment to see our solution (your code may look different!) #q_1.a.solution() . So, given the map above, do earthquakes coincide with plate boundaries? . &#51648;&#46020; &#49884;&#44033;&#54868; &#44208;&#47200; . 지진과 판 경계가 일치한다. | . #q_1.b.solution() . 2) Is there a relationship between earthquake depth and proximity to a plate boundary in Japan? . You recently read that the depth of earthquakes tells us important information about the structure of the earth. You&#39;re interested to see if there are any intereresting global patterns, and you&#39;d also like to understand how depth varies in Japan. . &#51068;&#48376;&#51032; &#54032; &#44221;&#44228;&#50752; &#51648;&#51652; &#44618;&#51060;, &#44540;&#51217;&#49457;&#51032; &#44288;&#44228; &#54028;&#50501;&#51012; &#50948;&#54620; &#51648;&#46020; &#49884;&#44033;&#54868; . m_2 = folium.Map(location=[35,136], tiles=&#39;cartodbpositron&#39;, zoom_start=5) for i in range(len(plate_boundaries)): folium.PolyLine(locations=plate_boundaries.coordinates.iloc[i], weight=2, color=&#39;darkred&#39;).add_to(m_2) #선긋기 # Your code here: Add a map to visualize earthquake depth def color_producer(val): if val &lt; 50: return &#39;forestgreen&#39; elif val &lt; 100: return &#39;orange&#39; else: return &#39;blue&#39; for i in range(0,len(earthquakes)): folium.Circle( location=[earthquakes.iloc[i][&#39;Latitude&#39;], earthquakes.iloc[i][&#39;Longitude&#39;]], radius=4000, color=color_producer(earthquakes.iloc[i][&#39;Depth&#39;])).add_to(m_2) #점 찍기 # Uncomment to see a hint #q_2.a.hint() # View the map embed_map(m_2, &#39;q_2.html&#39;) . #q_2.a.check() # Uncomment to see our solution (your code may look different!) #q_2.a.solution() . Can you detect a relationship between proximity to a plate boundary and earthquake depth? Does this pattern hold globally? In Japan? . &#54032; &#44221;&#44228;&#51032; &#44540;&#51217;&#49457;&#44284; &#51648;&#51652; &#44618;&#51060;&#51032; &#44288;&#44228;&#47484; &#44048;&#51648;&#54624; &#49688; &#51080;&#45716;&#51648;? . 일본 북부와 남미 서부 해안에서 판 경계에서 멀리 떨어질수록 지진의 강도가 높아지는 것을 관찰할 수 있다. | 하지만 이것이 전세계적으로 적용되는 것은 아니다. | 예외는 중국, 몽골, 러시아 이다. | . #q_2.b.solution() . 3) Which prefectures have high population density? . Run the next code cell (without changes) to create a GeoDataFrame prefectures that contains the geographical boundaries of Japanese prefectures. . &#51064;&#44396;&#48128;&#46020;&#44032; &#45458;&#51008; &#54788; . &#45936;&#51060;&#53552; &#47196;&#46300; . prefectures = gpd.read_file(&quot;archive japan-prefecture-boundaries japan-prefecture-boundaries japan-prefecture-boundaries.shp&quot;) prefectures.set_index(&#39;prefecture&#39;, inplace=True) prefectures.head() . geometry . prefecture . Aichi MULTIPOLYGON (((137.09523 34.65330, 137.09546 ... | . Akita MULTIPOLYGON (((139.55725 39.20330, 139.55765 ... | . Aomori MULTIPOLYGON (((141.39860 40.92472, 141.39806 ... | . Chiba MULTIPOLYGON (((139.82488 34.98967, 139.82434 ... | . Ehime MULTIPOLYGON (((132.55859 32.91224, 132.55904 ... | . The next code cell creates a DataFrame stats containing the population, area (in square kilometers), and population density (per square kilometer) for each Japanese prefecture. Run the code cell without changes. . population = pd.read_csv(&quot;archive japan-prefecture-population.csv&quot;) population.set_index(&#39;prefecture&#39;, inplace=True) # Calculate area (in square kilometers) of each prefecture area_sqkm = pd.Series(prefectures.geometry.to_crs(epsg=32654).area / 10**6, name=&#39;area_sqkm&#39;) stats = population.join(area_sqkm) # Add density (per square kilometer) of each prefecture stats[&#39;density&#39;] = stats[&quot;population&quot;] / stats[&quot;area_sqkm&quot;] stats.head() . population area_sqkm density . prefecture . Tokyo 12868000 | 1800.614782 | 7146.448049 | . Kanagawa 8943000 | 2383.038975 | 3752.771186 | . Osaka 8801000 | 1923.151529 | 4576.342460 | . Aichi 7418000 | 5164.400005 | 1436.372085 | . Saitama 7130000 | 3794.036890 | 1879.264806 | . &#51064;&#44396;&#48128;&#46020; &#49884;&#44033;&#54868; . Use the next code cell to create a choropleth map to visualize population density. . m_3 = folium.Map(location=[35,136], tiles=&#39;cartodbpositron&#39;, zoom_start=5) # Your code here: create a choropleth map to visualize population density Choropleth(geo_data=prefectures[&#39;geometry&#39;].__geo_interface__, data=stats[&#39;density&#39;], key_on=&quot;feature.id&quot;, fill_color=&#39;PuRd&#39;, legend_name=&#39;Population density (per square kilometer)&#39; ).add_to(m_3) # Uncomment to see a hint #q_3.a.hint() # View the map embed_map(m_3, &#39;q_3.html&#39;) . #q_3.a.check() # Uncomment to see our solution (your code may look different!) #q_3.a.solution() . Which three prefectures have relatively higher density than the others? Are they spread throughout the country, or all located in roughly the same geographical region? (If you&#39;re unfamiliar with Japanese geography, you might find this map useful to answer the questions.) . &#51064;&#44396;&#48128;&#46020;&#44032; &#45458;&#51008; &#54788; . Tokyo, Kanagawa,Osaka 이 인구밀도가 높다. | Tokyo, Kanagawa 이 밀접해있다. | . #q_3.b.solution() . 4) Which high-density prefecture is prone to high-magnitude earthquakes? . Create a map to suggest one prefecture that might benefit from earthquake reinforcement. Your map should visualize both density and earthquake magnitude. . &#44508;&#47784;&#44032; &#53360; &#51648;&#51652;&#51060; &#51068;&#50612;&#45216; &#49688; &#51080;&#45716; &#51064;&#44396;&#48128;&#46020;&#44032; &#45458;&#51008; &#54788; . m_4 = folium.Map(location=[35,136], tiles=&#39;cartodbpositron&#39;, zoom_start=5) # Your code here: create a map def color_producer(magnitude): if magnitude &gt; 6.5: return &#39;yellow&#39; else: return &#39;green&#39; Choropleth( geo_data=prefectures[&#39;geometry&#39;].__geo_interface__, data=stats[&#39;density&#39;], key_on=&quot;feature.id&quot;, fill_color=&#39;BuPu&#39;, legend_name=&#39;Population density (per square kilometer)&#39;).add_to(m_4) for i in range(0,len(earthquakes)): folium.Circle( location=[earthquakes.iloc[i][&#39;Latitude&#39;], earthquakes.iloc[i][&#39;Longitude&#39;]], popup=(&quot;{} ({})&quot;).format( earthquakes.iloc[i][&#39;Magnitude&#39;], earthquakes.iloc[i][&#39;DateTime&#39;].year), radius=earthquakes.iloc[i][&#39;Magnitude&#39;]**5.5, color=color_producer(earthquakes.iloc[i][&#39;Magnitude&#39;])).add_to(m_4) # Uncomment to see a hint #q_4.a.hint() # View the map embed_map(m_4, &#39;q_4.html&#39;) . #q_4.a.check() # Uncomment to see our solution (your code may look different!) #q_4.a.solution() . Which prefecture do you recommend for extra earthquake reinforcement? . &#51648;&#51652;&#51060; &#45916; &#51068;&#50612;&#45216; &#49688; &#51080;&#45716; &#54788; . 전체적으로 지진을 피할 수는 없지만 Tokyo, Kanagawa 근처에는 규모가 큰 지진이 일어난다. | Osaka는 인구밀도는 Tokyo 보다 상대적으로 낮지만 강한 지진을 겪었다. | Kanagawa 는 해안 주변으로 인한 지진을 겪었다. | 종합적으로 살펴보면 인구밀도가 낮고 해안가 주변을 피하는 것이 좋다고 생각한다. | . #q_4.b.solution() . Keep going . Learn how to convert names of places to geographic coordinates with geocoding. You&#39;ll also explore special ways to join information from multiple GeoDataFrames. . . Have questions or comments? Visit the course discussion forum to chat with other learners. .",
            "url": "https://yeaun12.github.io/blog/jupyter/python/2022/05/28/exercise(3)-interactive-maps.html",
            "relUrl": "/jupyter/python/2022/05/28/exercise(3)-interactive-maps.html",
            "date": " • May 28, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Exercise: Coordinate Reference Systems",
            "content": "This notebook is an exercise in the Geospatial Analysis course. You can reference the tutorial at this link. . . Introduction . You are a bird conservation expert and want to understand migration patterns of purple martins. In your research, you discover that these birds typically spend the summer breeding season in the eastern United States, and then migrate to South America for the winter. But since this bird is under threat of endangerment, you&#39;d like to take a closer look at the locations that these birds are more likely to visit. . There are several protected areas in South America, which operate under special regulations to ensure that species that migrate (or live) there have the best opportunity to thrive. You&#39;d like to know if purple martins tend to visit these areas. To answer this question, you&#39;ll use some recently collected data that tracks the year-round location of eleven different birds. . Before you get started, run the code cell below to set everything up. . import pandas as pd import geopandas as gpd from shapely.geometry import LineString . Exercises . 1) Load the data. . Run the next code cell (without changes) to load the GPS data into a pandas DataFrame birds_df. . &#45936;&#51060;&#53552; &#47196;&#46300; . birds_df = pd.read_csv(&quot;archive/purple_martin.csv&quot;, parse_dates=[&#39;timestamp&#39;]) print(&quot;There are {} different birds in the dataset.&quot;.format(birds_df[&quot;tag-local-identifier&quot;].nunique())) birds_df.head() . There are 11 different birds in the dataset. . timestamp location-long location-lat tag-local-identifier . 0 2014-08-15 05:56:00 | -88.146014 | 17.513049 | 30448 | . 1 2014-09-01 05:59:00 | -85.243501 | 13.095782 | 30448 | . 2 2014-10-30 23:58:00 | -62.906089 | -7.852436 | 30448 | . 3 2014-11-15 04:59:00 | -61.776826 | -11.723898 | 30448 | . 4 2014-11-30 09:59:00 | -61.241538 | -11.612237 | 30448 | . There are 11 birds in the dataset, where each bird is identified by a unique value in the &quot;tag-local-identifier&quot; column. Each bird has several measurements, collected at different times of the year. . Use the next code cell to create a GeoDataFrame birds. . birds should have all of the columns from birds_df, along with a &quot;geometry&quot; column that contains Point objects with (longitude, latitude) locations. | Set the CRS of birds to {&#39;init&#39;: &#39;epsg:4326&#39;}. | . &#45936;&#51060;&#53552;&#49483; &#49373;&#49457; . 새들은 데이터셋의 tag-local-identifier 로 구분 - &gt; 일년 중 관측된 값을 가지고 있다. | 경도, 위도 위치가 있는 POINT 가 포함된 geometry 과 모든 열이 있어야한다. | crs 는 4326 으로 설정 | . birds = gpd.GeoDataFrame(birds_df, geometry=gpd.points_from_xy(birds_df[&quot;location-long&quot;], birds_df[&quot;location-lat&quot;])) # Your code here: Set the CRS to {&#39;init&#39;: &#39;epsg:4326&#39;} birds.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # Check your answer #q_1.check() . c: Users 82108 AppData Local Programs Python Python38-32 lib site-packages pyproj crs crs.py:130: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 in_crs_string = _prepare_from_proj_string(in_crs_string) . #q_1.hint() #q_1.solution() . 2) Plot the data. . Next, we load in the &#39;naturalearth_lowres&#39; dataset from GeoPandas, and set americas to a GeoDataFrame containing the boundaries of all countries in the Americas (both North and South America). Run the next code cell without changes. . &#49884;&#44033;&#54868;&#54616;&#44592; . world = gpd.read_file(gpd.datasets.get_path(&#39;naturalearth_lowres&#39;)) americas = world.loc[world[&#39;continent&#39;].isin([&#39;North America&#39;, &#39;South America&#39;])] americas.head() . pop_est continent name iso_a3 gdp_md_est geometry . 3 35623680 | North America | Canada | CAN | 1674000.0 | MULTIPOLYGON (((-122.84000 49.00000, -122.9742... | . 4 326625791 | North America | United States of America | USA | 18560000.0 | MULTIPOLYGON (((-122.84000 49.00000, -120.0000... | . 9 44293293 | South America | Argentina | ARG | 879400.0 | MULTIPOLYGON (((-68.63401 -52.63637, -68.25000... | . 10 17789267 | South America | Chile | CHL | 436100.0 | MULTIPOLYGON (((-68.63401 -52.63637, -68.63335... | . 16 10646714 | North America | Haiti | HTI | 19340.0 | POLYGON ((-71.71236 19.71446, -71.62487 19.169... | . Use the next code cell to create a single plot that shows both: (1) the country boundaries in the americas GeoDataFrame, and (2) all of the points in the birds_gdf GeoDataFrame. . Don&#39;t worry about any special styling here; just create a preliminary plot, as a quick sanity check that all of the data was loaded properly. In particular, you don&#39;t have to worry about color-coding the points to differentiate between birds, and you don&#39;t have to differentiate starting points from ending points. We&#39;ll do that in the next part of the exercise. . &#51648;&#46020; &#49884;&#44033;&#54868; . 아메리카의 경계 | birds_gdf의 모든 point | . ax = americas.plot(figsize=(10,10), color=&#39;white&#39;, linestyle=&#39;:&#39;, edgecolor=&#39;gray&#39;) birds.plot(ax=ax, markersize=10) # Uncomment to see a hint #q_2.hint() . &lt;AxesSubplot:&gt; . ax 로 도화지 설정 후 점찍기 . #q_2.check() # Uncomment to see our solution (your code may look different!) #q_2.solution() . 3) Where does each bird start and end its journey? (Part 1) . Now, we&#39;re ready to look more closely at each bird&#39;s path. Run the next code cell to create two GeoDataFrames: . path_gdf contains LineString objects that show the path of each bird. It uses the LineString() method to create a LineString object from a list of Point objects. | start_gdf contains the starting points for each bird. | . path_df = birds.groupby(&quot;tag-local-identifier&quot;)[&#39;geometry&#39;].apply(list).apply(lambda x: LineString(x)).reset_index() path_gdf = gpd.GeoDataFrame(path_df, geometry=path_df.geometry) path_gdf.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # GeoDataFrame showing starting point for each bird start_df = birds.groupby(&quot;tag-local-identifier&quot;)[&#39;geometry&#39;].apply(list).apply(lambda x: x[0]).reset_index() start_gdf = gpd.GeoDataFrame(start_df, geometry=start_df.geometry) start_gdf.crs = {&#39;init&#39; :&#39;epsg:4326&#39;} # Show first five rows of GeoDataFrame start_gdf.head() . c: Users 82108 AppData Local Programs Python Python38-32 lib site-packages pyproj crs crs.py:130: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 in_crs_string = _prepare_from_proj_string(in_crs_string) c: Users 82108 AppData Local Programs Python Python38-32 lib site-packages pyproj crs crs.py:130: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 in_crs_string = _prepare_from_proj_string(in_crs_string) . tag-local-identifier geometry . 0 30048 | POINT (-90.12992 20.73242) | . 1 30054 | POINT (-93.60861 46.50563) | . 2 30198 | POINT (-80.31036 25.92545) | . 3 30263 | POINT (-76.78146 42.99209) | . 4 30275 | POINT (-76.78213 42.99207) | . Use the next code cell to create a GeoDataFrame end_gdf containing the final location of each bird. . The format should be identical to that of start_gdf, with two columns (&quot;tag-local-identifier&quot; and &quot;geometry&quot;), where the &quot;geometry&quot; column contains Point objects. | Set the CRS of end_gdf to {&#39;init&#39;: &#39;epsg:4326&#39;}. | . &#49352;&#51032; &#52572;&#51333; &#50948;&#52824; &#54364;&#49884; . end_gdf = birds.groupby(&quot;tag-local-identifier&quot;)[&#39;geometry&#39;].apply(list).apply(lambda x: x[-1]).reset_index() end_gdf = gpd.GeoDataFrame(end_gdf, geometry=end_gdf.geometry) end_gdf.crs = {&#39;init&#39;: &#39;epsg:4326&#39;} # Check your answer #q_3.check() . c: Users 82108 AppData Local Programs Python Python38-32 lib site-packages pyproj crs crs.py:130: FutureWarning: &#39;+init=&lt;authority&gt;:&lt;code&gt;&#39; syntax is deprecated. &#39;&lt;authority&gt;:&lt;code&gt;&#39; is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6 in_crs_string = _prepare_from_proj_string(in_crs_string) . end_gdf.crs . &lt;Geographic 2D CRS: +init=epsg:4326 +type=crs&gt; Name: WGS 84 Axis Info [ellipsoidal]: - lon[east]: Longitude (degree) - lat[north]: Latitude (degree) Area of Use: - name: World. - bounds: (-180.0, -90.0, 180.0, 90.0) Datum: World Geodetic System 1984 ensemble - Ellipsoid: WGS 84 - Prime Meridian: Greenwich . #q_3.hint() #q_3.solution() . 4) Where does each bird start and end its journey? (Part 2) . Use the GeoDataFrames from the question above (path_gdf, start_gdf, and end_gdf) to visualize the paths of all birds on a single map. You may also want to use the americas GeoDataFrame. . &#49352;&#51032; &#49884;&#51089;&#51216;&#44284; &#45149;&#51216;&#51012; &#54364;&#49884;, &#51060;&#46041;&#44221;&#47196; &#54364;&#49884; . ax = americas.plot(figsize=(20, 20), color=&#39;white&#39;, linestyle=&#39;:&#39;, edgecolor=&#39;gray&#39;) start_gdf.plot(ax=ax, color=&#39;green&#39;, markersize=30) #시작점 표현 path_gdf.plot(ax=ax, cmap=&#39;tab20b&#39;, linestyle=&#39;-&#39;, linewidth=1, zorder=1) #선 표현 end_gdf.plot(ax=ax, color=&#39;blue&#39;, markersize=30) #도착점 표현 # Uncomment to see a hint #q_4.hint() . &lt;AxesSubplot:&gt; . #q_4.check() # Uncomment to see our solution (your code may look different!) #q_4.solution() . 5) Where are the protected areas in South America? (Part 1) . It looks like all of the birds end up somewhere in South America. But are they going to protected areas? . In the next code cell, you&#39;ll create a GeoDataFrame protected_areas containing the locations of all of the protected areas in South America. The corresponding shapefile is located at filepath protected_filepath. . &#45224;&#50500;&#47700;&#47532;&#52852; &#48372;&#54840; &#51648;&#50669; &#45936;&#51060;&#53552; &#47196;&#46300; . protected_filepath = &quot;archive/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile/SAPA_Aug2019-shapefile-polygons.shp&quot; # Your code here protected_areas = gpd.read_file(protected_filepath) protected_areas # Check your answer #q_5.check() . WDPAID WDPA_PID PA_DEF NAME ORIG_NAME DESIG DESIG_ENG DESIG_TYPE IUCN_CAT INT_CRIT ... GOV_TYPE OWN_TYPE MANG_AUTH MANG_PLAN VERIF METADATAID SUB_LOC PARENT_ISO ISO3 geometry . 0 14067.0 | 14067 | 1 | Het Spaans Lagoen | Het Spaans Lagoen | Ramsar Site, Wetland of International Importance | Ramsar Site, Wetland of International Importance | International | Not Reported | Not Reported | ... | Not Reported | Not Reported | Not Reported | Management plan is not implemented and not ava... | State Verified | 1856 | Not Reported | NLD | ABW | POLYGON ((-69.97523 12.47379, -69.97523 12.473... | . 1 14003.0 | 14003 | 1 | Bubali Pond Bird Sanctuary | Bubali Pond Bird Sanctuary | Bird Sanctuary | Bird Sanctuary | National | Not Reported | Not Applicable | ... | Not Reported | Not Reported | Not Reported | Not Reported | State Verified | 1899 | Not Reported | NLD | ABW | POLYGON ((-70.04734 12.56329, -70.04615 12.563... | . 2 555624439.0 | 555624439 | 1 | Arikok National Park | Arikok National Park | National Park | National Park | National | Not Reported | Not Applicable | ... | Non-profit organisations | Non-profit organisations | Fundacion Parke Nacional Arikok | Not Reported | State Verified | 1899 | Not Reported | NLD | ABW | MULTIPOLYGON (((-69.96302 12.48384, -69.96295 ... | . 3 303894.0 | 303894 | 1 | Madidi | Madidi | Area Natural de Manejo Integrado | Natural Integrated Management Area | National | Not Reported | Not Applicable | ... | Federal or national ministry or agency | Not Reported | Not Reported | Not Reported | State Verified | 1860 | BO-L | BOL | BOL | POLYGON ((-68.59060 -14.43388, -68.59062 -14.4... | . 4 303893.0 | 303893 | 1 | Apolobamba | Apolobamba | Area Natural de Manejo Integado Nacional | National Natural Integrated Management Area | National | Not Reported | Not Applicable | ... | Federal or national ministry or agency | Not Reported | Not Reported | Not Reported | State Verified | 1860 | BO-L | BOL | BOL | POLYGON ((-69.20949 -14.73334, -69.20130 -14.7... | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 4743 555624155.0 | 555624155 | 1 | La Calera | La Calera | Reserva Natural de la Defensa | Nature Reserve of Defense | National | Not Reported | Not Applicable | ... | Federal or national ministry or agency | Not Reported | Ejercito Argentino | Not Reported | State Verified | 1852 | AR-X | ARG | ARG | POLYGON ((-64.40290 -31.35654, -64.40207 -31.3... | . 4744 555624161.0 | 555624161 | 1 | Punta Buenos Aires | Punta Buenos Aires | Reserva Natural de la Defensa | Nature Reserve of Defense | National | Not Reported | Not Applicable | ... | Federal or national ministry or agency | Not Reported | Armada Argentina | Not Reported | State Verified | 1852 | AR-U | ARG | ARG | POLYGON ((-64.10216 -42.25114, -64.10669 -42.2... | . 4745 555624160.0 | 555624160 | 1 | Ascochinga | Ascochinga | Reserva Natural de la Defensa | Nature Reserve of Defense | National | Not Reported | Not Applicable | ... | Federal or national ministry or agency | Not Reported | Fuerza Áerea Argentina | Not Reported | State Verified | 1852 | AR-X | ARG | ARG | MULTIPOLYGON (((-64.25944 -30.96300, -64.25832... | . 4746 555624158.0 | 555624158 | 1 | Baterías - Charles Darwin | Baterías - Charles Darwin | Reserva Natural de la Defensa | Nature Reserve of Defense | National | Not Reported | Not Applicable | ... | Federal or national ministry or agency | Not Reported | Armada Argentina | Not Reported | State Verified | 1852 | AR-B | ARG | ARG | POLYGON ((-61.69538 -38.96695, -61.69749 -38.9... | . 4747 555624858.0 | 555624858 | 1 | Los Alerces National Park | Parque Nacional Los Alerces | Sitio Patrimonio de la Humanidad (natural o mi... | World Heritage Site (natural or mixed) | International | II | (vii)(x) | ... | Federal or national ministry or agency | Not Reported | Not Reported | Not Reported | State Verified | 1852 | AR-U | ARG | ARG | POLYGON ((-71.63519 -42.56531, -71.63460 -42.5... | . 4748 rows × 29 columns . #q_5.hint() #q_5.solution() . 6) Where are the protected areas in South America? (Part 2) . Create a plot that uses the protected_areas GeoDataFrame to show the locations of the protected areas in South America. (You&#39;ll notice that some protected areas are on land, while others are in marine waters.) . &#45224;&#50500;&#47700;&#47532;&#52852;&#51032; &#48372;&#54840;&#51648;&#50669; &#51648;&#46020; &#49884;&#44033;&#54868; . 육지와 바다에 골고루 분포 | . south_america = americas.loc[americas[&#39;continent&#39;]==&#39;South America&#39;] # Your code here: plot protected areas in South America ax = south_america.plot(figsize=(10,10), color=&#39;white&#39;, edgecolor=&#39;gray&#39;) protected_areas.plot(ax=ax, alpha=0.2) # Uncomment to see a hint #q_6.hint() . &lt;AxesSubplot:&gt; . #q_6.check() # Uncomment to see our solution (your code may look different!) #q_6.solution() . 7) What percentage of South America is protected? . You&#39;re interested in determining what percentage of South America is protected, so that you know how much of South America is suitable for the birds. . As a first step, you calculate the total area of all protected lands in South America (not including marine area). To do this, you use the &quot;REP_AREA&quot; and &quot;REP_M_AREA&quot; columns, which contain the total area and total marine area, respectively, in square kilometers. . Run the code cell below without changes. . &#45224;&#50500;&#47700;&#47532;&#52852;&#51032; &#48372;&#54840;&#44032; &#46104;&#45716; &#51221;&#46020; &#54028;&#50501; . P_Area = sum(protected_areas[&#39;REP_AREA&#39;]-protected_areas[&#39;REP_M_AREA&#39;]) print(&quot;South America has {} square kilometers of protected areas.&quot;.format(P_Area)) . South America has 5396761.9116883585 square kilometers of protected areas. . Then, to finish the calculation, you&#39;ll use the south_america GeoDataFrame. . south_america.head() . pop_est continent name iso_a3 gdp_md_est geometry . 9 44293293 | South America | Argentina | ARG | 879400.0 | MULTIPOLYGON (((-68.63401 -52.63637, -68.25000... | . 10 17789267 | South America | Chile | CHL | 436100.0 | MULTIPOLYGON (((-68.63401 -52.63637, -68.63335... | . 20 2931 | South America | Falkland Is. | FLK | 281.8 | POLYGON ((-61.20000 -51.85000, -60.00000 -51.2... | . 28 3360148 | South America | Uruguay | URY | 73250.0 | POLYGON ((-57.62513 -30.21629, -56.97603 -30.1... | . 29 207353391 | South America | Brazil | BRA | 3081000.0 | POLYGON ((-53.37366 -33.76838, -53.65054 -33.2... | . Calculate the total area of South America by following these steps: . Calculate the area of each country using the area attribute of each polygon (with EPSG 3035 as the CRS), and add up the results. The calculated area will be in units of square meters. | Convert your answer to have units of square kilometeters. | . km로 변환 | crs는 epsg 3035 사용 | . totalArea = sum(south_america.geometry.to_crs(epsg=3035).area) / 10**6 # Check your answer #q_7.check() . #q_7.hint() #q_7.solution() . Run the code cell below to calculate the percentage of South America that is protected. . percentage_protected = P_Area/totalArea print(&#39;Approximately {}% of South America is protected.&#39;.format(round(percentage_protected*100, 2))) . Approximately 30.39% of South America is protected. . 남아메리카의 약 30.39 % 가 보호받고 있다. | . 8) Where are the birds in South America? . So, are the birds in protected areas? . Create a plot that shows for all birds, all of the locations where they were discovered in South America. Also plot the locations of all protected areas in South America. . To exclude protected areas that are purely marine areas (with no land component), you can use the &quot;MARINE&quot; column (and plot only the rows in protected_areas[protected_areas[&#39;MARINE&#39;]!=&#39;2&#39;], instead of every row in the protected_areas GeoDataFrame). . &#48156;&#44204;&#46108; &#49352;&#51032; &#50948;&#52824; &#49884;&#44033;&#54868; . ax = south_america.plot(figsize=(10,10), color=&#39;white&#39;, edgecolor=&#39;gray&#39;) protected_areas[protected_areas[&#39;MARINE&#39;]!=&#39;2&#39;].plot(ax=ax, alpha=0.4, zorder=1) birds[birds.geometry.y &lt; 0].plot(ax=ax, color=&#39;red&#39;, alpha=0.6, markersize=10, zorder=2) # Uncomment to see a hint #q_8.hint() . &lt;AxesSubplot:&gt; . #q_8.check() # Uncomment to see our solution (your code may look different!) #q_8.solution() . Keep going . Create stunning interactive maps with your geospatial data. . . Have questions or comments? Visit the course discussion forum to chat with other learners. .",
            "url": "https://yeaun12.github.io/blog/jupyter/python/2022/05/28/exercise(2)-coordinate-reference-systems.html",
            "relUrl": "/jupyter/python/2022/05/28/exercise(2)-coordinate-reference-systems.html",
            "date": " • May 28, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Matplotlib, Seaborn, Seaborn and Matplotlib",
            "content": "&#44592;&#48376; &#49444;&#51221; &amp; &#54056;&#53412;&#51648; &#47196;&#46300; . import matplotlib.pyplot as plt import seaborn as sns import pandas as pd import numpy as np import this from mpl_toolkits.mplot3d import Axes3D from numpy.random import rand import matplotlib.image as mpimg import matplotlib.animation as animation . %matplotlib inline . 1. Matplotlib . fig, ax = plt.subplots() #도화지 깔기 ax.plot([1, 2, 4, 9, 5, 3]) #그림 그리기 plt.show() #화면 출력 . x = np.linspace(-2, 2, 500) y = x**2 plt.plot(x, y) plt.title(&quot;Square function&quot;) #제목 plt.xlabel(&quot;x&quot;) #x축 plt.ylabel(&quot;y = x**2&quot;) #y축 plt.grid(True) #눈금 plt.show() . plt.plot([0, 100, 100, 0, 0], [0, 0, 100, 100, 0], &quot;r-&quot;, [0, 100, 50, 0, 100], [0, 100, 130, 100, 0], &quot;g--&quot;) #r-은 빨간색(red) 실선, g--는 초록색 점선을 의미 plt.axis([-10, 110, -10, 140]) plt.show() . x = np.linspace(-1.4, 1.4, 30) plt.plot(x, x, &#39;g--&#39;, x, x**2, &#39;r:&#39;, x, x**3, &#39;b^&#39;) # r: 은 빨간색 점, b^는 세모선 의미 plt.show() . x = np.linspace(-1.4, 1.4, 30) line1, line2, line3 = plt.plot(x, x, &#39;g--&#39;, x, x**2, &#39;r:&#39;, x, x**3, &#39;b^&#39;) line1.set_dash_capstyle(&quot;round&quot;) #대쉬 스타일 line1.set_linewidth(3.0) #선의 두께 line3.set_alpha(0.2) #투명도 plt.show() plt.savefig(&quot;my_square_function.png&quot;, transparent=True) #그림 저장 . &lt;Figure size 432x288 with 0 Axes&gt; . x = np.linspace(-1.4, 1.4, 30) fig, ax = plt.subplots(2, 2) #(행의 개수, 열의 개수) plt.subplot(2, 2, 1) # 2 행 2 열 크기의 격자 중 첫 번째 부분 그래프 = 좌측 상단 1번 plt.plot(x, x) plt.subplot(2, 2, 2) # 2 행 2 열 크기의 격자 중 두 번째 부분 그래프 = 우측 상단 2번 plt.plot(x, x**2) plt.subplot(2, 2, 3) # 2 행 2 열 크기의 격자 중 세 번째 부분 그래프 = 좌측 하단 3번 plt.plot(x, x**3) plt.subplot(2, 2, 4) # 2 행 2 열 크기의 격자 중 네 번째 부분 그래프 = 우측 하단 4번 plt.plot(x, x**4) plt.show() . plt.subplot2grid((3,3), (0, 0), rowspan=2, colspan=2) plt.plot(x, x**2) plt.subplot2grid((3,3), (0, 2)) plt.plot(x, x**3) plt.subplot2grid((3,3), (1, 2), rowspan=2) plt.plot(x, x**4) plt.subplot2grid((3,3), (2, 0), colspan=2) plt.plot(x, x**5) plt.show() #colspan, rowspan은 각각의 방향으로 확장을 의미 (숫자는 칸 수) . #여러개의 그림을 그림 x = np.linspace(-1.4, 1.4, 30) plt.figure(1) plt.subplot(211) plt.plot(x, x**2) plt.title(&quot;Square and Cube&quot;) plt.subplot(212) plt.plot(x, x**3) plt.figure(2, figsize=(10, 5)) plt.subplot(121) plt.plot(x, x**4) plt.title(&quot;y = x**4&quot;) plt.subplot(122) plt.plot(x, x**5) plt.title(&quot;y = x**5&quot;) plt.plot(x, -x**3, &quot;r:&quot;) plt.show() . x = np.linspace(-1.5, 1.5, 30) px = 0.8 py = px**2 plt.plot(x, x**2, &quot;b-&quot;, px, py, &quot;ro&quot;) plt.text(0, 1.5, &quot;Square function n$y = x^2$&quot;, fontsize=20, color=&#39;blue&#39;, horizontalalignment=&quot;center&quot;) plt.text(px - 0.08, py, &quot;Beautiful point&quot;, ha=&quot;right&quot;, weight=&quot;heavy&quot;) plt.text(px, py, &quot;x = %0.2f ny = %0.2f&quot;%(px, py), rotation=50, color=&#39;gray&#39;) #위의 세 줄은 각각 text 표시 plt.show() . plt.plot(x, x**2, px, py, &quot;ro&quot;) plt.annotate(&quot;Beautiful point&quot;, xy=(px, py), xytext=(px-1.3,py+0.5), color=&quot;green&quot;, weight=&quot;heavy&quot;, fontsize=14, arrowprops={&quot;facecolor&quot;: &quot;lightgreen&quot;}) #화살표 plt.show() . plt.plot(x, x**2, px, py, &quot;ro&quot;) bbox_props = dict(boxstyle=&quot;rarrow,pad=0.3&quot;, ec=&quot;b&quot;, lw=2, fc=&quot;lightblue&quot;) plt.text(px-0.2, py, &quot;Beautiful point&quot;, bbox=bbox_props, ha=&quot;right&quot;) #화살표박스 bbox_props = dict(boxstyle=&quot;round4,pad=1,rounding_size=0.2&quot;, ec=&quot;black&quot;, fc=&quot;#EEEEFF&quot;, lw=5) plt.text(0, 1.5, &quot;Square function n$y = x^2$&quot;, fontsize=20, color=&#39;black&#39;, ha=&quot;center&quot;, bbox=bbox_props) #큰 네모박스 plt.show() . with plt.xkcd(): plt.plot(x, x**2, px, py, &quot;ro&quot;) bbox_props = dict(boxstyle=&quot;rarrow,pad=0.3&quot;, ec=&quot;b&quot;, lw=2, fc=&quot;lightblue&quot;) plt.text(px-0.2, py, &quot;Beautiful point&quot;, bbox=bbox_props, ha=&quot;right&quot;) bbox_props = dict(boxstyle=&quot;round4,pad=1,rounding_size=0.2&quot;, ec=&quot;black&quot;, fc=&quot;#EEEEFF&quot;, lw=5) plt.text(0, 1.5, &quot;Square function n$y = x^2$&quot;, fontsize=20, color=&#39;black&#39;, ha=&quot;center&quot;, bbox=bbox_props) plt.show() #손으로 쓴 듯한 느낌을 표현 . x = np.linspace(-1.4, 1.4, 50) plt.plot(x, x**2, &quot;r--&quot;, label=&quot;Square function&quot;) plt.plot(x, x**3, &quot;g-&quot;, label=&quot;Cube function&quot;) plt.legend(loc=&quot;best&quot;) #legend plt.grid(True) plt.show() . x = np.linspace(-2, 2, 100) fig, ax = plt.subplots(ncols=3, figsize=(15, 10)) ax[0].plot(x, x**3) ax[0].grid(True) ax[0].set_title(&quot;Default ticks&quot;) ax[1].plot(x, x**3) ax[1].grid(True) ax[1].set_xticks(np.arange(-2, 2, 1)) ax[1].set_title(&quot;Manual ticks on the x-axis&quot;) ax[2].plot(x, x**3) ax[2].grid(True) ax[2].minorticks_on() ax[2].set_xticks([-2, 0, 1, 2], minor=False) ax[2].set_yticks(np.arange(-5, 5, 1)) ax[2].set_yticklabels([&quot;min&quot;, -4, -3, -2, -1, 0, 1, 2, 3, &quot;max&quot;]) ax[2].set_title(&quot;Manual ticks and tick labels n(plus minor ticks) on the y-axis&quot;) plt.show() . radius = 1 theta = np.linspace(0, 2*np.pi*radius, 1000) plt.subplot(111, projection=&#39;polar&#39;) plt.plot(theta, np.sin(5*theta), &quot;g-&quot;) plt.plot(theta, 0.5*np.cos(20*theta), &quot;b-&quot;) plt.show() . x = np.linspace(-5, 5, 50) y = np.linspace(-5, 5, 50) X, Y = np.meshgrid(x, y) R = np.sqrt(X**2 + Y**2) Z = np.sin(R) figure = plt.figure(1, figsize = (12, 4)) subplot3d = plt.subplot(111, projection=&#39;3d&#39;) # 이제현 주: Axes 객체입니다. surface = subplot3d.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.cm.coolwarm, linewidth=0.1) plt.show() . plt.contourf(X, Y, Z, cmap=plt.cm.coolwarm) plt.colorbar() plt.show() . x, y = rand(2, 100) fig, ax = plt.subplots() ax.scatter(x, y) plt.show() . x, y, scale = rand(3, 100) scale = 500 * scale ** 5 plt.scatter(x, y, s=scale) #점 크기 조정 plt.show() . for color in [&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;]: n = 100 x, y = rand(2, n) scale = 500.0 * rand(n) ** 5 plt.scatter(x, y, s=scale, c=color, alpha=0.3, edgecolors=&#39;blue&#39;) plt.grid(True) plt.show() #100개씩 3번을 돌리니까 n은 300개 . from numpy.random import randn def plot_line(axis, slope, intercept, **kargs): xmin, xmax = axis.get_xlim() plt.plot([xmin, xmax], [xmin*slope+intercept, xmax*slope+intercept], **kargs) x = randn(1000) y = 0.5*x + 5 + randn(1000)*2 plt.axis([-2.5, 2.5, -5, 15]) plt.scatter(x, y, alpha=0.2) plt.plot(1, 0, &quot;ro&quot;) plt.vlines(1, -5, 0, color=&quot;red&quot;) #수직선 그리기 plt.hlines(0, -2.5, 1, color=&quot;red&quot;) #수평선 그리기 plot_line(axis=plt.gca(), slope=0.5, intercept=5, color=&quot;magenta&quot;) plt.grid(True) plt.show() . data1 = np.random.randn(400) data2 = np.random.randn(500) + 3 data3 = np.random.randn(450) + 6 data4a = np.random.randn(200) + 9 data4b = np.random.randn(100) + 10 plt.hist(data1, bins=5, color=&#39;g&#39;, alpha=0.75, label=&#39;bar hist&#39;) # default histtype=&#39;bar&#39; plt.hist(data2, color=&#39;b&#39;, alpha=0.65, histtype=&#39;stepfilled&#39;, label=&#39;stepfilled hist&#39;) plt.hist(data3, color=&#39;r&#39;, histtype=&#39;step&#39;, label=&#39;step hist&#39;) plt.hist((data4a, data4b), color=(&#39;r&#39;,&#39;m&#39;), alpha=0.55, histtype=&#39;barstacked&#39;, label=(&#39;barstacked a&#39;, &#39;barstacked b&#39;)) plt.xlabel(&quot;Value&quot;) plt.ylabel(&quot;Frequency&quot;) plt.legend() plt.grid(True) plt.show() . img = mpimg.imread(&#39;my_square_function.png&#39;) print(img.shape, img.dtype) plt.imshow(img) plt.show() . (288, 432, 4) float32 . matplotlib.rc(&#39;animation&#39;, html=&#39;jshtml&#39;) x = np.linspace(-1, 1, 100) y = np.sin(x**2*25) data = np.array([x, y]) fig = plt.figure() line, = plt.plot([], [], &quot;r-&quot;) # start with an empty plot plt.axis([-1.1, 1.1, -1.1, 1.1]) plt.plot([-0.5, 0.5], [0, 0], &quot;b-&quot;, [0, 0], [-0.5, 0.5], &quot;b-&quot;, 0, 0, &quot;ro&quot;) plt.grid(True) plt.title(&quot;Marvelous animation&quot;) # this function will be called at every iteration def update_line(num, data, line): line.set_data(data[..., :num] + np.random.rand(2, num) / 25) # we only plot the first `num` data points. return line, line_ani = animation.FuncAnimation(fig, update_line, frames=100, fargs=(data, line), interval=67) plt.close() line_ani . &lt;/input&gt; Once Loop Reflect 2. seaborn . data_BM = pd.read_csv(&#39;bigmart_data.csv&#39;) data_BM = data_BM.dropna(how=&quot;any&quot;) data_BM[&quot;Visibility_Scaled&quot;] = data_BM[&quot;Item_Visibility&quot;] * 100 data_BM.head() . Item_Identifier Item_Weight Item_Fat_Content Item_Visibility Item_Type Item_MRP Outlet_Identifier Outlet_Establishment_Year Outlet_Size Outlet_Location_Type Outlet_Type Item_Outlet_Sales Visibility_Scaled . 0 FDA15 | 9.300 | Low Fat | 0.016047 | Dairy | 249.8092 | OUT049 | 1999 | Medium | Tier 1 | Supermarket Type1 | 3735.1380 | 1.604730 | . 1 DRC01 | 5.920 | Regular | 0.019278 | Soft Drinks | 48.2692 | OUT018 | 2009 | Medium | Tier 3 | Supermarket Type2 | 443.4228 | 1.927822 | . 2 FDN15 | 17.500 | Low Fat | 0.016760 | Meat | 141.6180 | OUT049 | 1999 | Medium | Tier 1 | Supermarket Type1 | 2097.2700 | 1.676007 | . 4 NCD19 | 8.930 | Low Fat | 0.000000 | Household | 53.8614 | OUT013 | 1987 | High | Tier 3 | Supermarket Type1 | 994.7052 | 0.000000 | . 5 FDP36 | 10.395 | Regular | 0.000000 | Baking Goods | 51.4008 | OUT018 | 2009 | Medium | Tier 3 | Supermarket Type2 | 556.6088 | 0.000000 | . sns.lineplot(x=&quot;Item_Weight&quot;, y=&quot;Item_MRP&quot;,data=data_BM[:50]); . sns.barplot(x=&quot;Item_Type&quot;, y=&quot;Item_MRP&quot;, data=data_BM[:5]) . &lt;AxesSubplot:xlabel=&#39;Item_Type&#39;, ylabel=&#39;Item_MRP&#39;&gt; . sns.distplot(data_BM[&#39;Item_MRP&#39;]) . c: Users 82108 AppData Local Programs Python Python38-32 lib site-packages seaborn distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . &lt;AxesSubplot:xlabel=&#39;Item_MRP&#39;, ylabel=&#39;Density&#39;&gt; . sns.boxplot(data_BM[&#39;Item_Outlet_Sales&#39;], orient=&#39;vertical&#39;) . c: Users 82108 AppData Local Programs Python Python38-32 lib site-packages seaborn _decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( c: Users 82108 AppData Local Programs Python Python38-32 lib site-packages seaborn _core.py:1326: UserWarning: Vertical orientation ignored with only `x` specified. warnings.warn(single_var_warning.format(&#34;Vertical&#34;, &#34;x&#34;)) . &lt;AxesSubplot:xlabel=&#39;Item_Outlet_Sales&#39;&gt; . sns.violinplot(data_BM[&#39;Item_Outlet_Sales&#39;], orient=&#39;vertical&#39;, color=&#39;magenta&#39;) . c: Users 82108 AppData Local Programs Python Python38-32 lib site-packages seaborn _decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation. warnings.warn( c: Users 82108 AppData Local Programs Python Python38-32 lib site-packages seaborn _core.py:1326: UserWarning: Vertical orientation ignored with only `x` specified. warnings.warn(single_var_warning.format(&#34;Vertical&#34;, &#34;x&#34;)) . &lt;AxesSubplot:xlabel=&#39;Item_Outlet_Sales&#39;&gt; . sns.relplot(x=&quot;Item_MRP&quot;, y=&quot;Item_Outlet_Sales&quot;, data=data_BM[:200], kind=&quot;scatter&quot;); . sns.relplot(x=&quot;Item_MRP&quot;, y=&quot;Item_Outlet_Sales&quot;, hue=&quot;Item_Type&quot;,data=data_BM[:200]); . sns.relplot(x=&quot;Item_MRP&quot;, y=&quot;Item_Outlet_Sales&quot;, data=data_BM[:200], kind=&quot;scatter&quot;, size=&quot;Visibility_Scaled&quot;, hue=&quot;Visibility_Scaled&quot;); . sns.relplot(x=&quot;Item_Weight&quot;, y=&quot;Item_Visibility&quot;,hue=&#39;Outlet_Size&#39;,style=&#39;Outlet_Size&#39;,col=&#39;Outlet_Size&#39;,data=data_BM[:100]); . sns.relplot(x=&quot;Item_Weight&quot;, y=&quot;Item_Visibility&quot;,hue=&#39;Outlet_Size&#39;,style=&#39;Outlet_Size&#39;,data=data_BM[:100]); . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;, kind=&#39;strip&#39;,data=data_BM[:250]); . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;, kind=&#39;swarm&#39;,data=data_BM[:250]); . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;box&quot;,data=data_BM); . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;violin&quot;,data=data_BM); . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;boxen&quot;,data=data_BM); . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;point&quot;,data=data_BM); . sns.catplot(x=&quot;Outlet_Size&quot;, y=&quot;Item_Outlet_Sales&quot;,kind=&quot;bar&quot;,data=data_BM); . plt.figure(figsize=(10,10)) sns.kdeplot(data_BM[&#39;Item_Visibility&#39;], shade=True); . plt.figure(figsize=(10,10)) sns.distplot(data_BM[&#39;Item_Outlet_Sales&#39;]); . c: Users 82108 AppData Local Programs Python Python38-32 lib site-packages seaborn distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms). warnings.warn(msg, FutureWarning) . iris = sns.load_dataset(&quot;iris&quot;) . sns.pairplot(iris, hue=&#39;species&#39;, height=2.5); . 3. seaborn and matplotlib . penguins = sns.load_dataset(&quot;penguins&quot;) . fig, axes = plt.subplots(ncols=2, figsize=(8,4)) fig.tight_layout() . fig, axes = plt.subplots(ncols=2,figsize=(8,4)) species_u = penguins[&quot;species&quot;].unique() for i, s in enumerate(species_u): axes[0].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s], penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s], c=f&quot;C{i}&quot;, label=s, alpha=0.3) axes[0].legend(species_u, title=&quot;species&quot;) axes[0].set_xlabel(&quot;Bill Length (mm)&quot;) axes[0].set_ylabel(&quot;Bill Depth (mm)&quot;) fig.tight_layout() #그림그리기 . fig, axes = plt.subplots(ncols=2,figsize=(8,4)) species_u = penguins[&quot;species&quot;].unique() # plot 0 : matplotlib for i, s in enumerate(species_u): axes[0].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s], penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s], c=f&quot;C{i}&quot;, label=s, alpha=0.3) axes[0].legend(species_u, title=&quot;species&quot;) axes[0].set_xlabel(&quot;Bill Length (mm)&quot;) axes[0].set_ylabel(&quot;Bill Depth (mm)&quot;) sns.scatterplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, hue=&quot;species&quot;, data=penguins, alpha=0.3, ax=axes[1]) axes[1].set_xlabel(&quot;Bill Length (mm)&quot;) axes[1].set_ylabel(&quot;Bill Depth (mm)&quot;) fig.tight_layout() #1,2 그리기 . fig, axes = plt.subplots(ncols=2, figsize=(8, 4)) species_u = penguins[&quot;species&quot;].unique() #matplotlib + seaborn for i, s in enumerate(species_u): #matplotlib 산점도 axes[0].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s], penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s], c=f&quot;C{i}&quot;, label=s, alpha=0.3 ) #seaborn 추세선 sns.regplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, data=penguins.loc[penguins[&quot;species&quot;]==s], scatter=False, ax=axes[0]) axes[0].legend(species_u, title=&quot;species&quot;) axes[0].set_xlabel(&quot;Bill Length (mm)&quot;) axes[0].set_ylabel(&quot;Bill Depth (mm)&quot;) #seaborn + matplotlib #seaborn 산점도 sns.scatterplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, hue=&quot;species&quot;, data=penguins, alpha=0.3, ax=axes[1]) axes[1].set_xlabel(&quot;Bill Length (mm)&quot;) axes[1].set_ylabel(&quot;Bill Depth (mm)&quot;) for i, s in enumerate(species_u): #matplotlib 중심점 axes[1].scatter(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), c=f&quot;C{i}&quot;, alpha=1, marker=&quot;x&quot;, s=100 ) fig.tight_layout() . fig, ax = plt.subplots(figsize=(6,5)) #scatter plot sns.scatterplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, color=&quot;k&quot;, data=penguins, alpha=0.3, ax=ax, legend=False) #kde plot sns.kdeplot(x=&quot;bill_length_mm&quot;, y=&quot;bill_depth_mm&quot;, hue=&quot;species&quot;, data=penguins, alpha=0.5, ax=ax, legend=False) #text: species_u = penguins[&quot;species&quot;].unique() for i, s in enumerate(species_u): ax.text(penguins[&quot;bill_length_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), penguins[&quot;bill_depth_mm&quot;].loc[penguins[&quot;species&quot;]==s].mean(), s = s, fontdict={&quot;fontsize&quot;:14, &quot;fontweight&quot;:&quot;bold&quot;,&quot;color&quot;:&quot;k&quot;} ) ax.set_xlabel(&quot;Bill Length (mm)&quot;) ax.set_ylabel(&quot;Bill Depth (mm)&quot;) fig.tight_layout() .",
            "url": "https://yeaun12.github.io/blog/jupyter/python/2022/05/13/3assignment.html",
            "relUrl": "/jupyter/python/2022/05/13/3assignment.html",
            "date": " • May 13, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Python Basics",
            "content": "&#45936;&#51060;&#53552; &#47560;&#51060;&#45789; 2022 1&#54617;&#44592; Python_Basics . a=5; b=6; c=7 . c . 7 . a= [1,2,3] b=a . append 함수를 적용했을 때 위에서 b와 a가 같다고 하였기 때문에 a에만 리스트를 추가해주어도 b의 값까지 바뀌는 것을 볼 수 있다. . a.append(4) b . [1, 2, 3, 4] . a . [1, 2, 3, 4] . 숫자가 들어있는 a의 타입은 int . a=5 type(a) . int . 문자가 들어있는 a의 타입은 str이다. . a=&#39;foo&#39; type(a) . str . 숫자와 문자를 더하면 값이 나오지 않는다. . &#39;5&#39;+5 . TypeError Traceback (most recent call last) c: Users 82108 Desktop data mining 2022-03-16-18-basics.ipynb Cell 13&#39; in &lt;cell line: 1&gt;() -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/82108/Desktop/data%20mining/2022-03-16-18-basics.ipynb#ch0000015?line=0&#39;&gt;1&lt;/a&gt; &#39;5&#39;+5 TypeError: can only concatenate str (not &#34;int&#34;) to str . a=4.5 b=2 . print 안에 {}딕셔너리를 사용하여 a is type(a), b is type(b)를 출력 . print(&#39;a is {0}, b is {1}&#39;.format(type(a), type(b))) a/b . a is &lt;class &#39;float&#39;&gt;, b is &lt;class &#39;int&#39;&gt; . 2.25 . isinstance 는 a가 int가 맞는지 출력 . a=5 isinstance(a, int) . True . 두가지 형태로하여 int나 float 중 a의 타입이 있는지 묻는 함수, 이 함수를 통해 변수의 타입을 알 수 있다. . a=5 b=4.5 isinstance(a,(int, float)) isinstance(b,(int, float)) . True . a=&#39;foo&#39; . upper()로 a에 있는 영문을 대문자로 만든다. . a.upper() . &#39;FOO&#39; . getattr(a,&#39;split&#39;) . &lt;function str.split(sep=None, maxsplit=-1)&gt; . def isiterable(obj): try: iter(obj) return True except TypeError: return False . isiterable(&#39;a string&#39;) . True . isiterable([1,2,3]) . True . isiterable(5) . False . some_module 은 두 줄 사이에 걸리는 시간 계산해준다. . some_module . Binary . 5-7 12+21.5 5&lt;=2 . False . a=[1,2,3] b=a c=list(a) a is b a is not c . True . a == c . True . a= None a is None . True . Mutable and immutable object . a_list=[&#39;foo&#39;, 2,[4,5]] a_list[2] =(3,4) a_list . [&#39;foo&#39;, 2, (3, 4)] . a_tuple=(3,5,(4,5)) a_tuple[1]=&#39;four&#39; . TypeError Traceback (most recent call last) c: Users 82108 Desktop data mining 2022-03-16-18-basics.ipynb Cell 39&#39; in &lt;cell line: 2&gt;() &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/82108/Desktop/data%20mining/2022-03-16-18-basics.ipynb#ch0000062?line=0&#39;&gt;1&lt;/a&gt; a_tuple=(3,5,(4,5)) -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/82108/Desktop/data%20mining/2022-03-16-18-basics.ipynb#ch0000062?line=1&#39;&gt;2&lt;/a&gt; a_tuple[1]=&#39;four&#39; TypeError: &#39;tuple&#39; object does not support item assignment . Scalar Types . ival=17239871 ival ** 6 . 26254519291092456596965462913230729701102721 . fval=7.243 fval2=6.78e-5 . 3/2 . 1.5 . type(3/2) . float . 3//2 . 1 . Strings . a=&#39;one way of writing a string&#39; b=&quot;another way&quot; . c=&quot;&quot;&quot; This is a longer string that spans multiple lines &quot;&quot;&quot; . c . &#39; nThis is a longer string that nspans multiple lines n&#39; . 괄호안에 있는 n이 얼마나 쓰였는지 알려준다. . c.count(&#39; n&#39;) . 3 . a=&#39;this is a string&#39; a[10]=&#39;f&#39; . TypeError Traceback (most recent call last) c: Users 82108 Desktop data mining 2022-03-16-18-basics.ipynb Cell 52&#39; in &lt;cell line: 2&gt;() &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/82108/Desktop/data%20mining/2022-03-16-18-basics.ipynb#ch0000070?line=0&#39;&gt;1&lt;/a&gt; a=&#39;this is a string&#39; -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/82108/Desktop/data%20mining/2022-03-16-18-basics.ipynb#ch0000070?line=1&#39;&gt;2&lt;/a&gt; a[10]=&#39;f&#39; TypeError: &#39;str&#39; object does not support item assignment . b=a.replace(&#39;string&#39;,&#39;longer string&#39;) b . &#39;this is a longer string&#39; . a . &#39;this is a string&#39; . a=5.6 s=str(a) print(s) . 5.6 . s=&#39;python&#39; list(s) . [&#39;p&#39;, &#39;y&#39;, &#39;t&#39;, &#39;h&#39;, &#39;o&#39;, &#39;n&#39;] . s[:3] . &#39;pyt&#39; . print(&#39;12 n34&#39;) . 12 34 . s=&#39;12 34&#39; print(s) . 12 34 . a=&#39;this is the first half &#39; b=&#39;and this is the second half&#39; a+b . &#39;this is the first half and this is the second half&#39; . template=&#39;{0:.2f} {1:s} are worth US${2:d}&#39; template . &#39;{0:.2f} {1:s} are worth US${2:d}&#39; . template.format(4.5560, &#39;Argentine Pesos&#39;, 1) . &#39;4.56 Argentine Pesos are worth US$1&#39; . template.format(1265.23, &#39;won&#39;,1) . &#39;1265.23 won are worth US$1&#39; . Booleans . True and True . True . False or False . False . s=&#39;3.14159&#39; fval=float(s) type(fval) . float . int(fval) . 3 . bool(0) . False . Nome . a=None a is None . True . def add_and_maybe_multiply(a,b,c=None): result = a+b if c is not None: result=result *c return result . add_and_maybe_multiply(5,3) . 8 . add_and_maybe_multiply(5,3,10) . 80 . type(None) . NoneType . Dates and times . from datetime import datetime, date, time . dt=datetime(2011, 10,29, 20,30,21) dt . datetime.datetime(2011, 10, 29, 20, 30, 21) . dt.day . 29 . dt.minute . 30 . dt.date() . datetime.date(2011, 10, 29) . dt.time() . datetime.time(20, 30, 21) . dt.strftime(&#39;%m/%d/%Y %H:%M&#39;) . &#39;10/29/2011 20:30&#39; . dt.strftime(&#39;%Y/%m/%d %H:%M&#39;) . &#39;2011/10/29 20:30&#39; . datetime.strptime(&#39;20091031&#39;,&#39;%Y%m%d&#39;) . datetime.datetime(2009, 10, 31, 0, 0) . dt.replace(minute=0, second=0) . datetime.datetime(2011, 10, 29, 20, 0) . dt2=datetime(2011,11,15,22,30) delta=dt2-dt delta . datetime.timedelta(days=17, seconds=7179) . type(delta) . datetime.timedelta . dt dt + delta . datetime.datetime(2011, 11, 15, 22, 30) . if, elif, and else . x=-5 if x &lt;0 : print(&#39;It is negative&#39;) . It is negative . x=-5 if x&lt;0: print(&#39;It is negative&#39;) elif x==0: print(&#39;Equal to zero&#39;) elif 0&lt;x&lt;5: print(&#39;Positive but smaller than 5&#39;) else: print(&#39;Positve and larger than or equal to 5&#39;) . It is negative . for loops . sequence=[1,2,None, 4, None, 5] total=0 for value in sequence: total +=value . TypeError Traceback (most recent call last) c: Users 82108 Desktop data mining 2022-03-16-18-basics.ipynb Cell 96&#39; in &lt;cell line: 3&gt;() &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/82108/Desktop/data%20mining/2022-03-16-18-basics.ipynb#ch0000124?line=1&#39;&gt;2&lt;/a&gt; total=0 &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/82108/Desktop/data%20mining/2022-03-16-18-basics.ipynb#ch0000124?line=2&#39;&gt;3&lt;/a&gt; for value in sequence: -&gt; &lt;a href=&#39;vscode-notebook-cell:/c%3A/Users/82108/Desktop/data%20mining/2022-03-16-18-basics.ipynb#ch0000124?line=3&#39;&gt;4&lt;/a&gt; total +=value TypeError: unsupported operand type(s) for +=: &#39;int&#39; and &#39;NoneType&#39; . sequence=[1,2,None,4, None, 5] total=0 for value in sequence: if value is None: continue total += value . total . 12 . sequence=[1,2,0,4,6,5,2,1] total_until_5 = 0 for value in sequence: if value ==5: break total_until_5 += value . total_until_5 . 13 . for i in range(4): for j in range(4): if j&gt;i: break print((i, j)) . (0, 0) (1, 0) (1, 1) (2, 0) (2, 1) (2, 2) (3, 0) (3, 1) (3, 2) (3, 3) . for a,b,c in [[1,2,3],[4,5,6],[7,8,9]]: print(a,b,c) . 1 2 3 4 5 6 7 8 9 . x=256 total=0 while x&gt;0: if total &gt;500: break total += x x=x//2 . total . 504 . x . 4 . 256+128+64+32+16+8 . 504 . range . range(10) . range(0, 10) . list(range(10)) . [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] . list(range(0,20,2)) . [0, 2, 4, 6, 8, 10, 12, 14, 16, 18] . list(range(5,0,-1)) . [5, 4, 3, 2, 1] . seq=[1,2,3,4] for i in range(len(seq)): val=seq[i] . val . 4 . sum = 0 for i in range(100000): if i % 3 ==0 or i % 5==0: sum+=i . x=5 &#39;Non-negative&#39; if x &gt;= 0 else &#39;Negative&#39; . &#39;Non-negative&#39; . x=5 a=100 if x&gt;=0 else -100 a . 100 .",
            "url": "https://yeaun12.github.io/blog/jupyter/python/2022/03/16/18-2-basics.html",
            "relUrl": "/jupyter/python/2022/03/16/18-2-basics.html",
            "date": " • Mar 16, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Numpy 기본",
            "content": "도구 - 넘파이(NumPy) . *넘파이(NumPy)는 파이썬의 과학 컴퓨팅을 위한 기본 라이브러리입니다. 넘파이의 핵심은 강력한 N-차원 배열 객체입니다. 또한 선형 대수, 푸리에(Fourier) 변환, 유사 난수 생성과 같은 유용한 함수들도 제공합니다.&quot; . 구글 코랩에서 실행하기 | &#48176;&#50676; &#49373;&#49457; . numpy를 임포트해 보죠. 대부분의 사람들이 np로 알리아싱하여 임포트합니다: . import numpy as np . np.zeros . zeros 함수는 0으로 채워진 배열을 만듭니다: . np.zeros(5) . array([0., 0., 0., 0., 0.]) . 2D 배열(즉, 행렬)을 만들려면 원하는 행과 열의 크기를 튜플로 전달합니다. 예를 들어 다음은 $3 times 4$ 크기의 행렬입니다: . np.zeros((3,4)) . array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) . &#50857;&#50612; . 넘파이에서 각 차원을 축(axis) 이라고 합니다 | 축의 개수를 랭크(rank) 라고 합니다. 예를 들어, 위의 $3 times 4$ 행렬은 랭크 2인 배열입니다(즉 2차원입니다). | 첫 번째 축의 길이는 3이고 두 번째 축의 길이는 4입니다. | . | 배열의 축 길이를 배열의 크기(shape)라고 합니다. 예를 들어, 위 행렬의 크기는 (3, 4)입니다. | 랭크는 크기의 길이와 같습니다. | . | 배열의 사이즈(size)는 전체 원소의 개수입니다. 축의 길이를 모두 곱해서 구할 수 있습니다(가령, $3 times 4=12$). | . a = np.zeros((3,4)) a . array([[0., 0., 0., 0.], [0., 0., 0., 0.], [0., 0., 0., 0.]]) . a.shape . (3, 4) . a.ndim # len(a.shape)와 같습니다 . 2 . a.size . 12 . N-&#52264;&#50896; &#48176;&#50676; . 임의의 랭크 수를 가진 N-차원 배열을 만들 수 있습니다. 예를 들어, 다음은 크기가 (2,3,4)인 3D 배열(랭크=3)입니다: . np.zeros((2,2,5)) . array([[[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]], [[0., 0., 0., 0., 0.], [0., 0., 0., 0., 0.]]]) . &#48176;&#50676; &#53440;&#51077; . 넘파이 배열의 타입은 ndarray입니다: . type(np.zeros((3,4))) . numpy.ndarray . np.ones . ndarray를 만들 수 있는 넘파이 함수가 많습니다. . 다음은 1로 채워진 $3 times 4$ 크기의 행렬입니다: . np.ones((3,4)) . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) . np.full . 주어진 값으로 지정된 크기의 배열을 초기화합니다. 다음은 π로 채워진 $3 times 4$ 크기의 행렬입니다. . np.full((3,4), np.pi) . array([[3.14159265, 3.14159265, 3.14159265, 3.14159265], [3.14159265, 3.14159265, 3.14159265, 3.14159265], [3.14159265, 3.14159265, 3.14159265, 3.14159265]]) . np.empty . 초기화되지 않은 $2 times 3$ 크기의 배열을 만듭니다(배열의 내용은 예측이 불가능하며 메모리 상황에 따라 달라집니다): . np.empty((2,3)) . array([[9.6677106e-317, 0.0000000e+000, 0.0000000e+000], [0.0000000e+000, 0.0000000e+000, 0.0000000e+000]]) . np.array . array 함수는 파이썬 리스트를 사용하여 ndarray를 초기화합니다: . np.array([[1,2,3,4], [10, 20, 30, 40]]) . array([[ 1, 2, 3, 4], [10, 20, 30, 40]]) . np.arange . 파이썬의 기본 range 함수와 비슷한 넘파이 arange 함수를 사용하여 ndarray를 만들 수 있습니다: . np.arange(1, 5) . array([1, 2, 3, 4]) . 부동 소수도 가능합니다: . np.arange(1.0, 5.0) . array([1., 2., 3., 4.]) . 파이썬의 기본 range 함수처럼 건너 뛰는 정도를 지정할 수 있습니다: . np.arange(1, 5, 0.5) . array([1. , 1.5, 2. , 2.5, 3. , 3.5, 4. , 4.5]) . 부동 소수를 사용하면 원소의 개수가 일정하지 않을 수 있습니다. 예를 들면 다음과 같습니다: . print(np.arange(0, 5/3, 1/3)) # 부동 소수 오차 때문에, 최댓값은 4/3 또는 5/3이 됩니다. print(np.arange(0, 5/3, 0.333333333)) print(np.arange(0, 5/3, 0.333333334)) . [0. 0.33333333 0.66666667 1. 1.33333333 1.66666667] [0. 0.33333333 0.66666667 1. 1.33333333 1.66666667] [0. 0.33333333 0.66666667 1. 1.33333334] . np.linspace . 이런 이유로 부동 소수를 사용할 땐 arange 대신에 linspace 함수를 사용하는 것이 좋습니다. linspace 함수는 지정된 개수만큼 두 값 사이를 나눈 배열을 반환합니다(arange와는 다르게 최댓값이 포함됩니다): . print(np.linspace(0, 5/3, 6)) . [0. 0.33333333 0.66666667 1. 1.33333333 1.66666667] . np.rand&#50752; np.randn . 넘파이의 random 모듈에는 ndarray를 랜덤한 값으로 초기화할 수 있는 함수들이 많이 있습니다. 예를 들어, 다음은 (균등 분포인) 0과 1사이의 랜덤한 부동 소수로 $3 times 4$ 행렬을 초기화합니다: . np.random.rand(3,4) . array([[0.37892456, 0.17966937, 0.38206837, 0.34922123], [0.80462136, 0.9845914 , 0.9416127 , 0.28305275], [0.21201033, 0.54891417, 0.03781613, 0.4369229 ]]) . 다음은 평균이 0이고 분산이 1인 일변량 정규 분포(가우시안 분포)에서 샘플링한 랜덤한 부동 소수를 담은 $3 times 4$ 행렬입니다: . np.random.randn(3,4) . array([[ 0.83811287, -0.57131751, -0.4381827 , 1.1485899 ], [ 1.45316084, -0.47259181, -1.23426057, -0.0669813 ], [ 1.01003549, 1.04381736, -0.93060038, 2.39043293]]) . 이 분포의 모양을 알려면 맷플롯립을 사용해 그려보는 것이 좋습니다(더 자세한 것은 맷플롯립 튜토리얼을 참고하세요): . %matplotlib inline import matplotlib.pyplot as plt . plt.hist(np.random.rand(100000), density=True, bins=100, histtype=&quot;step&quot;, color=&quot;blue&quot;, label=&quot;rand&quot;) plt.hist(np.random.randn(100000), density=True, bins=100, histtype=&quot;step&quot;, color=&quot;red&quot;, label=&quot;randn&quot;) plt.axis([-2.5, 2.5, 0, 1.1]) plt.legend(loc = &quot;upper left&quot;) plt.title(&quot;Random distributions&quot;) plt.xlabel(&quot;Value&quot;) plt.ylabel(&quot;Density&quot;) plt.show() . np.fromfunction . 함수를 사용하여 ndarray를 초기화할 수도 있습니다: . def my_function(z, y, x): return x + 10 * y + 100 * z np.fromfunction(my_function, (3, 2, 10)) . array([[[ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], [ 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]], [[100., 101., 102., 103., 104., 105., 106., 107., 108., 109.], [110., 111., 112., 113., 114., 115., 116., 117., 118., 119.]], [[200., 201., 202., 203., 204., 205., 206., 207., 208., 209.], [210., 211., 212., 213., 214., 215., 216., 217., 218., 219.]]]) . 넘파이는 먼저 크기가 (3, 2, 10)인 세 개의 ndarray(차원마다 하나씩)를 만듭니다. 각 배열은 축을 따라 좌표 값과 같은 값을 가집니다. 예를 들어, z 축에 있는 배열의 모든 원소는 z-축의 값과 같습니다: . [[[ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] [[ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.] [ 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]] [[ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.] [ 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.]]] . 위의 식 x + 10 * y + 100 * z에서 x, y, z는 사실 ndarray입니다(배열의 산술 연산에 대해서는 아래에서 설명합니다). 중요한 점은 함수 my_function이 원소마다 호출되는 것이 아니고 딱 한 번 호출된다는 점입니다. 그래서 매우 효율적으로 초기화할 수 있습니다. . &#48176;&#50676; &#45936;&#51060;&#53552; . dtype . 넘파이의 ndarray는 모든 원소가 동일한 타입(보통 숫자)을 가지기 때문에 효율적입니다. dtype 속성으로 쉽게 데이터 타입을 확인할 수 있습니다: . c = np.arange(1, 5) print(c.dtype, c) . int64 [1 2 3 4] . c = np.arange(1.0, 5.0) print(c.dtype, c) . float64 [1. 2. 3. 4.] . 넘파이가 데이터 타입을 결정하도록 내버려 두는 대신 dtype 매개변수를 사용해서 배열을 만들 때 명시적으로 지정할 수 있습니다: . d = np.arange(1, 5, dtype=np.complex64) print(d.dtype, d) . complex64 [1.+0.j 2.+0.j 3.+0.j 4.+0.j] . 가능한 데이터 타입은 int8, int16, int32, int64, uint8|16|32|64, float16|32|64, complex64|128가 있습니다. 전체 리스트는 온라인 문서를 참고하세요. . itemsize . itemsize 속성은 각 아이템의 크기(바이트)를 반환합니다: . e = np.arange(1, 5, dtype=np.complex64) e.itemsize . 8 . data &#48260;&#54140; . 배열의 데이터는 1차원 바이트 버퍼로 메모리에 저장됩니다. data 속성을 사용해 참조할 수 있습니다(사용할 일은 거의 없겠지만요). . f = np.array([[1,2],[1000, 2000]], dtype=np.int32) f.data . &lt;memory at 0x7f97929dd790&gt; . 파이썬 2에서는 f.data가 버퍼이고 파이썬 3에서는 memoryview입니다. . if (hasattr(f.data, &quot;tobytes&quot;)): data_bytes = f.data.tobytes() # python 3 else: data_bytes = memoryview(f.data).tobytes() # python 2 data_bytes . b&#39; x01 x00 x00 x00 x02 x00 x00 x00 xe8 x03 x00 x00 xd0 x07 x00 x00&#39; . 여러 개의 ndarray가 데이터 버퍼를 공유할 수 있습니다. 하나를 수정하면 다른 것도 바뀝니다. 잠시 후에 예를 살펴 보겠습니다. . &#48176;&#50676; &#53356;&#44592; &#48320;&#44221; . &#51088;&#49888;&#51012; &#48320;&#44221; . ndarray의 shape 속성을 지정하면 간단히 크기를 바꿀 수 있습니다. 배열의 원소 개수는 동일하게 유지됩니다. . g = np.arange(24) print(g) print(&quot;랭크:&quot;, g.ndim) . [ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23] 랭크: 1 . g.shape = (6, 4) print(g) print(&quot;랭크:&quot;, g.ndim) . [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15] [16 17 18 19] [20 21 22 23]] 랭크: 2 . g.shape = (2, 3, 4) print(g) print(&quot;랭크:&quot;, g.ndim) . [[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[12 13 14 15] [16 17 18 19] [20 21 22 23]]] 랭크: 3 . reshape . reshape 함수는 동일한 데이터를 가리키는 새로운 ndarray 객체를 반환합니다. 한 배열을 수정하면 다른 것도 함께 바뀝니다. . g2 = g.reshape(4,6) print(g2) print(&quot;랭크:&quot;, g2.ndim) . [[ 0 1 2 3 4 5] [ 6 7 8 9 10 11] [12 13 14 15 16 17] [18 19 20 21 22 23]] 랭크: 2 . 행 1, 열 2의 원소를 999로 설정합니다(인덱싱 방식은 아래를 참고하세요). . g2[1, 2] = 999 g2 . array([[ 0, 1, 2, 3, 4, 5], [ 6, 7, 999, 9, 10, 11], [ 12, 13, 14, 15, 16, 17], [ 18, 19, 20, 21, 22, 23]]) . 이에 상응하는 g의 원소도 수정됩니다. . g . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [999, 9, 10, 11]], [[ 12, 13, 14, 15], [ 16, 17, 18, 19], [ 20, 21, 22, 23]]]) . ravel . 마지막으로 ravel 함수는 동일한 데이터를 가리키는 새로운 1차원 ndarray를 반환합니다: . g.ravel() . array([ 0, 1, 2, 3, 4, 5, 6, 7, 999, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . &#49328;&#49696; &#50672;&#49328; . 일반적인 산술 연산자(+, -, *, /, //, ** 등)는 모두 ndarray와 사용할 수 있습니다. 이 연산자는 원소별로 적용됩니다: . a = np.array([14, 23, 32, 41]) b = np.array([5, 4, 3, 2]) print(&quot;a + b =&quot;, a + b) print(&quot;a - b =&quot;, a - b) print(&quot;a * b =&quot;, a * b) print(&quot;a / b =&quot;, a / b) print(&quot;a // b =&quot;, a // b) print(&quot;a % b =&quot;, a % b) print(&quot;a ** b =&quot;, a ** b) . a + b = [19 27 35 43] a - b = [ 9 19 29 39] a * b = [70 92 96 82] a / b = [ 2.8 5.75 10.66666667 20.5 ] a // b = [ 2 5 10 20] a % b = [4 3 2 1] a ** b = [537824 279841 32768 1681] . 여기 곱셈은 행렬 곱셈이 아닙니다. 행렬 연산은 아래에서 설명합니다. . 배열의 크기는 같아야 합니다. 그렇지 않으면 넘파이가 브로드캐스팅 규칙을 적용합니다. . &#48652;&#47196;&#46300;&#52880;&#49828;&#54021; . 일반적으로 넘파이는 동일한 크기의 배열을 기대합니다. 그렇지 않은 상황에는 브로드캐시틍 규칙을 적용합니다: . &#44508;&#52825; 1 . 배열의 랭크가 동일하지 않으면 랭크가 맞을 때까지 랭크가 작은 배열 앞에 1을 추가합니다. . h = np.arange(5).reshape(1, 1, 5) h . array([[[0, 1, 2, 3, 4]]]) . 여기에 (1,1,5) 크기의 3D 배열에 (5,) 크기의 1D 배열을 더해 보죠. 브로드캐스팅의 규칙 1이 적용됩니다! . h + [10, 20, 30, 40, 50] # 다음과 동일합니다: h + [[[10, 20, 30, 40, 50]]] . array([[[10, 21, 32, 43, 54]]]) . &#44508;&#52825; 2 . 특정 차원이 1인 배열은 그 차원에서 크기가 가장 큰 배열의 크기에 맞춰 동작합니다. 배열의 원소가 차원을 따라 반복됩니다. . k = np.arange(6).reshape(2, 3) k . array([[0, 1, 2], [3, 4, 5]]) . (2,3) 크기의 2D ndarray에 (2,1) 크기의 2D 배열을 더해 보죠. 넘파이는 브로드캐스팅 규칙 2를 적용합니다: . k + [[100], [200]] # 다음과 같습니다: k + [[100, 100, 100], [200, 200, 200]] . array([[100, 101, 102], [203, 204, 205]]) . 규칙 1과 2를 합치면 다음과 같이 동작합니다: . k + [100, 200, 300] # 규칙 1 적용: [[100, 200, 300]], 규칙 2 적용: [[100, 200, 300], [100, 200, 300]] . array([[100, 201, 302], [103, 204, 305]]) . 또 매우 간단히 다음 처럼 해도 됩니다: . k + 1000 # 다음과 같습니다: k + [[1000, 1000, 1000], [1000, 1000, 1000]] . array([[1000, 1001, 1002], [1003, 1004, 1005]]) . &#44508;&#52825; 3 . 규칙 1 &amp; 2을 적용했을 때 모든 배열의 크기가 맞아야 합니다. . try: k + [33, 44] except ValueError as e: print(e) . operands could not be broadcast together with shapes (2,3) (2,) . 브로드캐스팅 규칙은 산술 연산 뿐만 아니라 넘파이 연산에서 많이 사용됩니다. 아래에서 더 보도록 하죠. 브로드캐스팅에 관한 더 자세한 정보는 온라인 문서를 참고하세요. . &#50629;&#52880;&#49828;&#54021; . dtype이 다른 배열을 합칠 때 넘파이는 (실제 값에 상관없이) 모든 값을 다룰 수 있는 타입으로 업캐스팅합니다. . k1 = np.arange(0, 5, dtype=np.uint8) print(k1.dtype, k1) . uint8 [0 1 2 3 4] . k2 = k1 + np.array([5, 6, 7, 8, 9], dtype=np.int8) print(k2.dtype, k2) . int16 [ 5 7 9 11 13] . 모든 int8과 uint8 값(-128에서 255까지)을 표현하기 위해 int16이 필요합니다. 이 코드에서는 uint8이면 충분하지만 업캐스팅되었습니다. . k3 = k1 + 1.5 print(k3.dtype, k3) . float64 [1.5 2.5 3.5 4.5 5.5] . &#51312;&#44148; &#50672;&#49328;&#51088; . 조건 연산자도 원소별로 적용됩니다: . m = np.array([20, -5, 30, 40]) m &lt; [15, 16, 35, 36] . array([False, True, True, False]) . 브로드캐스팅을 사용합니다: . m &lt; 25 # m &lt; [25, 25, 25, 25] 와 동일 . array([ True, True, False, False]) . 불리언 인덱싱과 함께 사용하면 아주 유용합니다(아래에서 설명하겠습니다). . m[m &lt; 25] . array([20, -5]) . &#49688;&#54617; &#54632;&#49688;&#50752; &#53685;&#44228; &#54632;&#49688; . ndarray에서 사용할 수 있는 수학 함수와 통계 함수가 많습니다. . ndarray &#47700;&#49436;&#46300; . 일부 함수는 ndarray 메서드로 제공됩니다. 예를 들면: . a = np.array([[-2.5, 3.1, 7], [10, 11, 12]]) print(a) print(&quot;평균 =&quot;, a.mean()) . [[-2.5 3.1 7. ] [10. 11. 12. ]] 평균 = 6.766666666666667 . 이 명령은 크기에 상관없이 ndarray에 있는 모든 원소의 평균을 계산합니다. . 다음은 유용한 ndarray 메서드입니다: . for func in (a.min, a.max, a.sum, a.prod, a.std, a.var): print(func.__name__, &quot;=&quot;, func()) . min = -2.5 max = 12.0 sum = 40.6 prod = -71610.0 std = 5.084835843520964 var = 25.855555555555554 . 이 함수들은 선택적으로 매개변수 axis를 사용합니다. 지정된 축을 따라 원소에 연산을 적용하는데 사용합니다. 예를 들면: . c=np.arange(24).reshape(2,3,4) c . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) . c.sum(axis=0) # 첫 번째 축을 따라 더함, 결과는 3x4 배열 . array([[12, 14, 16, 18], [20, 22, 24, 26], [28, 30, 32, 34]]) . c.sum(axis=1) # 두 번째 축을 따라 더함, 결과는 2x4 배열 . array([[12, 15, 18, 21], [48, 51, 54, 57]]) . 여러 축에 대해서 더할 수도 있습니다: . c.sum(axis=(0,2)) # 첫 번째 축과 세 번째 축을 따라 더함, 결과는 (3,) 배열 . array([ 60, 92, 124]) . 0+1+2+3 + 12+13+14+15, 4+5+6+7 + 16+17+18+19, 8+9+10+11 + 20+21+22+23 . (60, 92, 124) . &#51068;&#48152; &#54632;&#49688; . 넘파이는 일반 함수(universal function) 또는 ufunc라고 부르는 원소별 함수를 제공합니다. 예를 들면 square 함수는 원본 ndarray를 복사하여 각 원소를 제곱한 새로운 ndarray 객체를 반환합니다: . a = np.array([[-2.5, 3.1, 7], [10, 11, 12]]) np.square(a) . array([[ 6.25, 9.61, 49. ], [100. , 121. , 144. ]]) . 다음은 유용한 단항 일반 함수들입니다: . print(&quot;원본 ndarray&quot;) print(a) for func in (np.abs, np.sqrt, np.exp, np.log, np.sign, np.ceil, np.modf, np.isnan, np.cos): print(&quot; n&quot;, func.__name__) print(func(a)) . 원본 ndarray [[-2.5 3.1 7. ] [10. 11. 12. ]] absolute [[ 2.5 3.1 7. ] [10. 11. 12. ]] sqrt [[ nan 1.76068169 2.64575131] [3.16227766 3.31662479 3.46410162]] exp [[8.20849986e-02 2.21979513e+01 1.09663316e+03] [2.20264658e+04 5.98741417e+04 1.62754791e+05]] log [[ nan 1.13140211 1.94591015] [2.30258509 2.39789527 2.48490665]] sign [[-1. 1. 1.] [ 1. 1. 1.]] ceil [[-2. 4. 7.] [10. 11. 12.]] modf (array([[-0.5, 0.1, 0. ], [ 0. , 0. , 0. ]]), array([[-2., 3., 7.], [10., 11., 12.]])) isnan [[False False False] [False False False]] cos [[-0.80114362 -0.99913515 0.75390225] [-0.83907153 0.0044257 0.84385396]] . &lt;ipython-input-59-d791c8e37e6f&gt;:5: RuntimeWarning: invalid value encountered in sqrt print(func(a)) &lt;ipython-input-59-d791c8e37e6f&gt;:5: RuntimeWarning: invalid value encountered in log print(func(a)) . &#51060;&#54637; &#51068;&#48152; &#54632;&#49688; . 두 개의 ndarray에 원소별로 적용되는 이항 함수도 많습니다. 두 배열이 동일한 크기가 아니면 브로드캐스팅 규칙이 적용됩니다: . a = np.array([1, -2, 3, 4]) b = np.array([2, 8, -1, 7]) np.add(a, b) # a + b 와 동일 . array([ 3, 6, 2, 11]) . np.greater(a, b) # a &gt; b 와 동일 . array([False, False, True, False]) . np.maximum(a, b) . array([2, 8, 3, 7]) . np.copysign(a, b) . array([ 1., 2., -3., 4.]) . &#48176;&#50676; &#51064;&#45937;&#49905; . 1&#52264;&#50896; &#48176;&#50676; . 1차원 넘파이 배열은 보통의 파이썬 배열과 비슷하게 사용할 수 있습니다: . a = np.array([1, 5, 3, 19, 13, 7, 3]) a[3] . 19 . a[2:5] . array([ 3, 19, 13]) . a[2:-1] . array([ 3, 19, 13, 7]) . a[:2] . array([1, 5]) . a[2::2] . array([ 3, 13, 3]) . a[::-1] . array([ 3, 7, 13, 19, 3, 5, 1]) . 물론 원소를 수정할 수 있죠: . a[3]=999 a . array([ 1, 5, 3, 999, 13, 7, 3]) . 슬라이싱을 사용해 ndarray를 수정할 수 있습니다: . a[2:5] = [997, 998, 999] a . array([ 1, 5, 997, 998, 999, 7, 3]) . &#48372;&#53685;&#51032; &#54028;&#51060;&#50028; &#48176;&#50676;&#44284; &#52264;&#51060;&#51216; . 보통의 파이썬 배열과 대조적으로 ndarray 슬라이싱에 하나의 값을 할당하면 슬라이싱 전체에 복사됩니다. 위에서 언급한 브로드캐스팅 덕택입니다. . a[2:5] = -1 a . array([ 1, 5, -1, -1, -1, 7, 3]) . 또한 이런 식으로 ndarray 크기를 늘리거나 줄일 수 없습니다: . try: a[2:5] = [1,2,3,4,5,6] # 너무 길어요 except ValueError as e: print(e) . cannot copy sequence with size 6 to array axis with dimension 3 . 원소를 삭제할 수도 없습니다: . try: del a[2:5] except ValueError as e: print(e) . cannot delete array elements . 중요한 점은 ndarray의 슬라이싱은 같은 데이터 버퍼를 바라보는 뷰(view)입니다. 슬라이싱된 객체를 수정하면 실제 원본 ndarray가 수정됩니다! . a_slice = a[2:6] a_slice[1] = 1000 a # 원본 배열이 수정됩니다! . array([ 1, 5, -1, 1000, -1, 7, 3]) . a[3] = 2000 a_slice # 비슷하게 원본 배열을 수정하면 슬라이싱 객체에도 반영됩니다! . array([ -1, 2000, -1, 7]) . 데이터를 복사하려면 copy 메서드를 사용해야 합니다: . another_slice = a[2:6].copy() another_slice[1] = 3000 a # 원본 배열이 수정되지 않습니다 . array([ 1, 5, -1, 2000, -1, 7, 3]) . a[3] = 4000 another_slice # 마찬가지로 원본 배열을 수정해도 복사된 배열은 바뀌지 않습니다 . array([ -1, 3000, -1, 7]) . &#45796;&#52264;&#50896; &#48176;&#50676; . 다차원 배열은 비슷한 방식으로 각 축을 따라 인덱싱 또는 슬라이싱해서 사용합니다. 콤마로 구분합니다: . b = np.arange(48).reshape(4, 12) b . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]) . b[1, 2] # 행 1, 열 2 . 14 . b[1, :] # 행 1, 모든 열 . array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . b[:, 1] # 모든 행, 열 1 . array([ 1, 13, 25, 37]) . 주의: 다음 두 표현에는 미묘한 차이가 있습니다: . b[1, :] . array([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]) . b[1:2, :] . array([[12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]]) . 첫 번째 표현식은 (12,) 크기인 1D 배열로 행이 하나입니다. 두 번째는 (1, 12) 크기인 2D 배열로 같은 행을 반환합니다. . &#54060;&#49884; &#51064;&#45937;&#49905;(Fancy indexing) . 관심 대상의 인덱스 리스트를 지정할 수도 있습니다. 이를 팬시 인덱싱이라고 부릅니다. . b[(0,2), 2:5] # 행 0과 2, 열 2에서 4(5-1)까지 . array([[ 2, 3, 4], [26, 27, 28]]) . b[:, (-1, 2, -1)] # 모든 행, 열 -1 (마지막), 2와 -1 (다시 반대 방향으로) . array([[11, 2, 11], [23, 14, 23], [35, 26, 35], [47, 38, 47]]) . 여러 개의 인덱스 리스트를 지정하면 인덱스에 맞는 값이 포함된 1D ndarray를 반환됩니다. . b[(-1, 2, -1, 2), (5, 9, 1, 9)] # returns a 1D array with b[-1, 5], b[2, 9], b[-1, 1] and b[2, 9] (again) . array([41, 33, 37, 33]) . &#44256;&#52264;&#50896; . 고차원에서도 동일한 방식이 적용됩니다. 몇 가지 예를 살펴 보겠습니다: . c = b.reshape(4,2,6) c . array([[[ 0, 1, 2, 3, 4, 5], [ 6, 7, 8, 9, 10, 11]], [[12, 13, 14, 15, 16, 17], [18, 19, 20, 21, 22, 23]], [[24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35]], [[36, 37, 38, 39, 40, 41], [42, 43, 44, 45, 46, 47]]]) . c[2, 1, 4] # 행렬 2, 행 1, 열 4 . 34 . c[2, :, 3] # 행렬 2, 모든 행, 열 3 . array([27, 33]) . 어떤 축에 대한 인덱스를 지정하지 않으면 이 축의 모든 원소가 반환됩니다: . c[2, 1] # 행렬 2, 행 1, 모든 열이 반환됩니다. c[2, 1, :]와 동일합니다. . array([30, 31, 32, 33, 34, 35]) . &#49373;&#47029; &#48512;&#54840; (...) . 생략 부호(...)를 쓰면 모든 지정하지 않은 축의 원소를 포함합니다. . c[2, ...] # 행렬 2, 모든 행, 모든 열. c[2, :, :]와 동일 . array([[24, 25, 26, 27, 28, 29], [30, 31, 32, 33, 34, 35]]) . c[2, 1, ...] # 행렬 2, 행 1, 모든 열. c[2, 1, :]와 동일 . array([30, 31, 32, 33, 34, 35]) . c[2, ..., 3] # 행렬 2, 모든 행, 열 3. c[2, :, 3]와 동일 . array([27, 33]) . c[..., 3] # 모든 행렬, 모든 행, 열 3. c[:, :, 3]와 동일 . array([[ 3, 9], [15, 21], [27, 33], [39, 45]]) . &#48520;&#47532;&#50616; &#51064;&#45937;&#49905; . 불리언 값을 가진 ndarray를 사용해 축의 인덱스를 지정할 수 있습니다. . b = np.arange(48).reshape(4, 12) b . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35], [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]) . rows_on = np.array([True, False, True, False]) b[rows_on, :] # 행 0과 2, 모든 열. b[(0, 2), :]와 동일 . array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35]]) . cols_on = np.array([False, True, False] * 4) b[:, cols_on] # 모든 행, 열 1, 4, 7, 10 . array([[ 1, 4, 7, 10], [13, 16, 19, 22], [25, 28, 31, 34], [37, 40, 43, 46]]) . np.ix_ . 여러 축에 걸쳐서는 불리언 인덱싱을 사용할 수 없고 ix_ 함수를 사용합니다: . b[np.ix_(rows_on, cols_on)] . array([[ 1, 4, 7, 10], [25, 28, 31, 34]]) . np.ix_(rows_on, cols_on) . (array([[0], [2]]), array([[ 1, 4, 7, 10]])) . ndarray와 같은 크기의 불리언 배열을 사용하면 해당 위치가 True인 모든 원소를 담은 1D 배열이 반환됩니다. 일반적으로 조건 연산자와 함께 사용합니다: . b[b % 3 == 1] . array([ 1, 4, 7, 10, 13, 16, 19, 22, 25, 28, 31, 34, 37, 40, 43, 46]) . &#48152;&#48373; . ndarray를 반복하는 것은 일반적인 파이썬 배열을 반복한는 것과 매우 유사합니다. 다차원 배열을 반복하면 첫 번째 축에 대해서 수행됩니다. . c = np.arange(24).reshape(2, 3, 4) # 3D 배열 (두 개의 3x4 행렬로 구성됨) c . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]]) . for m in c: print(&quot;아이템:&quot;) print(m) . 아이템: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] 아이템: [[12 13 14 15] [16 17 18 19] [20 21 22 23]] . for i in range(len(c)): # len(c) == c.shape[0] print(&quot;아이템:&quot;) print(c[i]) . 아이템: [[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] 아이템: [[12 13 14 15] [16 17 18 19] [20 21 22 23]] . ndarray에 있는 모든 원소를 반복하려면 flat 속성을 사용합니다: . for i in c.flat: print(&quot;아이템:&quot;, i) . 아이템: 0 아이템: 1 아이템: 2 아이템: 3 아이템: 4 아이템: 5 아이템: 6 아이템: 7 아이템: 8 아이템: 9 아이템: 10 아이템: 11 아이템: 12 아이템: 13 아이템: 14 아이템: 15 아이템: 16 아이템: 17 아이템: 18 아이템: 19 아이템: 20 아이템: 21 아이템: 22 아이템: 23 . &#48176;&#50676; &#49939;&#44592; . 종종 다른 배열을 쌓아야 할 때가 있습니다. 넘파이는 이를 위해 몇 개의 함수를 제공합니다. 먼저 배열 몇 개를 만들어 보죠. . q1 = np.full((3,4), 1.0) q1 . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]]) . q2 = np.full((4,4), 2.0) q2 . array([[2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.]]) . q3 = np.full((3,4), 3.0) q3 . array([[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) . vstack . vstack 함수를 사용하여 수직으로 쌓아보죠: . q4 = np.vstack((q1, q2, q3)) q4 . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) . q4.shape . (10, 4) . q1, q2, q3가 모두 같은 크기이므로 가능합니다(수직으로 쌓기 때문에 수직 축은 크기가 달라도 됩니다). . hstack . hstack을 사용해 수평으로도 쌓을 수 있습니다: . q5 = np.hstack((q1, q3)) q5 . array([[1., 1., 1., 1., 3., 3., 3., 3.], [1., 1., 1., 1., 3., 3., 3., 3.], [1., 1., 1., 1., 3., 3., 3., 3.]]) . q5.shape . (3, 8) . q1과 q3가 모두 3개의 행을 가지고 있기 때문에 가능합니다. q2는 4개의 행을 가지고 있기 때문에 q1, q3와 수평으로 쌓을 수 없습니다: . try: q5 = np.hstack((q1, q2, q3)) except ValueError as e: print(e) . all the input array dimensions for the concatenation axis must match exactly, but along dimension 0, the array at index 0 has size 3 and the array at index 1 has size 4 . concatenate . concatenate 함수는 지정한 축으로도 배열을 쌓습니다. . q7 = np.concatenate((q1, q2, q3), axis=0) # vstack과 동일 q7 . array([[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [2., 2., 2., 2.], [3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]) . q7.shape . (10, 4) . 예상했겠지만 hstack은 axis=1으로 concatenate를 호출하는 것과 같습니다. . stack . stack 함수는 새로운 축을 따라 배열을 쌓습니다. 모든 배열은 같은 크기를 가져야 합니다. . q8 = np.stack((q1, q3)) q8 . array([[[1., 1., 1., 1.], [1., 1., 1., 1.], [1., 1., 1., 1.]], [[3., 3., 3., 3.], [3., 3., 3., 3.], [3., 3., 3., 3.]]]) . q8.shape . (2, 3, 4) . &#48176;&#50676; &#48516;&#54624; . 분할은 쌓기의 반대입니다. 예를 들어 vsplit 함수는 행렬을 수직으로 분할합니다. . 먼저 6x4 행렬을 만들어 보죠: . r = np.arange(24).reshape(6,4) r . array([[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11], [12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]) . 수직으로 동일한 크기로 나누어 보겠습니다: . r1, r2, r3 = np.vsplit(r, 3) r1 . array([[0, 1, 2, 3], [4, 5, 6, 7]]) . r2 . array([[ 8, 9, 10, 11], [12, 13, 14, 15]]) . r3 . array([[16, 17, 18, 19], [20, 21, 22, 23]]) . split 함수는 주어진 축을 따라 배열을 분할합니다. vsplit는 axis=0으로 split를 호출하는 것과 같습니다. hsplit 함수는 axis=1로 split를 호출하는 것과 같습니다: . r4, r5 = np.hsplit(r, 2) r4 . array([[ 0, 1], [ 4, 5], [ 8, 9], [12, 13], [16, 17], [20, 21]]) . r5 . array([[ 2, 3], [ 6, 7], [10, 11], [14, 15], [18, 19], [22, 23]]) . &#48176;&#50676; &#51204;&#52824; . transpose 메서드는 주어진 순서대로 축을 뒤바꾸어 ndarray 데이터에 대한 새로운 뷰를 만듭니다. . 예를 위해 3D 배열을 만들어 보죠: . t = np.arange(24).reshape(4,2,3) t . array([[[ 0, 1, 2], [ 3, 4, 5]], [[ 6, 7, 8], [ 9, 10, 11]], [[12, 13, 14], [15, 16, 17]], [[18, 19, 20], [21, 22, 23]]]) . 0, 1, 2(깊이, 높이, 너비) 축을 1, 2, 0 (깊이→너비, 높이→깊이, 너비→높이) 순서로 바꾼 ndarray를 만들어 보겠습니다: . t1 = t.transpose((1,2,0)) t1 . array([[[ 0, 6, 12, 18], [ 1, 7, 13, 19], [ 2, 8, 14, 20]], [[ 3, 9, 15, 21], [ 4, 10, 16, 22], [ 5, 11, 17, 23]]]) . t1.shape . (2, 3, 4) . transpose 기본값은 차원의 순서를 역전시킵니다: . t2 = t.transpose() # t.transpose((2, 1, 0))와 동일 t2 . array([[[ 0, 6, 12, 18], [ 3, 9, 15, 21]], [[ 1, 7, 13, 19], [ 4, 10, 16, 22]], [[ 2, 8, 14, 20], [ 5, 11, 17, 23]]]) . t2.shape . (3, 2, 4) . 넘파이는 두 축을 바꾸는 swapaxes 함수를 제공합니다. 예를 들어 깊이와 높이를 뒤바꾸어 t의 새로운 뷰를 만들어 보죠: . t3 = t.swapaxes(0,1) # t.transpose((1, 0, 2))와 동일 t3 . array([[[ 0, 1, 2], [ 6, 7, 8], [12, 13, 14], [18, 19, 20]], [[ 3, 4, 5], [ 9, 10, 11], [15, 16, 17], [21, 22, 23]]]) . t3.shape . (2, 4, 3) . &#49440;&#54805; &#45824;&#49688;&#54617; . 넘파이 2D 배열을 사용하면 파이썬에서 행렬을 효율적으로 표현할 수 있습니다. 주요 행렬 연산을 간단히 둘러 보겠습니다. 선형 대수학, 벡터와 행렬에 관한 자세한 내용은 Linear Algebra tutorial를 참고하세요. . &#54665;&#47148; &#51204;&#52824; . T 속성은 랭크가 2보다 크거나 같을 때 transpose()를 호출하는 것과 같습니다: . m1 = np.arange(10).reshape(2,5) m1 . array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) . m1.T . array([[0, 5], [1, 6], [2, 7], [3, 8], [4, 9]]) . T 속성은 랭크가 0이거나 1인 배열에는 아무런 영향을 미치지 않습니다: . m2 = np.arange(5) m2 . array([0, 1, 2, 3, 4]) . m2.T . array([0, 1, 2, 3, 4]) . 먼저 1D 배열을 하나의 행이 있는 행렬(2D)로 바꾼다음 전치를 수행할 수 있습니다: . m2r = m2.reshape(1,5) m2r . array([[0, 1, 2, 3, 4]]) . m2r.T . array([[0], [1], [2], [3], [4]]) . &#54665;&#47148; &#44273;&#49480; . 두 개의 행렬을 만들어 dot 메서드로 행렬 곱셈을 실행해 보죠. . n1 = np.arange(10).reshape(2, 5) n1 . array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]) . n2 = np.arange(15).reshape(5,3) n2 . array([[ 0, 1, 2], [ 3, 4, 5], [ 6, 7, 8], [ 9, 10, 11], [12, 13, 14]]) . n1.dot(n2) . array([[ 90, 100, 110], [240, 275, 310]]) . 주의: 앞서 언급한 것처럼 n1*n2는 행렬 곱셈이 아니라 원소별 곱셈(또는 아다마르 곱이라 부릅니다)입니다. . &#50669;&#54665;&#47148;&#44284; &#50976;&#49324; &#50669;&#54665;&#47148; . numpy.linalg 모듈 안에 많은 선형 대수 함수들이 있습니다. 특히 inv 함수는 정방 행렬의 역행렬을 계산합니다: . import numpy.linalg as linalg m3 = np.array([[1,2,3],[5,7,11],[21,29,31]]) m3 . array([[ 1, 2, 3], [ 5, 7, 11], [21, 29, 31]]) . linalg.inv(m3) . array([[-2.31818182, 0.56818182, 0.02272727], [ 1.72727273, -0.72727273, 0.09090909], [-0.04545455, 0.29545455, -0.06818182]]) . pinv 함수를 사용하여 유사 역행렬을 계산할 수도 있습니다: . linalg.pinv(m3) . array([[-2.31818182, 0.56818182, 0.02272727], [ 1.72727273, -0.72727273, 0.09090909], [-0.04545455, 0.29545455, -0.06818182]]) . &#45800;&#50948; &#54665;&#47148; . 행렬과 그 행렬의 역행렬을 곱하면 단위 행렬이 됩니다(작은 소숫점 오차가 있습니다): . m3.dot(linalg.inv(m3)) . array([[ 1.00000000e+00, -1.66533454e-16, 0.00000000e+00], [ 6.31439345e-16, 1.00000000e+00, -1.38777878e-16], [ 5.21110932e-15, -2.38697950e-15, 1.00000000e+00]]) . eye 함수는 NxN 크기의 단위 행렬을 만듭니다: . np.eye(3) . array([[1., 0., 0.], [0., 1., 0.], [0., 0., 1.]]) . QR &#48516;&#54644; . qr 함수는 행렬을 QR 분해합니다: . q, r = linalg.qr(m3) q . array([[-0.04627448, 0.98786672, 0.14824986], [-0.23137241, 0.13377362, -0.96362411], [-0.97176411, -0.07889213, 0.22237479]]) . r . array([[-21.61018278, -29.89331494, -32.80860727], [ 0. , 0.62427688, 1.9894538 ], [ 0. , 0. , -3.26149699]]) . q.dot(r) # q.r는 m3와 같습니다 . array([[ 1., 2., 3.], [ 5., 7., 11.], [21., 29., 31.]]) . &#54665;&#47148;&#49885; . det 함수는 행렬식을 계산합니다: . linalg.det(m3) # 행렬식 계산 . 43.99999999999997 . &#44256;&#50995;&#44050;&#44284; &#44256;&#50976;&#48289;&#53552; . eig 함수는 정방 행렬의 고윳값과 고유벡터를 계산합니다: . eigenvalues, eigenvectors = linalg.eig(m3) eigenvalues # λ . array([42.26600592, -0.35798416, -2.90802176]) . eigenvectors # v . array([[-0.08381182, -0.76283526, -0.18913107], [-0.3075286 , 0.64133975, -0.6853186 ], [-0.94784057, -0.08225377, 0.70325518]]) . m3.dot(eigenvectors) - eigenvalues * eigenvectors # m3.v - λ*v = 0 . array([[ 8.88178420e-15, 2.22044605e-16, -3.10862447e-15], [ 3.55271368e-15, 2.02615702e-15, -1.11022302e-15], [ 3.55271368e-14, 3.33413852e-15, -8.43769499e-15]]) . &#53945;&#51079;&#44050; &#48516;&#54644; . svd 함수는 행렬을 입력으로 받아 그 행렬의 특잇값 분해를 반환합니다: . m4 = np.array([[1,0,0,0,2], [0,0,3,0,0], [0,0,0,0,0], [0,2,0,0,0]]) m4 . array([[1, 0, 0, 0, 2], [0, 0, 3, 0, 0], [0, 0, 0, 0, 0], [0, 2, 0, 0, 0]]) . U, S_diag, V = linalg.svd(m4) U . array([[ 0., 1., 0., 0.], [ 1., 0., 0., 0.], [ 0., 0., 0., -1.], [ 0., 0., 1., 0.]]) . S_diag . array([3. , 2.23606798, 2. , 0. ]) . svd 함수는 Σ의 대각 원소 값만 반환합니다. 전체 Σ 행렬은 다음과 같이 만듭니다: . S = np.zeros((4, 5)) S[np.diag_indices(4)] = S_diag S # Σ . array([[3. , 0. , 0. , 0. , 0. ], [0. , 2.23606798, 0. , 0. , 0. ], [0. , 0. , 2. , 0. , 0. ], [0. , 0. , 0. , 0. , 0. ]]) . V . array([[-0. , 0. , 1. , -0. , 0. ], [ 0.4472136 , 0. , 0. , 0. , 0.89442719], [-0. , 1. , 0. , -0. , 0. ], [ 0. , 0. , 0. , 1. , 0. ], [-0.89442719, 0. , 0. , 0. , 0.4472136 ]]) . U.dot(S).dot(V) # U.Σ.V == m4 . array([[1., 0., 0., 0., 2.], [0., 0., 3., 0., 0.], [0., 0., 0., 0., 0.], [0., 2., 0., 0., 0.]]) . &#45824;&#44033;&#50896;&#49548;&#50752; &#45824;&#44033;&#54633; . np.diag(m3) # m3의 대각 원소입니다(왼쪽 위에서 오른쪽 아래) . array([ 1, 7, 31]) . np.trace(m3) # np.diag(m3).sum()와 같습니다 . 39 . &#49440;&#54805; &#48169;&#51221;&#49885; &#54400;&#44592; . solve 함수는 다음과 같은 선형 방정식을 풉니다: . $2x + 6y = 6$ | $5x + 3y = -9$ | . coeffs = np.array([[2, 6], [5, 3]]) depvars = np.array([6, -9]) solution = linalg.solve(coeffs, depvars) solution . array([-3., 2.]) . solution을 확인해 보죠: . coeffs.dot(solution), depvars # 네 같네요 . (array([ 6., -9.]), array([ 6, -9])) . 좋습니다! 다른 방식으로도 solution을 확인해 보죠: . np.allclose(coeffs.dot(solution), depvars) . True . &#48289;&#53552;&#54868; . 한 번에 하나씩 개별 배열 원소에 대해 연산을 실행하는 대신 배열 연산을 사용하면 훨씬 효율적인 코드를 만들 수 있습니다. 이를 벡터화라고 합니다. 이를 사용하여 넘파이의 최적화된 성능을 활용할 수 있습니다. . 예를 들어, $sin(xy/40.5)$ 식을 기반으로 768x1024 크기 배열을 생성하려고 합니다. 중첩 반복문 안에 파이썬의 math 함수를 사용하는 것은 나쁜 방법입니다: . import math data = np.empty((768, 1024)) for y in range(768): for x in range(1024): data[y, x] = math.sin(x*y/40.5) # 매우 비효율적입니다! . 작동은 하지만 순수한 파이썬 코드로 반복문이 진행되기 때문에 아주 비효율적입니다. 이 알고리즘을 벡터화해 보죠. 먼저 넘파이 meshgrid 함수로 좌표 벡터를 사용해 행렬을 만듭니다. . x_coords = np.arange(0, 1024) # [0, 1, 2, ..., 1023] y_coords = np.arange(0, 768) # [0, 1, 2, ..., 767] X, Y = np.meshgrid(x_coords, y_coords) X . array([[ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023], ..., [ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023], [ 0, 1, 2, ..., 1021, 1022, 1023]]) . Y . array([[ 0, 0, 0, ..., 0, 0, 0], [ 1, 1, 1, ..., 1, 1, 1], [ 2, 2, 2, ..., 2, 2, 2], ..., [765, 765, 765, ..., 765, 765, 765], [766, 766, 766, ..., 766, 766, 766], [767, 767, 767, ..., 767, 767, 767]]) . 여기서 볼 수 있듯이 X와 Y 모두 768x1024 배열입니다. X에 있는 모든 값은 수평 좌표에 해당합니다. Y에 있는 모든 값은 수직 좌표에 해당합니다. . 이제 간단히 배열 연산을 사용해 계산할 수 있습니다: . data = np.sin(X*Y/40.5) . 맷플롯립의 imshow 함수를 사용해 이 데이터를 그려보죠(matplotlib tutorial을 참조하세요). . import matplotlib.pyplot as plt import matplotlib.cm as cm fig = plt.figure(1, figsize=(7, 6)) plt.imshow(data, cmap=cm.hot) plt.show() . &#51200;&#51109;&#44284; &#47196;&#46377; . 넘파이는 ndarray를 바이너리 또는 텍스트 포맷으로 손쉽게 저장하고 로드할 수 있습니다. . &#48148;&#51060;&#45320;&#47532; .npy &#54252;&#47607; . 랜덤 배열을 만들고 저장해 보죠. . a = np.random.rand(2,3) a . array([[0.5435938 , 0.92886307, 0.01535158], [0.4157283 , 0.9102127 , 0.55129708]]) . np.save(&quot;my_array&quot;, a) . 끝입니다! 파일 이름의 확장자를 지정하지 않았기 때문에 넘파이는 자동으로 .npy를 붙입니다. 파일 내용을 확인해 보겠습니다: . with open(&quot;my_array.npy&quot;, &quot;rb&quot;) as f: content = f.read() content . b&#34; x93NUMPY x01 x00v x00{&#39;descr&#39;: &#39;&lt;f8&#39;, &#39;fortran_order&#39;: False, &#39;shape&#39;: (2, 3), } nY xc1 xfc xd0 x1ee xe1? xde{3 t? xb9 xed? x80V x08 xef xa5p x8f? x96I} xe0J x9b xda? xe0U xfaav xed? xd8 xe50 xc59 xa4 xe1?&#34; . 이 파일을 넘파이 배열로 로드하려면 load 함수를 사용합니다: . a_loaded = np.load(&quot;my_array.npy&quot;) a_loaded . array([[0.5435938 , 0.92886307, 0.01535158], [0.4157283 , 0.9102127 , 0.55129708]]) . &#53581;&#49828;&#53944; &#54252;&#47607; . 배열을 텍스트 포맷으로 저장해 보죠: . np.savetxt(&quot;my_array.csv&quot;, a) . 파일 내용을 확인해 보겠습니다: . with open(&quot;my_array.csv&quot;, &quot;rt&quot;) as f: print(f.read()) . 5.435937959464737235e-01 9.288630656918674955e-01 1.535157809943688001e-02 4.157283012656532994e-01 9.102126992826775620e-01 5.512970782648904944e-01 . 이 파일은 탭으로 구분된 CSV 파일입니다. 다른 구분자를 지정할 수도 있습니다: . np.savetxt(&quot;my_array.csv&quot;, a, delimiter=&quot;,&quot;) . 이 파일을 로드하려면 loadtxt 함수를 사용합니다: . a_loaded = np.loadtxt(&quot;my_array.csv&quot;, delimiter=&quot;,&quot;) a_loaded . array([[0.5435938 , 0.92886307, 0.01535158], [0.4157283 , 0.9102127 , 0.55129708]]) . &#50517;&#52629;&#46108; .npz &#54252;&#47607; . 여러 개의 배열을 압축된 한 파일로 저장하는 것도 가능합니다: . b = np.arange(24, dtype=np.uint8).reshape(2, 3, 4) b . array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]], dtype=uint8) . np.savez(&quot;my_arrays&quot;, my_a=a, my_b=b) . 파일 내용을 확인해 보죠. .npz 파일 확장자가 자동으로 추가되었습니다. . with open(&quot;my_arrays.npz&quot;, &quot;rb&quot;) as f: content = f.read() repr(content)[:180] + &quot;[...]&quot; . &#39;b&#34;PK x03 x04 x14 x00 x00 x00 x00 x00 x00 x00! x00 x063 xcf xb9 xb0 x00 x00 x00 xb0 x00 x00 x00 x08 x00 x14 x00my_a.npy x01 x00 x10 x00 xb0 x00 x00 x00 x00 x00 x00 x00 xb0 x00 x00 x[...]&#39; . 다음과 같이 이 파일을 로드할 수 있습니다: . my_arrays = np.load(&quot;my_arrays.npz&quot;) my_arrays . &lt;numpy.lib.npyio.NpzFile at 0x7f9791c73d60&gt; . 게으른 로딩을 수행하는 딕셔너리와 유사한 객체입니다: . my_arrays.keys() . KeysView(&lt;numpy.lib.npyio.NpzFile object at 0x7f9791c73d60&gt;) . my_arrays[&quot;my_a&quot;] . array([[0.5435938 , 0.92886307, 0.01535158], [0.4157283 , 0.9102127 , 0.55129708]]) . &#44536; &#45796;&#51020;&#51008;? . 넘파이 기본 요소를 모두 배웠지만 훨씬 더 많은 기능이 있습니다. 이를 배우는 가장 좋은 방법은 넘파이를 직접 실습해 보고 훌륭한 넘파이 문서에서 필요한 함수와 기능을 찾아 보세요. .",
            "url": "https://yeaun12.github.io/blog/jupyter/python/2022/03/11/numpy.html",
            "relUrl": "/jupyter/python/2022/03/11/numpy.html",
            "date": " • Mar 11, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Markdown Cheat Sheet",
            "content": "Markdown Cheat Sheet . Thanks for visiting The Markdown Guide! . This Markdown cheat sheet provides a quick overview of all the Markdown syntax elements. It can’t cover every edge case, so if you need more information about any of these elements, refer to the reference guides for basic syntax and extended syntax. . Basic Syntax . These are the elements outlined in John Gruber’s original design document. All Markdown applications support these elements. . Heading . H1 . H2 . H3 . Bold . bold text . Italic . italicized text . Blockquote . blockquote . Ordered List . First item | Second item | Third item | Unordered List . First item | Second item | Third item | . Code . code . Horizontal Rule . . Link . Markdown Guide . Image . . Extended Syntax . These elements extend the basic syntax by adding additional features. Not all Markdown applications support these elements. . Table . Syntax Description . Header | Title | . Paragraph | Text | . Fenced Code Block . { &quot;firstName&quot;: &quot;John&quot;, &quot;lastName&quot;: &quot;Smith&quot;, &quot;age&quot;: 25 } . Footnote . Here’s a sentence with a footnote. 1 . Heading ID . My Great Heading . Definition List . term definition Strikethrough . The world is flat. . Task List . Write the press release | Update the website | Contact the media | . Emoji . That is so funny! :joy: . (See also Copying and Pasting Emoji) . Highlight . I need to highlight these ==very important words==. . Subscript . H~2~O . Superscript . X^2^ . This is the footnote. &#8617; . |",
            "url": "https://yeaun12.github.io/blog/markdown/2022/03/11/markdown-cheat-sheet.html",
            "relUrl": "/markdown/2022/03/11/markdown-cheat-sheet.html",
            "date": " • Mar 11, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://yeaun12.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://yeaun12.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "관심분야 . 빅데이터 .",
          "url": "https://yeaun12.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://yeaun12.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}